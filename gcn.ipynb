{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    \"\"\"\n",
    "    From Thomas Kips repo\n",
    "    \"\"\"\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_MAO():\n",
    "    import networkx as nx\n",
    "    from gklearn.utils.graphfiles import loadDataset\n",
    "    atom_to_onehot = {'C': [1., 0., 0.], 'N': [0., 1., 0.], 'O': [0., 0., 1.]}\n",
    "    Gs, y = loadDataset(\n",
    "        \"/home/luc/TRAVAIL/DeepGED/MAO/dataset.ds\")\n",
    "    #t_classes = torch.Tensor(encode_onehot(y))\n",
    "    max_size = 30\n",
    "    adjs = []\n",
    "    inputs = []\n",
    "    for i, G in enumerate(Gs):\n",
    "        I = torch.eye(G.order(), G.order())\n",
    "        A = torch.Tensor(nx.adjacency_matrix(G).todense())\n",
    "        adj = F.pad(A+I, pad=(0, max_size-G.order(), 0, max_size-G.order()))\n",
    "        adjs.append(adj)\n",
    "\n",
    "        f_0 = []\n",
    "        for _, label in G.nodes(data=True):\n",
    "            cur_label = atom_to_onehot[label['atom']].copy()\n",
    "            f_0.append(cur_label)\n",
    "\n",
    "        X = F.pad(torch.Tensor(f_0), pad=(0, 0, 0, max_size-G.order()))\n",
    "        inputs.append(X)\n",
    "    return inputs, adjs, y  # t_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'N', 'O']\n",
      "{'C': 0, 'N': 1, 'O': 2} 3\n",
      "nb_edge labels= 3\n",
      "torch.Size([68, 729])\n",
      "adjacency matrices tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "node labels tensor([[[0., 1., 0.],\n",
      "         [1., 0., 0.],\n",
      "         [1., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0.],\n",
      "         [1., 0., 0.],\n",
      "         [1., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0.],\n",
      "         [1., 0., 0.],\n",
      "         [1., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 1., 0.],\n",
      "         [1., 0., 0.],\n",
      "         [1., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0.],\n",
      "         [1., 0., 0.],\n",
      "         [1., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0.],\n",
      "         [1., 0., 0.],\n",
      "         [1., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]], device='cuda:0')\n",
      "order of the graphs tensor([11, 12, 14, 14, 15, 17, 15, 16, 19, 15, 16, 19, 12, 13, 15, 18, 16, 16,\n",
      "        17, 16, 17, 17, 18, 19, 19, 18, 18, 22, 22, 15, 14, 13, 16, 17, 17, 21,\n",
      "        17, 18, 18, 21, 24, 25, 25, 14, 17, 18, 18, 22, 23, 23, 25, 27, 27, 15,\n",
      "        16, 23, 24, 24, 16, 17, 17, 20, 23, 24, 26, 16, 17, 21],\n",
      "       device='cuda:0')\n",
      "nb_edge_pair_label,nb_edge_labels 3 3\n",
      "6\n",
      "Parameter containing:\n",
      "tensor([1., 1.], device='cuda:0', requires_grad=True)\n",
      "27 68\n",
      "toto\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import svd\n",
    "import gcn\n",
    "#from svd import iterated_power as compute_major_axis\n",
    "compute_major_axis=svd.CustomMajorAxis.apply\n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "        \n",
    "    def __init__(self,GraphList,normalize=False):\n",
    "        super(Net, self).__init__()   \n",
    "        self.normalize=normalize\n",
    "        self.total_nb_graphs=len(GraphList)\n",
    "        self.nb_feature=20\n",
    "        \n",
    "        dict,self.nb_edge_labels=self.build_node_dictionnary(GraphList)\n",
    "        self.nb_labels=len(dict)\n",
    "        print('nb_edge labels=',self.nb_edge_labels)\n",
    "        self.device=torch.device(\"cuda:0\")\n",
    "        self.gcn=gcn.GCN2(self.nb_labels,self.nb_feature,dropout=0.1).to(self.device)\n",
    "        self.node_weighs=nn.Parameter(torch.ones(2,requires_grad=True,device=self.device)) # all substitution costs+ nodeIns/Del. old version: 0 node subs, 1 nodeIns/Del, 2 : edgeSubs, 3 edgeIns/Del\n",
    "        \n",
    "        nb_edge_pair_label=int(self.nb_edge_labels*(self.nb_edge_labels-1)/2)\n",
    "        self.edge_weighs=nn.Parameter(torch.rand(nb_edge_pair_label+1,requires_grad=True,device=self.device)) #edgeIns/Del\n",
    "        \n",
    "        self.card=torch.tensor([G.order() for G in GraphList]).to(self.device)\n",
    "        card_max=self.card.max()\n",
    "        self.A=torch.empty((len(GraphList),card_max*card_max),dtype=torch.float,device=self.device)\n",
    "        self.labels=torch.empty(self.total_nb_graphs,card_max,self.nb_labels,dtype=torch.float,device=self.device)\n",
    "        print(self.A.shape)\n",
    "        for k in range(len(GraphList)):\n",
    "            A,l=self.from_networkx_to_tensor(GraphList[k],dict,card_max)             \n",
    "            self.A[k,0:A.shape[1]]=A[0]\n",
    "            self.labels[k,:,:]=l\n",
    "        print('adjacency matrices',self.A)\n",
    "        print('node labels',self.labels)\n",
    "        print('order of the graphs',self.card)\n",
    "        print('nb_edge_pair_label,nb_edge_labels',nb_edge_pair_label,self.nb_edge_labels)\n",
    "        \n",
    "    def forward(self, input):        \n",
    "        ged=torch.zeros(len(input)).to(self.device) \n",
    "        node_subst,nodeInsDel,edge_costs,edgeInsDel=self.from_weighs_to_costs()\n",
    "        \n",
    "        f_computed= torch.zeros(self.total_nb_graphs).to(self.device)    \n",
    "        node_features=torch.zeros(self.total_nb_graphs,self.card.max(),self.nb_feature).to(self.device)    \n",
    "        #print('weighs:',self.weighs.device,'device:',self.device,'card:',self.card.device,'A:',self.A.device,'labels:',self.labels.device)\n",
    "        \n",
    "        for k in range(len(input)):            \n",
    "            g1=input[k][0]\n",
    "            g2=input[k][1]\n",
    "            \n",
    "            n=self.card[g1]\n",
    "            m=self.card[g2]\n",
    "            if not f_computed[g1]:\n",
    "                node_features[g1,0:n,:]=self.gcn(self.labels[g1][0:n,:],self.A[g1][0:n*n].view(n,n)+torch.eye(n,n,device=self.device))                \n",
    "                f_computed[g1]=1\n",
    "                #print('m1=',self.gcn(self.labels[g1][0:n,:],self.A[g1][0:n*n].view(n,n)+torch.eye(n,n,device=self.device)))\n",
    "            if not f_computed[g2]:\n",
    "                node_features[g2,0:m,:]=self.gcn(self.labels[g2][0:m,:],self.A[g2][0:m*m].view(m,m)+torch.eye(m,m,device=self.device))\n",
    "                f_computed[g2]=1\n",
    "                #print('m2=',self.gcn(self.labels[g2][0:m,:],self.A[g2][0:m*m].view(m,m)+torch.eye(m,m,device=self.device)))\n",
    "        \n",
    "        for k in range(len(input)):            \n",
    "            g1=input[k][0]\n",
    "            g2=input[k][1]\n",
    "            \n",
    "            n=self.card[g1]\n",
    "            m=self.card[g2]\n",
    "            m1=node_features[g1,0:n,:]\n",
    "            m2=node_features[g2,0:m,:]\n",
    "            C=self.construct_cost_matrix(g1,g2,m1,m2,node_subst,edge_costs,nodeInsDel,edgeInsDel)      \n",
    "            S=self.mapping_from_cost(C,n,m) # FW\n",
    "            #S=self.mapping_from_similarity(C,n,m) # SVD or iterated power.\n",
    "            v=torch.flatten(S)\n",
    "            \n",
    "            normalize_factor=1.0\n",
    "            if self.normalize:\n",
    "                nb_edge1=(self.A[g1][0:n*n] != torch.zeros(n*n,device=self.device)).int().sum()\n",
    "                nb_edge2=(self.A[g2][0:m*m] != torch.zeros(m*m,device=self.device)).int().sum()\n",
    "                #normalize_factor=(nodeInsDel*n+edgeInsDel*nb_edge1)*(nodeInsDel*m+edgeInsDel*nb_edge2)                                    \n",
    "                normalize_factor=(nodeInsDel*(n+m)+edgeInsDel*(nb_edge1+nb_edge2))\n",
    "            ged[k]=.5*(v.t()@(C+torch.diag(C))@v)/normalize_factor    \n",
    "        return ged\n",
    "    \n",
    "    def from_weighs_to_costs(self):\n",
    "        \n",
    "        #cn=torch.exp(self.node_weighs)\n",
    "        #ce=torch.exp(self.edge_weighs)\n",
    "        cn=self.node_weighs*self.node_weighs\n",
    "        #cn=torch.where(x<1.0,x,torch.ones_like(x))\n",
    "        ce=self.edge_weighs*self.edge_weighs\n",
    "        #ce=torch.where(y<1.0,y,torch.ones_like(y))\n",
    "        total_cost=cn.sum()+ce.sum()\n",
    "        cn=cn/total_cost\n",
    "        ce=ce/total_cost\n",
    "        edgeInsDel=ce[-1]\n",
    "\n",
    "        #node_costs=torch.zeros((self.nb_labels,self.nb_labels),device=self.device)\n",
    "        #upper_part=torch.triu_indices(node_costs.shape[0],node_costs.shape[1],offset=1,device=self.device)        \n",
    "        #node_costs[upper_part[0],upper_part[1]]=cn[0:-1]\n",
    "        #node_costs=node_costs+node_costs.T\n",
    "\n",
    "        if self.nb_edge_labels>1:\n",
    "            edge_costs=torch.zeros((self.nb_edge_labels,self.nb_edge_labels),device=self.device)\n",
    "            upper_part=torch.triu_indices(edge_costs.shape[0],edge_costs.shape[1],offset=1,device=self.device)        \n",
    "            edge_costs[upper_part[0],upper_part[1]]=ce[0:-1]\n",
    "            edge_costs=edge_costs+edge_costs.T\n",
    "        else:\n",
    "            edge_costs=torch.zeros(0,device=self.device)\n",
    "        \n",
    "        return cn[0],cn[1],edge_costs,edgeInsDel\n",
    "    \n",
    "    def build_node_dictionnary(self,GraphList):\n",
    "        #extraction de tous les labels d'atomes\n",
    "        node_labels=[]\n",
    "        for G in Gs:\n",
    "            for v in nx.nodes(G):\n",
    "                if not G.nodes[v]['label'][0] in node_labels:\n",
    "                    node_labels.append(G.nodes[v]['label'][0])\n",
    "        node_labels.sort()\n",
    "        #extraction d'un dictionnaire permettant de numéroter chaque label par un numéro.\n",
    "        dict={}\n",
    "        k=0\n",
    "        for label in node_labels:\n",
    "            dict[label]=k\n",
    "            k=k+1\n",
    "        print(node_labels)\n",
    "        print(dict,len(dict))\n",
    "    \n",
    "        return dict,max(max([[int(G[e[0]][e[1]]['bond_type']) for e in G.edges()] for G in GraphList]))\n",
    "    \n",
    "    def from_networkx_to_tensor(self,G,dict,nb_max_vertices):    \n",
    "        A=torch.tensor(nx.to_scipy_sparse_matrix(G,dtype=float,weight='bond_type').todense(),dtype=torch.int)        \n",
    "        lab=torch.zeros(nb_max_vertices,self.nb_labels)\n",
    "        k=0\n",
    "        \n",
    "        for v in nx.nodes(G):\n",
    "            lab[k][dict[G.nodes[v]['label'][0]]]=1.0\n",
    "            k=k+1\n",
    "   \n",
    "        return (A.view(1,A.shape[0]*A.shape[1]),lab)\n",
    "\n",
    "    def construct_cost_matrix(self,g1,g2,m1,m2,node_subst,edge_costs,nodeInsDel,edgeInsDel):\n",
    "        n = self.card[g1].item()\n",
    "        m = self.card[g2].item()\n",
    "        \n",
    "        A1=torch.zeros((n+1,n+1),dtype=torch.int,device=self.device)\n",
    "        A1[0:n,0:n]=self.A[g1][0:n*n].view(n,n)\n",
    "        A2=torch.zeros((m+1,m+1),dtype=torch.int,device=self.device)\n",
    "        A2[0:m,0:m]=self.A[g2][0:m*m].view(m,m)\n",
    "        \n",
    "        \n",
    "         # costs: 0 node subs, 1 nodeIns/Del, 2 : edgeSubs, 3 edgeIns/Del\n",
    "        \n",
    "        #C=cost[3]*torch.cat([torch.cat([C12[l][k] for k in range(n+1)],1) for l in range(n+1)])\n",
    "        #Pas bien sur mais cela semble fonctionner.\n",
    "        C=edgeInsDel*self.matrix_edgeInsDel(A1,A2)\n",
    "        if self.nb_edge_labels>1:\n",
    "            for k in range(self.nb_edge_labels):\n",
    "                for l in range(self.nb_edge_labels):\n",
    "                    if k != l:\n",
    "#                    C.add_(self.matrix_edgeSubst(A1,A2,k+1,l+1),alpha=edge_costs[k][l])\n",
    "                        C=C+edge_costs[k][l]*self.matrix_edgeSubst(A1,A2,k+1,l+1)\n",
    "        #C=cost[3]*torch.tensor(np.array([ [  k!=l and A1[k//(m+1),l//(m+1)]^A2[k%(m+1),l%(m+1)] for k in range((n+1)*(m+1))] for l in range((n+1)*(m+1))]),device=self.device)        \n",
    "\n",
    "        #l1=self.labels[g1][0:n,:]\n",
    "        #l2=self.labels[g2][0:m,:]        \n",
    "        D=torch.zeros((n+1)*(m+1),device=self.device)\n",
    "        D[n*(m+1):]=nodeInsDel\n",
    "        D[n*(m+1)+m]=0\n",
    "        norm_l1=torch.einsum('i,j->ij',torch.diag(m1@m1.T),torch.ones(m,device=self.device))\n",
    "        norm_l2=torch.einsum('i,j->ij',torch.diag(m2@m2.T),torch.ones(n,device=self.device)).T\n",
    "        scalar=m1@m2.T\n",
    "        \n",
    "        d=torch.empty(n,m+1,device=self.device)\n",
    "        x=(norm_l1+norm_l2-2*scalar)/m1.shape[1]\n",
    "        eps=10**(-6)\n",
    "        d[:,0:m]=node_subst*torch.sqrt(torch.where(x>eps,x,eps*torch.ones(n,m,device=self.device)))       \n",
    "        d[:,-1]=nodeInsDel\n",
    "       # print('d=',d)\n",
    "        D[0:n*(m+1)]=d.view(1,d.shape[0]*d.shape[1])\n",
    "        #D[[i*(m+1)+m for i in range(n)]]=nodeInsDel\n",
    "        #pdist = nn.PairwiseDistance(p=2)\n",
    "        #D[[k for k in range(n*(m+1)) if k%(m+1) != m]]=torch.tensor([pdist(l1[k//(m+1),:],l2[k%(m+1),:]) for k in range(n*(m+1)) if k%(m+1) != m],device=self.device )\n",
    "        mask = torch.diag(torch.ones_like(D))\n",
    "        C=mask*torch.diag(D) + (1. - mask)*C\n",
    "        \n",
    "        #C[range(len(C)),range(len(C))]=D\n",
    "      \n",
    "        return C\n",
    "    def matrix_edgeInsDel(self,A1,A2):\n",
    "        Abin1=(A1!=torch.zeros((A1.shape[0],A1.shape[1]),device=self.device))\n",
    "        Abin2=(A2!=torch.zeros((A2.shape[0],A2.shape[1]),device=self.device))\n",
    "        C1=torch.einsum('ij,kl->ijkl',torch.logical_not(Abin1),Abin2)\n",
    "        C2=torch.einsum('ij,kl->ijkl',Abin1,torch.logical_not(Abin2))\n",
    "        C12=torch.logical_or(C1,C2).int()\n",
    "    \n",
    "        return torch.cat(torch.unbind(torch.cat(torch.unbind(C12,1),1),0),1)\n",
    "\n",
    "    def matrix_edgeSubst(self,A1,A2,lab1,lab2):\n",
    "        Abin1=(A1==lab1*torch.ones((A1.shape[0],A1.shape[1]),device=self.device)).int()\n",
    "        Abin2=(A2==lab2*torch.ones((A2.shape[0],A2.shape[1]),device=self.device)).int()\n",
    "        C=torch.einsum('ij,kl->ijkl',Abin1,Abin2)\n",
    "        \n",
    "        return torch.cat(torch.unbind(torch.cat(torch.unbind(C,1),1),0),1)\n",
    "     \n",
    "    def mapping_from_cost(self,C,n,m):\n",
    "        c=torch.diag(C)\n",
    "        D=C-torch.eye(C.shape[0],device=self.device)*c\n",
    "        x0=svd.eps_assigment_from_mapping(torch.exp(-c.view(n+1,m+1)),10).view((n+1)*(m+1),1)\n",
    "    \n",
    "        return svd.franck_wolfe(x0,D,c,2,10,n,m)\n",
    "    \n",
    "    def similarity_from_cost(self,C):\n",
    "        N=C.shape[0]\n",
    "        \n",
    "     \n",
    "        return (C.max()*torch.eye(N,device=self.device) -C)\n",
    "    \n",
    "    def mapping_from_similarity(self,C,n,m):\n",
    "        M=self.similarity_from_cost(C)\n",
    "        first_ev=compute_major_axis(M)\n",
    "        #first_ev=self.iterated_power(M,inv=True)\n",
    "        if(first_ev.sum() <0):\n",
    "            first_ev=-first_ev\n",
    "        S=torch.exp(first_ev.view(n+1,m+1)) # enforce the difference, accelerate the convergence. \n",
    "        S=self.eps_assigment_from_mapping(S)\n",
    "        return  S\n",
    "        \n",
    "    def eps_assigment_from_mapping(self,S):\n",
    "        ones_n = torch.ones(S.shape[0],device=S.device)\n",
    "        ones_m = torch.ones(S.shape[1],device=S.device)\n",
    "    \n",
    "        Sk = S\n",
    "        for i in range(20):\n",
    "            D=torch.diag(1.0/(Sk@ones_m))\n",
    "            D[D.shape[0]-1,D.shape[1]-1]=1.0\n",
    "            Sk1 = D@Sk\n",
    "            D=torch.diag(1.0/(ones_n@Sk1))\n",
    "            D[D.shape[0]-1,D.shape[1]-1]=1.0\n",
    "            Sk = Sk1@D\n",
    "        \n",
    "        return Sk\n",
    "\n",
    "import networkx as nx\n",
    "from gklearn.utils.graphfiles import loadDataset\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "#writer = SummaryWriter('runs/fashion_GED_experiment_1')\n",
    "\n",
    "Gs, y = loadDataset(\"/home/luc/TRAVAIL/DeepGED/MAO/dataset.ds\")\n",
    "model = Net(Gs,normalize=True)\n",
    "\n",
    "params = list(model.parameters())\n",
    "print(len(params))\n",
    "print(params[0])\n",
    "#print(model(input))\n",
    "print(max([G.order() for G in Gs]),len(Gs))\n",
    "print('toto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([19, 28, 28, 28, 23, 26, 27, 17])\n",
      "tensor([20,  4, 25, 18, 34, 66, 53, 36, 12, 67,  0, 39, 31, 51, 57])\n",
      "105\n",
      "data= tensor([[20,  4],\n",
      "        [20, 25],\n",
      "        [20, 18],\n",
      "        [20, 34],\n",
      "        [20, 66],\n",
      "        [20, 53],\n",
      "        [20, 36],\n",
      "        [20, 12],\n",
      "        [20, 67],\n",
      "        [20,  0],\n",
      "        [20, 39],\n",
      "        [20, 31],\n",
      "        [20, 51],\n",
      "        [20, 57],\n",
      "        [ 4, 25],\n",
      "        [ 4, 18],\n",
      "        [ 4, 34],\n",
      "        [ 4, 66],\n",
      "        [ 4, 53],\n",
      "        [ 4, 36],\n",
      "        [ 4, 12],\n",
      "        [ 4, 67],\n",
      "        [ 4,  0],\n",
      "        [ 4, 39],\n",
      "        [ 4, 31],\n",
      "        [ 4, 51],\n",
      "        [ 4, 57],\n",
      "        [25, 18],\n",
      "        [25, 34],\n",
      "        [25, 66],\n",
      "        [25, 53],\n",
      "        [25, 36],\n",
      "        [25, 12],\n",
      "        [25, 67],\n",
      "        [25,  0],\n",
      "        [25, 39],\n",
      "        [25, 31],\n",
      "        [25, 51],\n",
      "        [25, 57],\n",
      "        [18, 34],\n",
      "        [18, 66],\n",
      "        [18, 53],\n",
      "        [18, 36],\n",
      "        [18, 12],\n",
      "        [18, 67],\n",
      "        [18,  0],\n",
      "        [18, 39],\n",
      "        [18, 31],\n",
      "        [18, 51],\n",
      "        [18, 57],\n",
      "        [34, 66],\n",
      "        [34, 53],\n",
      "        [34, 36],\n",
      "        [34, 12],\n",
      "        [34, 67],\n",
      "        [34,  0],\n",
      "        [34, 39],\n",
      "        [34, 31],\n",
      "        [34, 51],\n",
      "        [34, 57],\n",
      "        [66, 53],\n",
      "        [66, 36],\n",
      "        [66, 12],\n",
      "        [66, 67],\n",
      "        [66,  0],\n",
      "        [66, 39],\n",
      "        [66, 31],\n",
      "        [66, 51],\n",
      "        [66, 57],\n",
      "        [53, 36],\n",
      "        [53, 12],\n",
      "        [53, 67],\n",
      "        [53,  0],\n",
      "        [53, 39],\n",
      "        [53, 31],\n",
      "        [53, 51],\n",
      "        [53, 57],\n",
      "        [36, 12],\n",
      "        [36, 67],\n",
      "        [36,  0],\n",
      "        [36, 39],\n",
      "        [36, 31],\n",
      "        [36, 51],\n",
      "        [36, 57],\n",
      "        [12, 67],\n",
      "        [12,  0],\n",
      "        [12, 39],\n",
      "        [12, 31],\n",
      "        [12, 51],\n",
      "        [12, 57],\n",
      "        [67,  0],\n",
      "        [67, 39],\n",
      "        [67, 31],\n",
      "        [67, 51],\n",
      "        [67, 57],\n",
      "        [ 0, 39],\n",
      "        [ 0, 31],\n",
      "        [ 0, 51],\n",
      "        [ 0, 57],\n",
      "        [39, 31],\n",
      "        [39, 51],\n",
      "        [39, 57],\n",
      "        [31, 51],\n",
      "        [31, 57],\n",
      "        [51, 57]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "nb=len(Gs)\n",
    "class1=torch.tensor([k for k in range(len(y)) if y[k]==1])\n",
    "class2=torch.tensor([k for k in range(len(y)) if y[k]==0])\n",
    "\n",
    "train_size=15\n",
    "\n",
    "if train_size % 2 == 0:\n",
    "    nb_class1=int(train_size/2)\n",
    "    nb_class2=int(train_size/2)\n",
    "else:\n",
    "    nb_class1=int(train_size/2)+1\n",
    "    nb_class2=int(train_size/2)\n",
    "    \n",
    "print((torch.abs(10000*torch.randn(nb_class1)).int()%class1.size()[0]).long())\n",
    "random_class1=class1[(torch.abs(10000*torch.randn(nb_class1)).int()%class1.size()[0]).long()]\n",
    "random_class2=class2[(torch.abs(10000*torch.randn(nb_class2)).int()%class2.size()[0]).long()]\n",
    "train_graphs=torch.cat((random_class1,random_class2),0)\n",
    "print(train_graphs)\n",
    "\n",
    "couples=torch.triu_indices(train_size,train_size,offset=1)\n",
    "\n",
    "#combinations=itertools.combinations(range(nb),2)\n",
    "\n",
    "nb_elt=int(train_size*(train_size-1)/2)\n",
    "data=torch.empty((nb_elt,2),dtype=torch.int)\n",
    "yt=torch.ones(nb_elt)\n",
    "\n",
    "data[0:nb_elt,0]=train_graphs[couples[0]]\n",
    "data[0:nb_elt,1]=train_graphs[couples[1]]\n",
    "print(nb_elt)\n",
    "#couples=[]\n",
    "for k in range(nb_elt):\n",
    "    if (y[data[k][0]]!=y[data[k][1]]):\n",
    "        yt[k]=-1.0        \n",
    "\n",
    "print('data=',data)\n",
    "\n",
    "#print(couples[1:2])\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")          # a CUDA device object   \n",
    "    \n",
    "#writer.add_graph(model,data)\n",
    "#writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "0 0.5665070414543152\n",
      "Distances:  tensor([0.6399, 0.8878, 0.7011, 0.7248, 0.6648, 0.6269, 0.7417, 0.5102, 1.0706,\n",
      "        0.5099, 1.0377, 0.5207, 1.7675, 1.2460, 0.9008, 0.8991, 0.8909, 0.8342,\n",
      "        0.6741, 0.8458, 0.5010, 1.3704, 0.4439, 1.1853, 0.5829, 2.1328, 1.6657,\n",
      "        0.7359, 0.7635, 0.6948, 0.6048, 0.7335, 0.4940, 1.1384, 0.4805, 1.0479,\n",
      "        0.5438, 1.6951, 1.2808, 0.7708, 0.6871, 0.6107, 0.7394, 0.5214, 1.1163,\n",
      "        0.5272, 1.0717, 0.5302, 1.8186, 1.2934, 0.7013, 0.6510, 0.8094, 0.5186,\n",
      "        1.0973, 0.5404, 1.0487, 0.5512, 1.8117, 1.2699, 0.5906, 0.7324, 0.4898,\n",
      "        1.0640, 0.4934, 1.0143, 0.5055, 1.5441, 1.2188, 0.8554, 0.5084, 1.2391,\n",
      "        0.5072, 1.1486, 0.5277, 1.7990, 1.4331, 0.5258, 1.1335, 0.4793, 1.0845,\n",
      "        0.5584, 1.8392, 1.4222, 1.5567, 0.4798, 1.4234, 0.6493, 2.3074, 1.8390,\n",
      "        0.5678, 0.8961, 0.5583, 1.4448, 1.1050, 1.6175, 0.7232, 2.5672, 2.0863,\n",
      "        0.5081, 1.2532, 1.0098, 2.0949, 1.6529, 0.8015], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "Loss Triangular: 0.0\n",
      "nodeInsDel: 0.36196205019950867\n",
      "node Subst: 0.36196205019950867\n",
      "edge_costs :\n",
      "tensor([[0.0000, 0.0747, 0.0582],\n",
      "        [0.0747, 0.0000, 0.0093],\n",
      "        [0.0582, 0.0093, 0.0000]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "edgeInsDel: 0.1339436173439026\n",
      "1 0.5363285541534424\n",
      "2 0.521121621131897\n",
      "3 0.5170004367828369\n",
      "4 0.5162662863731384\n",
      "5 0.5130221247673035\n",
      "6 0.513128936290741\n",
      "7 0.5104386210441589\n",
      "8 0.5146821737289429\n",
      "9 0.5294013619422913\n",
      "10 0.5260109901428223\n",
      "11 0.5309742093086243\n",
      "12 0.5305291414260864\n",
      "13 0.5313937664031982\n",
      "14 0.5182075500488281\n",
      "15 0.5115387439727783\n",
      "16 0.508145272731781\n",
      "17 0.5233461856842041\n",
      "18 0.5299025177955627\n",
      "19 0.5454962849617004\n",
      "20 0.5451669692993164\n",
      "21 0.5430054664611816\n",
      "22 0.5430685877799988\n",
      "23 0.5434022545814514\n",
      "24 0.5427476763725281\n",
      "25 0.5415695905685425\n",
      "26 0.5410972237586975\n",
      "27 0.5400740504264832\n",
      "28 0.5382630825042725\n",
      "29 0.5362043976783752\n",
      "30 0.5360634922981262\n",
      "31 0.5348963737487793\n",
      "32 0.5265499353408813\n",
      "33 0.5194488167762756\n",
      "34 0.5107308626174927\n",
      "35 0.5052899718284607\n",
      "36 0.49799567461013794\n",
      "37 0.49564090371131897\n",
      "38 0.4841894805431366\n",
      "39 0.47954490780830383\n",
      "40 0.47867557406425476\n",
      "41 0.4762766659259796\n",
      "42 0.47855058312416077\n",
      "43 0.47789430618286133\n",
      "44 0.47811612486839294\n",
      "45 0.47998782992362976\n",
      "46 0.4796692728996277\n",
      "47 0.4794839322566986\n",
      "48 0.4794076383113861\n",
      "49 0.4791237711906433\n",
      "50 0.4785609245300293\n",
      "51 0.4784347712993622\n",
      "52 0.47814756631851196\n",
      "53 0.47767502069473267\n",
      "54 0.4772815704345703\n",
      "55 0.4764367341995239\n",
      "56 0.4754341244697571\n",
      "57 0.4757646322250366\n",
      "58 0.475100576877594\n",
      "59 0.47561153769493103\n",
      "60 0.47460177540779114\n",
      "61 0.4744899868965149\n",
      "62 0.474729061126709\n",
      "63 0.4744383990764618\n",
      "64 0.47459569573402405\n",
      "65 0.47337642312049866\n",
      "66 0.4739898145198822\n",
      "67 0.4741670787334442\n",
      "68 0.47345399856567383\n",
      "69 0.4734308421611786\n",
      "70 0.47343409061431885\n",
      "71 0.47363102436065674\n",
      "72 0.47312238812446594\n",
      "73 0.4726608395576477\n",
      "74 0.47311410307884216\n",
      "75 0.47338366508483887\n",
      "76 0.4736294746398926\n",
      "77 0.4733276665210724\n",
      "78 0.47317200899124146\n",
      "79 0.47302523255348206\n",
      "80 0.4728868901729584\n",
      "81 0.4729713499546051\n",
      "82 0.4717603623867035\n",
      "83 0.4726204574108124\n",
      "84 0.47372809052467346\n",
      "85 0.47280415892601013\n",
      "86 0.47339367866516113\n",
      "87 0.47287940979003906\n",
      "88 0.47344470024108887\n",
      "89 0.4730779230594635\n",
      "90 0.4740140736103058\n",
      "91 0.4729800820350647\n",
      "92 0.47248777747154236\n",
      "93 0.47283440828323364\n",
      "94 0.4733254015445709\n",
      "95 0.4724080562591553\n",
      "96 0.4725770056247711\n",
      "97 0.47230565547943115\n",
      "98 0.472120463848114\n",
      "99 0.4723738729953766\n",
      "Distances:  tensor([0.6903, 0.7147, 0.6754, 0.7092, 0.6899, 0.7134, 0.6961, 0.7757, 0.7673,\n",
      "        0.8065, 0.7805, 0.7622, 1.0355, 0.8888, 0.7166, 0.6579, 0.6863, 0.6735,\n",
      "        0.6472, 0.6786, 0.6762, 0.8096, 0.7014, 0.8247, 0.6813, 1.1384, 0.9651,\n",
      "        0.7164, 0.7327, 0.7184, 0.7434, 0.7147, 0.8564, 0.7623, 0.8794, 0.7703,\n",
      "        0.8080, 0.9822, 0.8596, 0.7037, 0.6866, 0.7112, 0.6917, 0.7730, 0.7637,\n",
      "        0.8055, 0.7787, 0.7604, 1.0300, 0.8860, 0.7070, 0.7239, 0.7069, 0.7986,\n",
      "        0.7840, 0.8308, 0.7985, 0.7691, 1.0507, 0.9038, 0.7129, 0.6974, 0.7901,\n",
      "        0.7721, 0.8179, 0.7828, 0.7614, 1.0356, 0.8908, 0.6840, 0.7104, 0.8219,\n",
      "        0.7151, 0.8269, 0.6809, 1.1339, 0.9693, 0.7971, 0.7703, 0.8199, 0.7820,\n",
      "        0.7589, 1.0278, 0.8877, 0.9892, 0.5980, 0.9893, 0.6420, 1.3769, 1.1751,\n",
      "        1.0504, 0.7679, 0.9583, 0.8615, 0.7910, 1.0268, 0.6199, 1.4185, 1.2216,\n",
      "        0.9523, 0.8793, 0.8043, 1.2929, 1.1045, 0.8447], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "Loss Triangular: 0.0\n",
      "nodeInsDel: 0.14745210111141205\n",
      "node Subst: 0.28580957651138306\n",
      "edge_costs :\n",
      "tensor([[0.0000, 0.1148, 0.0559],\n",
      "        [0.1148, 0.0000, 0.0055],\n",
      "        [0.0559, 0.0055, 0.0000]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "edgeInsDel: 0.3904849886894226\n",
      "100 0.47289714217185974\n",
      "101 0.472409188747406\n",
      "102 0.47265371680259705\n",
      "103 0.4727650284767151\n",
      "104 0.47177594900131226\n",
      "105 0.473131000995636\n",
      "106 0.47235581278800964\n",
      "107 0.47259700298309326\n",
      "108 0.47285839915275574\n",
      "109 0.47271671891212463\n",
      "110 0.4729584753513336\n",
      "111 0.4719404876232147\n",
      "112 0.4725256562232971\n",
      "113 0.4723924994468689\n",
      "114 0.4723728895187378\n",
      "115 0.47166913747787476\n",
      "116 0.4725044369697571\n",
      "117 0.47218477725982666\n",
      "118 0.471357136964798\n",
      "119 0.472660094499588\n",
      "120 0.4716736972332001\n",
      "121 0.4709685444831848\n",
      "122 0.47196832299232483\n",
      "123 0.4721325933933258\n",
      "124 0.47212040424346924\n",
      "125 0.4718494713306427\n",
      "126 0.47267046570777893\n",
      "127 0.470411479473114\n",
      "128 0.47136354446411133\n",
      "129 0.4703861474990845\n",
      "130 0.4714784324169159\n",
      "131 0.4711591303348541\n",
      "132 0.47086405754089355\n",
      "133 0.46944206953048706\n",
      "134 0.47054633498191833\n",
      "135 0.4721585810184479\n",
      "136 0.4718492925167084\n",
      "137 0.47131073474884033\n",
      "138 0.4714915156364441\n",
      "139 0.4704238176345825\n",
      "140 0.47103992104530334\n",
      "141 0.4702478349208832\n",
      "142 0.47061237692832947\n",
      "143 0.4708106219768524\n",
      "144 0.4711633026599884\n",
      "145 0.47112759947776794\n",
      "146 0.47172093391418457\n",
      "147 0.47101685404777527\n",
      "148 0.47049257159233093\n",
      "149 0.47108396887779236\n",
      "150 0.4713124632835388\n",
      "151 0.4710600674152374\n",
      "152 0.47117769718170166\n",
      "153 0.4704139530658722\n",
      "154 0.471564382314682\n",
      "155 0.47086116671562195\n",
      "156 0.4712335765361786\n",
      "157 0.47159621119499207\n",
      "158 0.47061121463775635\n",
      "159 0.4707599878311157\n",
      "160 0.47104358673095703\n",
      "161 0.4706179201602936\n",
      "162 0.4709450602531433\n",
      "163 0.47136354446411133\n",
      "164 0.4722718894481659\n",
      "165 0.4713951051235199\n",
      "166 0.47169366478919983\n",
      "167 0.47060418128967285\n",
      "168 0.47163328528404236\n",
      "169 0.47101086378097534\n",
      "170 0.47048261761665344\n",
      "171 0.4698432683944702\n",
      "172 0.47030001878738403\n",
      "173 0.471779465675354\n",
      "174 0.47118276357650757\n",
      "175 0.4707394242286682\n",
      "176 0.470810204744339\n",
      "177 0.47057169675827026\n",
      "178 0.47105467319488525\n",
      "179 0.4703079164028168\n",
      "180 0.4708968698978424\n",
      "181 0.47041958570480347\n",
      "182 0.4697329103946686\n",
      "183 0.46990063786506653\n",
      "184 0.47004711627960205\n",
      "185 0.4704323410987854\n",
      "186 0.4709813892841339\n",
      "187 0.4686626195907593\n",
      "188 0.469588965177536\n",
      "189 0.46951502561569214\n",
      "190 0.4704505503177643\n",
      "191 0.470449298620224\n",
      "192 0.4709072411060333\n",
      "193 0.47039854526519775\n",
      "194 0.4684641659259796\n",
      "195 0.47053274512290955\n",
      "196 0.468585729598999\n",
      "197 0.470404714345932\n",
      "198 0.46948105096817017\n",
      "199 0.4699741303920746\n",
      "Distances:  tensor([0.6609, 0.6932, 0.6701, 0.6711, 0.6608, 0.6765, 0.7037, 0.7600, 0.7451,\n",
      "        0.7994, 0.7851, 0.7705, 1.0113, 0.8694, 0.7011, 0.6571, 0.6606, 0.6454,\n",
      "        0.6309, 0.6858, 0.6700, 0.7989, 0.6934, 0.8351, 0.6920, 1.1149, 0.9520,\n",
      "        0.7131, 0.7003, 0.7009, 0.7185, 0.7234, 0.8370, 0.7420, 0.8768, 0.7786,\n",
      "        0.8136, 0.9745, 0.8468, 0.6821, 0.6709, 0.6854, 0.7125, 0.7685, 0.7584,\n",
      "        0.8074, 0.7940, 0.7772, 1.0257, 0.8836, 0.6723, 0.6778, 0.6970, 0.7801,\n",
      "        0.7442, 0.8141, 0.7836, 0.7626, 1.0040, 0.8643, 0.6754, 0.7020, 0.7649,\n",
      "        0.7435, 0.8020, 0.7855, 0.7708, 1.0047, 0.8669, 0.6896, 0.6869, 0.8062,\n",
      "        0.7076, 0.8365, 0.6897, 1.1232, 0.9602, 0.7907, 0.7774, 0.8246, 0.8064,\n",
      "        0.7727, 1.0430, 0.8992, 0.9870, 0.5961, 0.9995, 0.6570, 1.3214, 1.1675,\n",
      "        1.0546, 0.7622, 0.9624, 0.8309, 0.7589, 1.0502, 0.6515, 1.3871, 1.1923,\n",
      "        0.9684, 0.8901, 0.8086, 1.3117, 1.1263, 0.7949], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "Loss Triangular: 0.0\n",
      "nodeInsDel: 0.11964816600084305\n",
      "node Subst: 0.23754826188087463\n",
      "edge_costs :\n",
      "tensor([[0.0000, 0.2086, 0.0869],\n",
      "        [0.2086, 0.0000, 0.0316],\n",
      "        [0.0869, 0.0316, 0.0000]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "edgeInsDel: 0.3156697750091553\n",
      "200 0.47071608901023865\n",
      "201 0.46990740299224854\n",
      "202 0.4691360592842102\n",
      "203 0.4688172936439514\n",
      "204 0.4702073037624359\n",
      "205 0.46883729100227356\n",
      "206 0.4704396426677704\n",
      "207 0.47004398703575134\n",
      "208 0.470059871673584\n",
      "209 0.4685273766517639\n",
      "210 0.47013768553733826\n",
      "211 0.4673592150211334\n",
      "212 0.46735629439353943\n",
      "213 0.4689399302005768\n",
      "214 0.4671224057674408\n",
      "215 0.4686776399612427\n",
      "216 0.4657060205936432\n",
      "217 0.46782946586608887\n",
      "218 0.46622365713119507\n",
      "219 0.46536460518836975\n",
      "220 0.4655570089817047\n",
      "221 0.46415239572525024\n",
      "222 0.46438276767730713\n",
      "223 0.4626598656177521\n",
      "224 0.4635719954967499\n",
      "225 0.46577179431915283\n",
      "226 0.46493053436279297\n",
      "227 0.46175631880760193\n",
      "228 0.4644502103328705\n",
      "229 0.463400274515152\n",
      "230 0.46365582942962646\n",
      "231 0.46506327390670776\n",
      "232 0.46436718106269836\n",
      "233 0.46624264121055603\n",
      "234 0.46541938185691833\n",
      "235 0.46463972330093384\n",
      "236 0.465710312128067\n",
      "237 0.46590864658355713\n",
      "238 0.4634595513343811\n",
      "239 0.46408316493034363\n",
      "240 0.4633409082889557\n",
      "241 0.465852290391922\n",
      "242 0.46531498432159424\n",
      "243 0.46523532271385193\n",
      "244 0.46415725350379944\n",
      "245 0.46398815512657166\n",
      "246 0.4628779888153076\n",
      "247 0.4630008041858673\n",
      "248 0.4623846709728241\n",
      "249 0.4635767936706543\n",
      "250 0.4619964361190796\n",
      "251 0.4606184959411621\n",
      "252 0.46049392223358154\n",
      "253 0.46078604459762573\n",
      "254 0.4610934555530548\n",
      "255 0.46090441942214966\n",
      "256 0.4585901200771332\n",
      "257 0.45678532123565674\n",
      "258 0.4591797888278961\n",
      "259 0.46022501587867737\n",
      "260 0.45912590622901917\n",
      "261 0.4600411355495453\n",
      "262 0.46066978573799133\n",
      "263 0.45975419878959656\n",
      "264 0.4632134437561035\n",
      "265 0.46126651763916016\n",
      "266 0.45984143018722534\n",
      "267 0.4601180851459503\n",
      "268 0.4594493508338928\n",
      "269 0.46148401498794556\n",
      "270 0.4604721665382385\n",
      "271 0.46295472979545593\n",
      "272 0.4617522060871124\n",
      "273 0.462051659822464\n",
      "274 0.4591820240020752\n",
      "275 0.45914676785469055\n",
      "276 0.46279487013816833\n",
      "277 0.45875829458236694\n",
      "278 0.46217647194862366\n",
      "279 0.45869818329811096\n",
      "280 0.46164199709892273\n",
      "281 0.4597163200378418\n",
      "282 0.4588749408721924\n",
      "283 0.46024832129478455\n",
      "284 0.4608195126056671\n",
      "285 0.4574219584465027\n",
      "286 0.46139225363731384\n",
      "287 0.4605919420719147\n",
      "288 0.46282437443733215\n",
      "289 0.4629780650138855\n",
      "290 0.46236345171928406\n",
      "291 0.46368280053138733\n",
      "292 0.46134671568870544\n",
      "293 0.4608762860298157\n",
      "294 0.4595426917076111\n",
      "295 0.46204566955566406\n",
      "296 0.4633803069591522\n",
      "297 0.46069177985191345\n",
      "298 0.4622257351875305\n",
      "299 0.4622437357902527\n",
      "Distances:  tensor([0.5579, 0.5215, 0.6290, 0.5172, 0.5646, 0.5854, 0.5110, 0.8359, 0.5686,\n",
      "        0.9208, 0.6733, 0.7379, 1.0676, 0.8150, 0.5361, 0.6102, 0.5042, 0.5508,\n",
      "        0.5190, 0.5012, 0.6661, 0.7481, 0.7494, 0.7871, 0.6036, 1.3366, 1.0218,\n",
      "        0.6597, 0.5493, 0.5953, 0.6407, 0.5411, 0.9080, 0.5550, 1.0361, 0.6322,\n",
      "        0.8127, 0.9615, 0.7478, 0.6187, 0.6287, 0.6606, 0.6126, 0.8537, 0.7374,\n",
      "        0.9570, 0.7555, 0.7661, 1.2055, 0.9364, 0.5620, 0.5831, 0.5085, 0.8351,\n",
      "        0.5657, 0.9203, 0.6708, 0.7361, 1.0640, 0.8118, 0.6174, 0.5558, 0.8417,\n",
      "        0.6680, 0.9247, 0.7078, 0.7481, 1.1279, 0.8672, 0.5311, 0.6791, 0.7810,\n",
      "        0.7614, 0.8112, 0.6196, 1.3703, 1.0522, 0.8322, 0.5658, 0.9183, 0.6706,\n",
      "        0.7345, 1.0642, 0.8120, 1.0430, 0.5584, 1.1759, 0.5462, 1.6929, 1.3348,\n",
      "        1.3430, 0.6422, 1.0196, 0.7003, 0.6111, 1.1428, 0.5457, 1.8684, 1.4894,\n",
      "        1.0226, 0.7943, 0.6853, 1.5015, 1.3604, 0.6969], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n",
      "Loss Triangular: 0.0\n",
      "nodeInsDel: 0.22731469571590424\n",
      "node Subst: 0.3518371880054474\n",
      "edge_costs :\n",
      "tensor([[0.0000, 0.1046, 0.0251],\n",
      "        [0.1046, 0.0000, 0.0001],\n",
      "        [0.0251, 0.0001, 0.0000]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "edgeInsDel: 0.29098764061927795\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-58eca065da1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnodeSubst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInsDel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0medgeSub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_plt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mnodeSubst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInsDel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0medgeSub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_plt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-58eca065da1b>\u001b[0m in \u001b[0;36mclassification\u001b[0;34m(model, data, yt)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Zero gradients, perform a backward pass, and update the weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnodeSubst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInsDel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0medgeSub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_plt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from triangular_losses import ReducedTriangularConstraint as triangular_constraint\n",
    "model.to(device)\n",
    "def classification(model,data,yt,nb_iter):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    criterion = torch.nn.HingeEmbeddingLoss(margin=1.0)\n",
    "    criterionTri=triangular_constraint()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    print(device)\n",
    "\n",
    "#torch.cat((same_class[0:20],diff_class[0:20]),0).to(device)\n",
    "    input=data.to(device)\n",
    "    target=yt.to(device) \n",
    "#torch.ones(40,device=device)\n",
    "#target[20:]=-1.0\n",
    "#target=(yt[0:20]).to(device)\n",
    "    InsDel=np.empty((nb_iter,2))\n",
    "    nodeSubst=np.empty(nb_iter)\n",
    "    node_subst,nodeInsDel,edge_costs,edgeInsDel=model.from_weighs_to_costs()\n",
    "    edgeSub=np.empty((nb_iter,int(edge_costs.shape[0]*(edge_costs.shape[0]-1)/2)))\n",
    "    loss_plt=np.empty(nb_iter)\n",
    "    for t in range(nb_iter):    \n",
    "        # Forward pass: Compute predicted y by passing data to the model    \n",
    "\n",
    "\n",
    "        y_pred = model(input).to(device)\n",
    "    \n",
    "    \n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, target).to(device)    \n",
    "        node_subst,nodeInsDel,edge_costs,edgeInsDel=model.from_weighs_to_costs()\n",
    "        triangularInq=criterionTri(node_subst,nodeInsDel,edge_costs,edgeInsDel)\n",
    "        loss=loss*(1+triangularInq)\n",
    "        loss.to(device)\n",
    "        InsDel[t][0]=nodeInsDel.item()\n",
    "        InsDel[t][1]=edgeInsDel.item()\n",
    "        loss_plt[t]=loss.item()\n",
    "        nodeSubst[t]=node_subst.item()\n",
    "        k=0\n",
    "        \n",
    "        for p in range(edge_costs.shape[0]):\n",
    "            for q in range(p+1,edge_costs.shape[0]):\n",
    "                edgeSub[t][k]=edge_costs[p][q]\n",
    "                k=k+1\n",
    "        print(t, loss.item()) \n",
    "        if t % 100 == 99 or t==0:                           \n",
    "            print('Distances: ',y_pred)\n",
    "            print('Loss Triangular:',triangularInq.item())\n",
    "            print('nodeInsDel:',nodeInsDel.item())\n",
    "            print('node Subst:',node_subst.item())\n",
    "            print('edge_costs :')\n",
    "            print(edge_costs)\n",
    "            print('edgeInsDel:',edgeInsDel.item())\n",
    "        \n",
    "        \n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return nodeSubst,InsDel,edgeSub,loss_plt\n",
    "nb_iter=100\n",
    "nodeSubst,InsDel,edgeSub,loss_plt=classification(model,data,yt,nb_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(0)\n",
    "plt.plot(InsDel[0:500,0],label=\"node\")\n",
    "plt.plot(InsDel[0:500,1],label=\"edge\")\n",
    "plt.title('Node/Edge insertion/deletion costs')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(nodeSubst,label=\"node subst\")\n",
    "plt.title('Node substitution costs')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(2)\n",
    "for k in range(edgeSub.shape[1]):\n",
    "    plt.plot(edgeSub[0:500,k])\n",
    "plt.title('Edge Substitutions costs')\n",
    "plt.figure(3)\n",
    "plt.plot(loss_plt)\n",
    "plt.title('Evolution of the loss')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from models import GCN\n",
    "from utils import load_MAO\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCN(3,\n",
    "            nhid=10,\n",
    "            nclass=2,\n",
    "            dropout=0.1)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "inputs, adjs, t_classes = load_MAO()  # dataset, n_batch=1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "min_loss = 10000\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(10000):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    acc = 0\n",
    "    data = list(zip(inputs, adjs, t_classes))\n",
    "    random.shuffle(data)\n",
    "    for X, adj, y in data:\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        label = torch.tensor([y]).long()\n",
    "        # zero the parameter gradients\n",
    "\n",
    "        p = torch.randperm(30)\n",
    "        X = X[p, :]\n",
    "        adj = adj[p, :][:, p]\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(X, adj)\n",
    "        outputs = outputs.reshape(1, -1)\n",
    "        loss = criterion(outputs, label)\n",
    "        pred = outputs.argmax().item()\n",
    "        # print(pred)\n",
    "        if (pred == y):\n",
    "            acc = acc + 1\n",
    "\n",
    "        if (i % 10 == 0):\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        i = i + 1\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    # if epoch == 3000:\n",
    "    #     for param_group in optimizer.param_groups:\n",
    "    #         param_group['lr'] = 0.00001\n",
    "\n",
    "    #optimizer = adjust_learning_rate(optimizer, epoch)\n",
    "    cur_loss = running_loss/len(inputs)\n",
    "    if (cur_loss < min_loss):\n",
    "        min_loss = cur_loss\n",
    "        PATH = './weights/optimal_net.pth'\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "    cur_acc = acc/len(t_classes)\n",
    "    if (cur_acc > best_acc):\n",
    "        best_acc = cur_acc\n",
    "    if epoch % 1000 ==0:\n",
    "        print(f\"Epoch {epoch}, loss: {cur_loss}, acc : {acc/len(t_classes)}\")\n",
    "\n",
    "print(\n",
    "    f\"Finished Training, best acc achieved : {best_acc}, best loss : {min_loss}\")\n",
    "\n",
    "PATH = './weights/my_net.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (data[0])\n",
    "model2 = GCN2(3,\n",
    "            nhid=10,\n",
    "            nclass=2,\n",
    "            dropout=0.1)\n",
    "X,adj,y=data[0]\n",
    "output=model2(X,adj)\n",
    "print(len(data))\n",
    "print(adj)\n",
    "print('X=',X)\n",
    "print('output=',output)\n",
    "#print(model(X,adj),model(X,adj).reshape(1,-1),y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
