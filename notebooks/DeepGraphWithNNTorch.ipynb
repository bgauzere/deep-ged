{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPUtil\n",
    "from gklearn.utils.graphfiles import loadDataset\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "#matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import deepged.svd as svd\n",
    "import deepged.rings as rings\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from deepged.triangular_losses import TriangularConstraint as triangular_constraint\n",
    "import torch.autograd.profiler as profiler\n",
    "from deepged.data_manager.label_manager import compute_extended_labels\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "#We use a gpu \n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Getting the GPU status :\n",
    "GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Gs =  20\n"
     ]
    }
   ],
   "source": [
    "def label_to_color(label):\n",
    "    if label == 'C':\n",
    "        return 0.1\n",
    "    elif label == 'O':\n",
    "        return 0.8\n",
    "    \n",
    "def nodes_to_color_sequence(G):\n",
    "    return [label_to_color(c[1]['label'][0]) for c in G.nodes(data=True)]\n",
    "\n",
    "# Loading the dataset :\n",
    "Gs,y = loadDataset('../data/MAO/dataset.ds')\n",
    "Gs=Gs[:20]\n",
    "y=y[:20]\n",
    "\n",
    "print(\"Length of Gs = \",len(Gs))\n",
    "#print('edge max label',max(max([[G[e[0]][e[1]]['bond_type'] for e in G.edges()] for G in Gs])))\n",
    "\n",
    "for g in Gs:\n",
    "    compute_extended_labels(g,label_node='label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nG1 = Gs[1]\\nG2 = Gs[9]\\n\\nplt.figure(0)\\nnx.draw_networkx(G1,with_labels=True,node_color = nodes_to_color_sequence(G1),cmap='autumn')\\n\\nplt.figure(1)\\nnx.draw_networkx(G2,with_labels=True,node_color = nodes_to_color_sequence(G2),cmap='autumn')\\n\\nplt.show()\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "G1 = Gs[1]\n",
    "G2 = Gs[9]\n",
    "\n",
    "plt.figure(0)\n",
    "nx.draw_networkx(G1,with_labels=True,node_color = nodes_to_color_sequence(G1),cmap='autumn')\n",
    "\n",
    "plt.figure(1)\n",
    "nx.draw_networkx(G2,with_labels=True,node_color = nodes_to_color_sequence(G2),cmap='autumn')\n",
    "\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class Net representing the neural network :\n",
    "\n",
    "class Net(nn.Module):\n",
    "        \n",
    "    def __init__(self,GraphList,normalize=False,node_label='label'):\n",
    "        super(Net, self).__init__()   \n",
    "        self.normalize=normalize\n",
    "        self.node_label=node_label\n",
    "        dict,self.nb_edge_labels=self.build_node_dictionnary(GraphList)\n",
    "        self.nb_labels=len(dict)\n",
    "        print(\"nb_edge_labels = \",self.nb_edge_labels)\n",
    "        self.device= torch.device(\"cpu\")  #'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        nb_node_pair_label=self.nb_labels*(self.nb_labels-1)/2.0\n",
    "        nb_edge_pair_label=int(self.nb_edge_labels*(self.nb_edge_labels-1)/2)\n",
    "        \n",
    "        self.node_weighs=nn.Parameter(torch.tensor(1.0/(nb_node_pair_label+nb_edge_pair_label+2))+(1e-3)*torch.rand(int(self.nb_labels*(self.nb_labels-1)/2+1),requires_grad=True,device=self.device)) # all substitution costs+ nodeIns/Del. old version: 0 node subs, 1 nodeIns/Del, 2 : edgeSubs, 3 edgeIns/Del        \n",
    "        self.edge_weighs=nn.Parameter(torch.tensor(1.0/(nb_node_pair_label+nb_edge_pair_label+2))+(1e-3)*torch.rand(nb_edge_pair_label+1,requires_grad=True,device=self.device)) #edgeIns/Del\n",
    "        self.card=torch.tensor([G.order() for G in GraphList]).to(self.device)\n",
    "        card_max=self.card.max()\n",
    "        self.A=torch.zeros((len(GraphList),card_max*card_max),dtype=torch.int,device=self.device) \n",
    "        self.labels=torch.zeros((len(GraphList),card_max),dtype=torch.int,device=self.device) #node labels \n",
    "        for k in range(len(GraphList)):\n",
    "            A,l=self.from_networkx_to_tensor(GraphList[k],dict)             \n",
    "            self.A[k,0:A.shape[1]]=A[0]\n",
    "            self.labels[k,0:l.shape[0]]=l\n",
    "        print('adjacency matrices',self.A)\n",
    "        print('node labels',self.labels)\n",
    "        print('order of the graphs',self.card)\n",
    "        \n",
    "    # The forward pass of the neural network \n",
    "    def forward(self, input):  \n",
    "        self=self.to(self.device)\n",
    "        input=input.to(self.device)\n",
    "        ged=torch.zeros(len(input)).to(self.device) \n",
    "        node_costs,nodeInsDel,edge_costs,edgeInsDel=self.from_weighs_to_costs()\n",
    "        \n",
    "        # Here, we empty the cache so that the gpu doesn't keep all the information all along\n",
    "        torch.cuda.empty_cache()\n",
    "        GPUtil.showUtilization(all=True)\n",
    "        \n",
    "        for k in tqdm(range(len(input))): \n",
    "            g1=input[k][0].to(self.device)\n",
    "            g2=input[k][1].to(self.device)\n",
    "            n=self.card[g1]\n",
    "            m=self.card[g2]\n",
    "            \n",
    "            # Building the rings around every couple of graphs \n",
    "            self.ring_g,self.ring_h = rings.build_rings(g1,edgeInsDel.size()), rings.build_rings(g2,edgeInsDel.size()) \n",
    "            \n",
    "            # Constructing a cost matrix C for every couple of graphs, given the different costs \n",
    "            C=self.construct_cost_matrix(g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel) \n",
    "            \n",
    "            # Calculating an optimal mapping S based on the cost matrix C \n",
    "            S=self.mapping_from_cost_sans_FW(n,m,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)\n",
    "            #S=self.mapping_from_cost(C,n,m)   \n",
    "            #S=self.new_mapping_from_cost(C,n,m,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)\n",
    "            \n",
    "            # Flattenning S by reshaping it into a one-dimensional tensor\n",
    "            v=torch.flatten(S).to(self.device)\n",
    "            \n",
    "            # Detaching from the current graph, so that the result will never require gradient\n",
    "            S=S.detach()\n",
    "            normalize_factor=1.0\n",
    "            if self.normalize:\n",
    "                nb_edge1=(self.A[g1][0:n*n] != torch.zeros(n*n,device=self.device)).int().sum()\n",
    "                nb_edge2=(self.A[g2][0:m*m] != torch.zeros(m*m,device=self.device)).int().sum()\n",
    "                normalize_factor=nodeInsDel*(n+m)+edgeInsDel*(nb_edge1+nb_edge2)\n",
    "            \n",
    "            # Getting the main diagonal of the cost matrix C\n",
    "            c=torch.diag(C)\n",
    "            D=C-torch.eye(C.shape[0],device=self.device)*c\n",
    "            \n",
    "            # Calculating the optimal ged for every couple of graphs, based on c, v and D \n",
    "            ged[k]=(.5*v.T@D@v+c.T@v)/normalize_factor\n",
    "            \n",
    "            # We delete C after every iteration so that it's not kept in memory, because torch keeps it automatically\n",
    "            del C\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        max=torch.max(ged)\n",
    "        min=torch.min(ged)\n",
    "        ged=(ged-min)/(max-min)\n",
    "        \n",
    "        return ged\n",
    "    \n",
    "    def from_weighs_to_costs(self):\n",
    "        # We apply the ReLU (rectified linear unit) function element-wise\n",
    "        relu=torch.nn.ReLU()\n",
    "        cn=relu(self.node_weighs)\n",
    "        ce=relu(self.edge_weighs)\n",
    "        edgeInsDel=ce[-1]\n",
    "        \n",
    "        # Or we can use the exponential function\n",
    "        # Returns a new tensor with the exponential of the elements of the input tensor \n",
    "        '''\n",
    "        #cn=torch.exp(self.node_weighs)\n",
    "        #ce=torch.exp(self.edge_weighs)\n",
    "        cn=self.node_weighs*self.node_weighs\n",
    "        ce=self.edge_weighs*self.edge_weighs\n",
    "        total_cost=cn.sum()+ce.sum()\n",
    "        cn=cn/total_cost #/max\n",
    "        ce=ce/total_cost\n",
    "        edgeInsDel=ce[-1]\n",
    "        '''\n",
    "        \n",
    "        # Initialization of the node costs\n",
    "        node_costs=torch.zeros((self.nb_labels,self.nb_labels),device=self.device)\n",
    "        upper_part=torch.triu_indices(node_costs.shape[0],node_costs.shape[1],offset=1,device=self.device)        \n",
    "        node_costs[upper_part[0],upper_part[1]]=cn[0:-1]\n",
    "        node_costs=node_costs+node_costs.T\n",
    "\n",
    "        if self.nb_edge_labels>1:\n",
    "            edge_costs=torch.zeros((self.nb_edge_labels,self.nb_edge_labels),device=self.device)\n",
    "            upper_part=torch.triu_indices(edge_costs.shape[0],edge_costs.shape[1],offset=1,device=self.device)        \n",
    "            edge_costs[upper_part[0],upper_part[1]]=ce[0:-1]\n",
    "            edge_costs=edge_costs+edge_costs.T\n",
    "            del upper_part\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            edge_costs=torch.zeros(0,device=self.device)\n",
    "        \n",
    "        return node_costs,cn[-1],edge_costs,edgeInsDel\n",
    "    \n",
    "    # Extraction of all atom labels \n",
    "    def build_node_dictionnary(self,GraphList):\n",
    "        node_labels=[]\n",
    "        for G in Gs:\n",
    "            for v in nx.nodes(G):\n",
    "                if not G.nodes[v][self.node_label][0] in node_labels:\n",
    "                    node_labels.append(G.nodes[v][self.node_label][0])\n",
    "        node_labels.sort()\n",
    "        # Extraction of a dictionary allowing to number each label by a number.\n",
    "        dict={}\n",
    "        k=0\n",
    "        for label in node_labels:\n",
    "            dict[label]=k\n",
    "            k=k+1\n",
    "        print(\"node_labels : \",node_labels)\n",
    "    \n",
    "        return dict,max(max([[int(G[e[0]][e[1]]['bond_type']) for e in G.edges()] for G in GraphList]))\n",
    "    \n",
    "    # Transforming a networkx to a torch tensor\n",
    "    def from_networkx_to_tensor(self,G,dict):    \n",
    "        A=torch.tensor(nx.to_scipy_sparse_matrix(G,dtype=int,weight='bond_type').todense(),dtype=torch.int)        \n",
    "        lab=[dict[G.nodes[v][self.node_label][0]] for v in nx.nodes(G)]\n",
    "   \n",
    "        return (A.view(1,A.shape[0]*A.shape[1]),torch.tensor(lab))\n",
    "\n",
    "    # This function is used to construct a cost matrix C between two graphs g1 and g2, given the costs\n",
    "    def construct_cost_matrix(self,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel):\n",
    "        n = self.card[g1].item()\n",
    "        m = self.card[g2].item()\n",
    "        \n",
    "        # We use the no_grad to disable gradient calculation, that will reduce memory consumption \n",
    "        with torch.no_grad() : \n",
    "            A1=torch.zeros((n+1,n+1),dtype=torch.int,device=self.device)\n",
    "            A1[0:n,0:n]=self.A[g1][0:n*n].view(n,n)\n",
    "            A2=torch.zeros((m+1,m+1),dtype=torch.int,device=self.device)\n",
    "            A2[0:m,0:m]=self.A[g2][0:m*m].view(m,m)\n",
    "\n",
    "         # costs: 0 node subs, 1 nodeIns/Del, 2 : edgeSubs, 3 edgeIns/Del\n",
    "        \n",
    "        #C=cost[3]*torch.cat([torch.cat([C12[l][k] for k in range(n+1)],1) for l in range(n+1)])\n",
    "        C=edgeInsDel*self.matrix_edgeInsDel(A1,A2)\n",
    "        if self.nb_edge_labels>1:\n",
    "            for k in range(self.nb_edge_labels):\n",
    "                for l in range(self.nb_edge_labels):\n",
    "                    if k != l:\n",
    "                        C.add_(self.matrix_edgeSubst(A1,A2,k+1,l+1).multiply_(edge_costs[k][l]))\n",
    "                        C=C+edge_costs[k][l]*self.matrix_edgeSubst(A1,A2,k+1,l+1) \n",
    "        \n",
    "        l1=self.labels[g1][0:n]\n",
    "        l2=self.labels[g2][0:m]\n",
    "        D=torch.zeros((n+1)*(m+1),device=self.device)\n",
    "        D[n*(m+1):]=nodeInsDel\n",
    "        D[n*(m+1)+m]=0\n",
    "        D[[i*(m+1)+m for i in range(n)]]=nodeInsDel\n",
    "        for k in range(n*(m+1)):\n",
    "            if k%(m+1) != m:\n",
    "                D[k]=node_costs[l1[k//(m+1)]][l2[k%(m+1)]]\n",
    "        mask = torch.diag(torch.ones_like(D))\n",
    "        C=mask*torch.diag(D) #+ (1. - mask)*C\n",
    "\n",
    "        return C\n",
    "    \n",
    "    def matrix_edgeInsDel(self,A1,A2):\n",
    "        Abin1=(A1!=torch.zeros((A1.shape[0],A1.shape[1]),device=self.device))\n",
    "        Abin2=(A2!=torch.zeros((A2.shape[0],A2.shape[1]),device=self.device))\n",
    "        C1=torch.einsum('ij,kl->ijkl',torch.logical_not(Abin1),Abin2)\n",
    "        C2=torch.einsum('ij,kl->ijkl',Abin1,torch.logical_not(Abin2))\n",
    "        C12=torch.logical_or(C1,C2).int()\n",
    "        return torch.cat(torch.unbind(torch.cat(torch.unbind(C12,1),1),0),1)\n",
    "\n",
    "    def matrix_edgeSubst(self,A1,A2,lab1,lab2):\n",
    "        Abin1=(A1==lab1*torch.ones((A1.shape[0],A1.shape[1]),device=self.device)).int()\n",
    "        Abin2=(A2==lab2*torch.ones((A2.shape[0],A2.shape[1]),device=self.device)).int()\n",
    "        C=torch.einsum('ij,kl->ijkl',Abin1,Abin2)\n",
    "        return torch.cat(torch.unbind(torch.cat(torch.unbind(C,1),1),0),1).float()\n",
    "    \n",
    "    # ring_g, ring_h come from global ring with all graphs in so ring_g = rings['g'] and ring_h = rings['h']\n",
    "    def lsape_populate_instance(self,first_graph,second_graph,average_node_cost, average_edge_cost,alpha,lbda):\n",
    "        g,h = Gs[first_graph], Gs[second_graph]\n",
    "        self.average_cost =[average_node_cost, average_edge_cost]\n",
    "        self.first_graph, self.second_graph = first_graph,second_graph\n",
    "        \n",
    "        node_costs,nodeInsDel,edge_costs,edgeInsDel=self.from_weighs_to_costs()\n",
    "\n",
    "        lsape_instance = [[0 for _ in range(len(g) + 1)] for __ in range(len(h) + 1)]\n",
    "        for g_node_index in range(len(g) + 1):\n",
    "            for h_node_index in range(len(h) + 1):\n",
    "                lsape_instance[h_node_index][g_node_index] = rings.compute_ring_distance(g,h,self.ring_g,self.ring_h,g_node_index,h_node_index,alpha,lbda,node_costs,nodeInsDel,edge_costs,edgeInsDel,first_graph,second_graph)\n",
    "        for i in lsape_instance :\n",
    "            i = torch.as_tensor(i)\n",
    "        lsape_instance = torch.as_tensor(lsape_instance)\n",
    "        return lsape_instance\n",
    "    \n",
    "    # Calculating a mapping based on the cost matrix C, using the rings function and a derivable Hungarian approximation\n",
    "    def mapping_from_cost_sans_FW(self,n,m,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel): \n",
    "        c_0 =self.lsape_populate_instance(g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)\n",
    "        x0=svd.eps_assigment_from_mapping(torch.exp(-c_0),10).view((n+1)*(m+1),1)\n",
    "        return x0\n",
    "    \n",
    "    # Calculating a mapping based on the cost matrix C, not using the rings function and using a derivable Hungarian approximation\n",
    "    def new_mapping_from_cost(self,C,n,m,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel): \n",
    "        c=torch.diag(C)       \n",
    "        D=C-torch.eye(C.shape[0],device=self.device)*c\n",
    "        x0=svd.eps_assigment_from_mapping(torch.exp(-c),10).view((n+1)*(m+1),1)\n",
    "        return x0\n",
    "    \n",
    "    # Calculating a mapping based on the cost matrix C, not using the rings function and using the Frank Wolfe algorithm\n",
    "    def mapping_from_cost(self,C,n,m):\n",
    "        c=torch.diag(C)\n",
    "        D=C-torch.eye(C.shape[0],device=self.device)*c\n",
    "        x0=svd.eps_assigment_from_mapping(torch.exp(-.5*c.view(n+1,m+1)),10).view((n+1)*(m+1),1)\n",
    "        x=svd.franck_wolfe(x0,D,c,5,10,n,m)\n",
    "        def print_grad(grad):\n",
    "            if(grad.norm()!= 0.0):\n",
    "                print(grad)        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_labels :  ['C_1C', 'C_1C1C1N', 'C_1C1C2C', 'C_1C1N', 'C_1C1N2C', 'C_1C2C', 'C_1C3C', 'C_1N', 'C_2C', 'C_2C2C', 'C_3C', 'N_1C', 'N_1C1C', 'N_1C1C1C']\n",
      "nb_edge_labels =  3\n",
      "adjacency matrices tensor([[0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0]], dtype=torch.int32)\n",
      "node labels tensor([[12,  4,  5,  2,  5,  5,  5,  5,  4,  3, 11,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0],\n",
      "        [12,  4,  5,  2,  5,  5,  5,  5,  4,  3, 12,  7,  0,  0,  0,  0,  0,  0,\n",
      "          0],\n",
      "        [12,  4,  5,  2,  5,  5,  5,  5,  4,  3, 12,  1,  0,  0,  0,  0,  0,  0,\n",
      "          0],\n",
      "        [12,  4,  5,  2,  5,  5,  5,  5,  4,  3, 12,  3,  6, 10,  0,  0,  0,  0,\n",
      "          0],\n",
      "        [12,  4,  5,  2,  5,  5,  5,  5,  4,  3, 13,  3,  6, 10,  7,  0,  0,  0,\n",
      "          0],\n",
      "        [12,  4,  5,  2,  5,  5,  5,  5,  4,  3, 13,  3,  6, 10,  1,  0,  0,  0,\n",
      "          0],\n",
      "        [12,  4,  5,  2,  5,  5,  5,  5,  4,  3, 12,  3,  6,  6,  0,  0,  0,  0,\n",
      "          0],\n",
      "        [12,  4,  5,  2,  5,  5,  5,  5,  4,  3, 13,  3,  6,  6,  0,  7,  0,  0,\n",
      "          0],\n",
      "        [12,  4,  5,  2,  5,  5,  5,  5,  4,  3, 13,  3,  6,  6,  0,  3,  6,  6,\n",
      "          0],\n",
      "        [12,  4,  5,  2,  5,  5,  5,  5,  4,  3, 12,  3,  5,  9,  8,  0,  0,  0,\n",
      "          0],\n",
      "        [12,  4,  5,  2,  5,  5,  5,  5,  4,  3, 13,  3,  5,  9,  8,  7,  0,  0,\n",
      "          0],\n",
      "        [12,  4,  5,  2,  5,  5,  5,  5,  4,  3, 13,  3,  5,  9,  8,  3,  5,  9,\n",
      "          8],\n",
      "        [13,  4,  5,  2,  5,  5,  5,  5,  4,  3, 11,  7,  0,  0,  0,  0,  0,  0,\n",
      "          0],\n",
      "        [13,  4,  5,  2,  5,  5,  5,  5,  4,  3, 12,  7,  7,  0,  0,  0,  0,  0,\n",
      "          0],\n",
      "        [13,  4,  5,  2,  5,  5,  5,  5,  4,  3, 12,  3,  6, 10,  7,  0,  0,  0,\n",
      "          0],\n",
      "        [13,  4,  5,  2,  5,  5,  5,  5,  4,  3, 13,  3,  6, 10,  3,  6, 10,  7,\n",
      "          0],\n",
      "        [13,  4,  5,  2,  5,  5,  5,  5,  4,  3, 13,  3,  6, 10,  7,  7,  0,  0,\n",
      "          0],\n",
      "        [13,  4,  5,  2,  5,  5,  5,  5,  4,  3, 12,  3,  6,  6,  0,  7,  0,  0,\n",
      "          0],\n",
      "        [13,  4,  5,  2,  5,  5,  5,  5,  4,  3, 13,  3,  6,  6,  0,  7,  7,  0,\n",
      "          0],\n",
      "        [13,  4,  5,  2,  5,  5,  5,  5,  4,  3, 12,  3,  5,  9,  8,  7,  0,  0,\n",
      "          0]], dtype=torch.int32)\n",
      "order of the graphs tensor([11, 12, 14, 14, 15, 17, 15, 16, 19, 15, 16, 19, 12, 13, 15, 18, 16, 16,\n",
      "        17, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating our model, and sending it to the gpu\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "model = Net(Gs,normalize=True,node_label='extended_label')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates sets of couples of graphs from sets of graphs \n",
    "\n",
    "def creating_couples_after_splitting(train_D, valid_D,train_L,valid_L):\n",
    "    couples_train=[]\n",
    "    couples_test_train=[]\n",
    "    for i,g1_idx in enumerate(train_D): \n",
    "        for j,g2_idx in enumerate(train_D):\n",
    "            n=g1_idx\n",
    "            m=g2_idx\n",
    "            couples_train.append([n,m])\n",
    "    yt=np.ones(len(couples_train))\n",
    "    for k in couples_train:\n",
    "        if (y[k[0]]!=y[k[1]]):\n",
    "            yt[k]=-1.0  \n",
    "    for i,g1_idx in enumerate(valid_D):\n",
    "        for j,g2_idx in enumerate(train_D):\n",
    "            n=g1_idx\n",
    "            m=g2_idx\n",
    "            couples_test_train.append([n,m])\n",
    "            \n",
    "    yv=np.ones(len(couples_test_train))\n",
    "    for k in couples_test_train:\n",
    "        if (y[k[0]]!=y[k[1]]):\n",
    "            yv[k]=-1.0\n",
    "            \n",
    "    return torch.tensor(couples_train),yt,torch.tensor(couples_test_train),yv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying that the two sets contain different graphs\n",
    "\n",
    "def different_sets(my_train_D,my_valid_D): \n",
    "    cp=my_valid_D\n",
    "    for i in range(len(my_valid_D)):\n",
    "        if my_valid_D[i] in my_train_D:\n",
    "            tmp=random.choice(Gs)\n",
    "            if tmp not in my_train_D: \n",
    "                cp[i]=tmp\n",
    "    my_valid_D=cp\n",
    "    \n",
    "    return my_train_D,my_valid_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function splits the graph list Gs into two distinct sets of couples of graphs\n",
    "# One for training the model and one for testing it\n",
    "\n",
    "def splitting(Gs): \n",
    "    my_list=[i for i in range(len(Gs))] \n",
    "\n",
    "    [train_D, valid_D,train_L,valid_L]= train_test_split(my_list,y, test_size=0.20,train_size=0.80, shuffle=True, stratify=y) # we stratify so that y is used as the class labels\n",
    "    # We make sure that the two sets contain distinct graphs\n",
    "    train_D, valid_D=different_sets(train_D,valid_D)\n",
    "    \n",
    "    couples_train,yt,couples_test_train,yv = creating_couples_after_splitting(train_D, valid_D,train_L,valid_L)\n",
    "    yt=torch.tensor(yt)\n",
    "    yv=torch.tensor(yv)\n",
    "    DatasetTrain = TensorDataset(couples_train,yt) \n",
    "    DatasetValid=TensorDataset(couples_test_train, yv)\n",
    "\n",
    "    trainloader=torch.utils.data.DataLoader(DatasetTrain,batch_size=len(couples_train),shuffle=True,drop_last=True, num_workers=0) #128, len(couples_train)\n",
    "    validationloader=torch.utils.data.DataLoader(DatasetValid, batch_size=len(couples_test_train), drop_last=True,num_workers=0) #64,128,len(couples_test_train)\n",
    "    \n",
    "    print(len(trainloader),len(validationloader))\n",
    "    print(len(trainloader),len(validationloader))\n",
    "    \n",
    "    # We save our sets in pickle files\n",
    "    #torch.save(train_D, 'pickle_files/train_D', pickle_module=pkl) \n",
    "    #torch.save(valid_D, 'pickle_files/valid_D', pickle_module=pkl) \n",
    "    #torch.save(train_L, 'pickle_files/train_L', pickle_module=pkl) \n",
    "    #torch.save(valid_L, 'pickle_files/valid_L', pickle_module=pkl) \n",
    "    \n",
    "    return trainloader,validationloader,couples_train,yt,couples_test_train,yv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function used for classification problems\n",
    "\n",
    "def classification(model,Gs,nb_iter):\n",
    "    \n",
    "    trainloader,validationloader,couples_train,yt,couples_test_train,yv=splitting(Gs)\n",
    "    criterion = torch.nn.HingeEmbeddingLoss(margin=1.0,reduction='mean')\n",
    "    criterionTri=triangular_constraint()\n",
    "    optimizer = torch.optim.Adam(model.parameters()) #, lr=1e-3\n",
    "    \n",
    "    train_input=couples_train.to(device)\n",
    "    #valid_input=couples_test_train.to(device)\n",
    "    \n",
    "    target=yt.to(device) \n",
    "    InsDel=np.empty((nb_iter,2))\n",
    "    node_costs,nodeInsDel,edge_costs,edgeInsDel=model.from_weighs_to_costs()\n",
    "    nodeSub=np.empty((nb_iter,int(node_costs.shape[0]*(node_costs.shape[0]-1)/2)))\n",
    "    edgeSub=np.empty((nb_iter,int(edge_costs.shape[0]*(edge_costs.shape[0]-1)/2)))\n",
    "    loss_plt=np.empty(nb_iter)\n",
    "    loss_train_plt=np.empty(nb_iter)\n",
    "    loss_valid_plt=np.empty(nb_iter)\n",
    "    min_valid_loss = np.inf\n",
    "    iter_min_valid_loss = 0\n",
    "    \n",
    "    \n",
    "    for t in range(nb_iter):   \n",
    "        print(f\"epoch {t}\")\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        tmp=np.inf\n",
    "        \n",
    "        # The training part :\n",
    "        for train_data,train_labels in trainloader:\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad()\n",
    "            #inputt=train_data.to(device)\n",
    "            \n",
    "            # Forward pass: Compute predicted y by passing data to the model\n",
    "            y_pred = model(train_data).to(device)  \n",
    "\n",
    "            # Computing and printing loss\n",
    "            train_labels=train_labels.to(device)\n",
    "            loss = criterion(y_pred, train_labels).to(device)\n",
    "            node_costs,nodeInsDel,edge_costs,edgeInsDel=model.from_weighs_to_costs()\n",
    "            triangularInq=criterionTri(node_costs,nodeInsDel,edge_costs,edgeInsDel)\n",
    "            loss=loss*(1+triangularInq)\n",
    "            loss.to(device)\n",
    "            loss.backward()\n",
    "            loss=loss.detach()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            print('grad loss of the train = ', t, loss.grad)\n",
    "            print('loss.item of the train = ', t, loss.item())\n",
    "            train_loss =+ loss.item() #* train_data.size(0) \n",
    "            if (loss.item()<tmp): tmp=loss.item()\n",
    "                \n",
    "         # Getting the training loss\n",
    "        loss_plt[t]=loss.item()  \n",
    "        loss_train_plt[t]=train_loss /len(trainloader)\n",
    "        #loss_plt[t]=tmp\n",
    "          \n",
    "        # Getting the costs of the first iteration, to compare later\n",
    "        if t==0:\n",
    "            nodeInsDelInit=nodeInsDel\n",
    "            edgeInsDelInit=edgeInsDel\n",
    "            nodeSubInit=node_costs\n",
    "            edgeSubInit=edge_costs\n",
    "            torch.save(nodeInsDelInit, 'pickle_files/nodeInsDelInit', pickle_module=pkl) \n",
    "            torch.save(edgeInsDelInit, 'pickle_files/edgeInsDelInit', pickle_module=pkl) \n",
    "            torch.save(nodeSubInit, 'pickle_files/nodeSubInit', pickle_module=pkl) \n",
    "            torch.save(edgeSubInit, 'pickle_files/edgeSubInit', pickle_module=pkl) \n",
    "\n",
    "        # Getting some information every 100 iterations, to follow the evolution\n",
    "        if t % 100 == 99 or t==0:   \n",
    "            print('ged=',y_pred*target)  #train_labels\n",
    "            print('Distances: ',y_pred)\n",
    "            print('Loss Triangular:',triangularInq.item())\n",
    "            print('node_costs : \\n', node_costs)\n",
    "            print('nodeInsDel:',nodeInsDel.item())\n",
    "            print('edge_costs : \\n', edge_costs)\n",
    "            print('edgeInsDel:',edgeInsDel.item())\n",
    "        \n",
    "        print(f'Iteration {t+1} \\t\\t Training Loss: {train_loss / len(trainloader)}')\n",
    "        \n",
    "        # We delete to liberate some memory\n",
    "        del y_pred, train_loss,loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # The validation part :\n",
    "        for valid_data,valid_labels in validationloader:\n",
    "            inputt=valid_data.to(device)\n",
    "            y_pred = model(inputt).to(device)\n",
    "            # Compute and print loss\n",
    "            valid_labels=valid_labels.to(device)\n",
    "            loss = criterion(y_pred, valid_labels).to(device)    \n",
    "            loss.to(device)\n",
    "            print('loss.item of the valid = ', t, loss.item())  \n",
    "            \n",
    "            valid_loss = valid_loss + loss.item() #* valid_data.size(0)\n",
    "\n",
    "        # Getting the validation loss\n",
    "        loss_valid_plt[t]=valid_loss / len(validationloader)   \n",
    "        \n",
    "        # Getting edges and nodes Insertion/Deletion costs\n",
    "        InsDel[t][0]=nodeInsDel.item()\n",
    "        InsDel[t][1]=edgeInsDel.item()\n",
    "        \n",
    "        k=0\n",
    "        for p in range(node_costs.shape[0]):\n",
    "            for q in range(p+1,node_costs.shape[0]):\n",
    "                nodeSub[t][k]=node_costs[p][q]\n",
    "                k=k+1\n",
    "        k=0\n",
    "        for p in range(edge_costs.shape[0]):\n",
    "            for q in range(p+1,edge_costs.shape[0]):\n",
    "                edgeSub[t][k]=edge_costs[p][q]\n",
    "                k=k+1\n",
    "        \n",
    "        print(f'Iteration {t+1} \\t\\t Validation Loss: {valid_loss/len(validationloader)}')\n",
    "        if min_valid_loss > valid_loss:\n",
    "            print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f})')\n",
    "            min_valid_loss = valid_loss\n",
    "            iter_min_valid_loss = t\n",
    "            nodeSub_min = node_costs\n",
    "            edgeSub_min = edge_costs\n",
    "            nodeInsDel_min = nodeInsDel\n",
    "            edgeInsDel_min = edgeInsDel\n",
    "        \n",
    "        # We delete to liberate some memory\n",
    "        del valid_loss,loss\n",
    "        torch.cuda.empty_cache()                \n",
    "            \n",
    "    print('iter and min_valid_loss = ',iter_min_valid_loss, min_valid_loss)\n",
    "    print(' Min cost for nodeInsDel = ', nodeInsDel_min)\n",
    "    print(' Min cost for edgeInsDel = ', edgeInsDel_min)\n",
    "    print(' Min cost for nodeSub = ', nodeSub_min)\n",
    "    print(' Min cost for edgeSub = ', edgeSub_min)\n",
    "    # Saving the minimum costs into pickle files\n",
    "    torch.save(nodeInsDel_min, 'pickle_files/nodeInsDel_min', pickle_module=pkl) \n",
    "    torch.save(edgeInsDel_min, 'pickle_files/edgeInsDel_min', pickle_module=pkl) \n",
    "    torch.save(nodeSub_min, 'pickle_files/nodeSub_min', pickle_module=pkl) \n",
    "    torch.save(edgeSub_min, 'pickle_files/edgeSub_min', pickle_module=pkl)\n",
    "    return InsDel,nodeSub,edgeSub,loss_plt,loss_valid_plt,loss_train_plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "1 1\n",
      "1 1\n",
      "epoch 0\n",
      "| ID | Name | Serial | UUID || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:59<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad loss of the train =  0 None\n",
      "loss.item of the train =  0 0.44381392002105713\n",
      "ged= tensor([-0.4872, -0.6927, -0.3384,  0.4745, -0.3613, -0.1665, -0.4775, -0.2812,\n",
      "        -0.3666,  0.6997, -0.2924, -0.3110, -0.3401, -0.0575, -0.6307, -0.5376,\n",
      "        -0.9035, -0.3135,  0.4059,  0.3594,  0.4430,  0.4537,  0.4705,  0.4705,\n",
      "         0.5663,  0.5023,  0.3017,  0.3601,  0.6857,  0.7168,  0.6695,  0.4801,\n",
      "         0.3783,  0.4042,  0.5040,  0.4507,  0.0313,  0.3891,  0.8030,  0.4924,\n",
      "         0.0410,  0.2523,  0.3486,  0.3758,  0.4004,  0.4686,  0.4775,  0.3499,\n",
      "         0.1384,  0.3785,  0.4323,  0.4796,  0.3693,  0.6055,  0.4049,  0.3939,\n",
      "         0.3183,  0.3331,  0.3586,  0.0414,  0.2877,  0.2362,  0.5071,  0.4480,\n",
      "         0.4888,  0.3811,  0.3960,  0.2760,  0.4450,  0.3088,  0.5639,  0.3778,\n",
      "         0.2029,  0.8171,  0.1687,  0.3674,  0.4846,  0.4694,  0.3594,  0.4044,\n",
      "         0.4766,  0.5204,  0.5845,  0.3057,  0.4429,  0.4934,  0.4686,  0.1821,\n",
      "         0.4883,  0.4113,  0.8021,  0.2784,  1.0000,  0.5068,  0.4080,  0.5523,\n",
      "         0.4869,  0.3559,  0.4059,  0.6618,  0.5439,  0.2649,  0.7218,  0.6337,\n",
      "         0.6695,  0.3928,  0.2737,  0.1967,  0.4434,  0.3782,  0.4513,  0.4884,\n",
      "         0.5167,  0.4745,  0.5446,  0.4686,  0.4766,  0.5003,  0.3937,  0.5632,\n",
      "         0.2950,  0.3226,  0.5439,  0.3230,  0.5297,  0.3157,  0.3110,  0.4954,\n",
      "         0.4884,  0.2901,  0.3473,  0.2949,  0.4146,  0.2885,  0.5648,  0.3840,\n",
      "         0.6894,  0.4725,  0.5345,  0.2737,  0.4445,  0.3457,  0.4438,  0.6826,\n",
      "         0.3581,  0.3853,  0.0000,  0.2799,  0.4775,  0.4855,  0.5960,  0.8729,\n",
      "         0.4714,  0.4443,  0.3483,  0.5166,  0.3712,  0.3531,  0.5040,  0.7215,\n",
      "         0.5140,  0.3367,  0.5136,  0.7851,  0.4209,  0.3055,  0.3963,  0.3038,\n",
      "         0.3178,  0.6932,  0.4334,  0.5243,  0.4605,  0.4546,  0.3066,  0.2862,\n",
      "         0.4686,  0.4342,  0.3506,  0.4109,  0.9131,  0.3312,  0.5040,  0.4219,\n",
      "         0.3506,  0.0162,  0.3471,  0.7368,  0.6112,  0.4552,  0.4633,  0.6715,\n",
      "         0.3613,  0.3079,  0.1928,  0.4841,  0.4775,  0.2872,  0.4414,  0.4496,\n",
      "         0.3946,  0.3076,  0.3791,  0.2834,  0.3891,  0.4434,  0.3963,  0.0313,\n",
      "         0.3067,  0.4848,  0.4387,  0.5619,  0.5494,  0.4779,  0.3157,  0.4524,\n",
      "         0.3793,  0.9239,  0.3035,  0.5023,  0.3488,  0.3981,  0.0410,  0.3490,\n",
      "         0.6919,  0.5071,  0.4745,  0.4768,  0.3957,  0.4766,  0.4527,  0.4551,\n",
      "         0.5074,  0.4745,  0.3444,  0.5040,  0.4005,  0.5868,  0.3384,  0.4477,\n",
      "         0.6305,  0.3778,  0.4766,  0.4455,  0.4867,  0.4475,  0.4633,  0.5868,\n",
      "         0.4748,  0.3091,  0.4639,  0.3085,  0.5845,  0.3964,  0.4150,  0.3346],\n",
      "       dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "Distances:  tensor([0.4872, 0.6927, 0.3384, 0.4745, 0.3613, 0.1665, 0.4775, 0.2812, 0.3666,\n",
      "        0.6997, 0.2924, 0.3110, 0.3401, 0.0575, 0.6307, 0.5376, 0.9035, 0.3135,\n",
      "        0.4059, 0.3594, 0.4430, 0.4537, 0.4705, 0.4705, 0.5663, 0.5023, 0.3017,\n",
      "        0.3601, 0.6857, 0.7168, 0.6695, 0.4801, 0.3783, 0.4042, 0.5040, 0.4507,\n",
      "        0.0313, 0.3891, 0.8030, 0.4924, 0.0410, 0.2523, 0.3486, 0.3758, 0.4004,\n",
      "        0.4686, 0.4775, 0.3499, 0.1384, 0.3785, 0.4323, 0.4796, 0.3693, 0.6055,\n",
      "        0.4049, 0.3939, 0.3183, 0.3331, 0.3586, 0.0414, 0.2877, 0.2362, 0.5071,\n",
      "        0.4480, 0.4888, 0.3811, 0.3960, 0.2760, 0.4450, 0.3088, 0.5639, 0.3778,\n",
      "        0.2029, 0.8171, 0.1687, 0.3674, 0.4846, 0.4694, 0.3594, 0.4044, 0.4766,\n",
      "        0.5204, 0.5845, 0.3057, 0.4429, 0.4934, 0.4686, 0.1821, 0.4883, 0.4113,\n",
      "        0.8021, 0.2784, 1.0000, 0.5068, 0.4080, 0.5523, 0.4869, 0.3559, 0.4059,\n",
      "        0.6618, 0.5439, 0.2649, 0.7218, 0.6337, 0.6695, 0.3928, 0.2737, 0.1967,\n",
      "        0.4434, 0.3782, 0.4513, 0.4884, 0.5167, 0.4745, 0.5446, 0.4686, 0.4766,\n",
      "        0.5003, 0.3937, 0.5632, 0.2950, 0.3226, 0.5439, 0.3230, 0.5297, 0.3157,\n",
      "        0.3110, 0.4954, 0.4884, 0.2901, 0.3473, 0.2949, 0.4146, 0.2885, 0.5648,\n",
      "        0.3840, 0.6894, 0.4725, 0.5345, 0.2737, 0.4445, 0.3457, 0.4438, 0.6826,\n",
      "        0.3581, 0.3853, 0.0000, 0.2799, 0.4775, 0.4855, 0.5960, 0.8729, 0.4714,\n",
      "        0.4443, 0.3483, 0.5166, 0.3712, 0.3531, 0.5040, 0.7215, 0.5140, 0.3367,\n",
      "        0.5136, 0.7851, 0.4209, 0.3055, 0.3963, 0.3038, 0.3178, 0.6932, 0.4334,\n",
      "        0.5243, 0.4605, 0.4546, 0.3066, 0.2862, 0.4686, 0.4342, 0.3506, 0.4109,\n",
      "        0.9131, 0.3312, 0.5040, 0.4219, 0.3506, 0.0162, 0.3471, 0.7368, 0.6112,\n",
      "        0.4552, 0.4633, 0.6715, 0.3613, 0.3079, 0.1928, 0.4841, 0.4775, 0.2872,\n",
      "        0.4414, 0.4496, 0.3946, 0.3076, 0.3791, 0.2834, 0.3891, 0.4434, 0.3963,\n",
      "        0.0313, 0.3067, 0.4848, 0.4387, 0.5619, 0.5494, 0.4779, 0.3157, 0.4524,\n",
      "        0.3793, 0.9239, 0.3035, 0.5023, 0.3488, 0.3981, 0.0410, 0.3490, 0.6919,\n",
      "        0.5071, 0.4745, 0.4768, 0.3957, 0.4766, 0.4527, 0.4551, 0.5074, 0.4745,\n",
      "        0.3444, 0.5040, 0.4005, 0.5868, 0.3384, 0.4477, 0.6305, 0.3778, 0.4766,\n",
      "        0.4455, 0.4867, 0.4475, 0.4633, 0.5868, 0.4748, 0.3091, 0.4639, 0.3085,\n",
      "        0.5845, 0.3964, 0.4150, 0.3346], grad_fn=<DivBackward0>)\n",
      "Loss Triangular: 0.0\n",
      "node_costs : \n",
      " tensor([[0.0000, 0.0110, 0.0114, 0.0113, 0.0107, 0.0110, 0.0106, 0.0111, 0.0106,\n",
      "         0.0107, 0.0114, 0.0108, 0.0108, 0.0113],\n",
      "        [0.0110, 0.0000, 0.0109, 0.0113, 0.0113, 0.0105, 0.0111, 0.0106, 0.0110,\n",
      "         0.0113, 0.0106, 0.0110, 0.0111, 0.0113],\n",
      "        [0.0114, 0.0109, 0.0000, 0.0111, 0.0108, 0.0114, 0.0107, 0.0111, 0.0111,\n",
      "         0.0109, 0.0109, 0.0111, 0.0112, 0.0113],\n",
      "        [0.0113, 0.0113, 0.0111, 0.0000, 0.0109, 0.0112, 0.0105, 0.0111, 0.0109,\n",
      "         0.0109, 0.0105, 0.0105, 0.0106, 0.0112],\n",
      "        [0.0107, 0.0113, 0.0108, 0.0109, 0.0000, 0.0112, 0.0110, 0.0112, 0.0108,\n",
      "         0.0109, 0.0111, 0.0106, 0.0110, 0.0111],\n",
      "        [0.0110, 0.0105, 0.0114, 0.0112, 0.0112, 0.0000, 0.0108, 0.0105, 0.0112,\n",
      "         0.0105, 0.0112, 0.0109, 0.0108, 0.0112],\n",
      "        [0.0106, 0.0111, 0.0107, 0.0105, 0.0110, 0.0108, 0.0000, 0.0109, 0.0108,\n",
      "         0.0107, 0.0110, 0.0105, 0.0112, 0.0104],\n",
      "        [0.0111, 0.0106, 0.0111, 0.0111, 0.0112, 0.0105, 0.0109, 0.0000, 0.0110,\n",
      "         0.0104, 0.0109, 0.0104, 0.0109, 0.0114],\n",
      "        [0.0106, 0.0110, 0.0111, 0.0109, 0.0108, 0.0112, 0.0108, 0.0110, 0.0000,\n",
      "         0.0114, 0.0113, 0.0108, 0.0108, 0.0109],\n",
      "        [0.0107, 0.0113, 0.0109, 0.0109, 0.0109, 0.0105, 0.0107, 0.0104, 0.0114,\n",
      "         0.0000, 0.0107, 0.0113, 0.0110, 0.0109],\n",
      "        [0.0114, 0.0106, 0.0109, 0.0105, 0.0111, 0.0112, 0.0110, 0.0109, 0.0113,\n",
      "         0.0107, 0.0000, 0.0112, 0.0110, 0.0113],\n",
      "        [0.0108, 0.0110, 0.0111, 0.0105, 0.0106, 0.0109, 0.0105, 0.0104, 0.0108,\n",
      "         0.0113, 0.0112, 0.0000, 0.0110, 0.0113],\n",
      "        [0.0108, 0.0111, 0.0112, 0.0106, 0.0110, 0.0108, 0.0112, 0.0109, 0.0108,\n",
      "         0.0110, 0.0110, 0.0110, 0.0000, 0.0114],\n",
      "        [0.0113, 0.0113, 0.0113, 0.0112, 0.0111, 0.0112, 0.0104, 0.0114, 0.0109,\n",
      "         0.0109, 0.0113, 0.0113, 0.0114, 0.0000]], grad_fn=<AddBackward0>)\n",
      "nodeInsDel: 0.011243173852562904\n",
      "edge_costs : \n",
      " tensor([[0.0000, 0.0111, 0.0114],\n",
      "        [0.0111, 0.0000, 0.0114],\n",
      "        [0.0114, 0.0114, 0.0000]], grad_fn=<AddBackward0>)\n",
      "edgeInsDel: 0.01060066744685173\n",
      "Iteration 1 \t\t Training Loss: 0.44381392002105713\n",
      "| ID | Name | Serial | UUID || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [00:15<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item of the valid =  0 0.5016830563545227\n",
      "Iteration 1 \t\t Validation Loss: 0.5016830563545227\n",
      "Validation Loss Decreased(inf--->0.501683)\n",
      "epoch 1\n",
      "| ID | Name | Serial | UUID || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:59<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad loss of the train =  1 None\n",
      "loss.item of the train =  1 0.39528271555900574\n",
      "Iteration 2 \t\t Training Loss: 0.39528271555900574\n",
      "| ID | Name | Serial | UUID || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [00:16<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item of the valid =  1 0.523048460483551\n",
      "Iteration 2 \t\t Validation Loss: 0.523048460483551\n",
      "epoch 2\n",
      "| ID | Name | Serial | UUID || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:00<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad loss of the train =  2 None\n",
      "loss.item of the train =  2 0.33250582218170166\n",
      "Iteration 3 \t\t Training Loss: 0.33250582218170166\n",
      "| ID | Name | Serial | UUID || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [00:15<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item of the valid =  2 0.5264009237289429\n",
      "Iteration 3 \t\t Validation Loss: 0.5264009237289429\n",
      "epoch 3\n",
      "| ID | Name | Serial | UUID || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:01<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad loss of the train =  3 None\n",
      "loss.item of the train =  3 0.31230542063713074\n",
      "Iteration 4 \t\t Training Loss: 0.31230542063713074\n",
      "| ID | Name | Serial | UUID || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [00:14<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item of the valid =  3 0.5194927453994751\n",
      "Iteration 4 \t\t Validation Loss: 0.5194927453994751\n",
      "epoch 4\n",
      "| ID | Name | Serial | UUID || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [01:01<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad loss of the train =  4 None\n",
      "loss.item of the train =  4 0.2855408489704132\n",
      "Iteration 5 \t\t Training Loss: 0.2855408489704132\n",
      "| ID | Name | Serial | UUID || GPU temp. | GPU util. | Memory util. || Memory total | Memory used | Memory free || Display mode | Display active |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [00:14<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item of the valid =  4 0.5142555236816406\n",
      "Iteration 5 \t\t Validation Loss: 0.5142555236816406\n",
      "iter and min_valid_loss =  0 0.5016830563545227\n",
      " Min cost for nodeInsDel =  tensor(0.0112, grad_fn=<SelectBackward0>)\n",
      " Min cost for edgeInsDel =  tensor(0.0106, grad_fn=<SelectBackward0>)\n",
      " Min cost for nodeSub =  tensor([[0.0000, 0.0110, 0.0114, 0.0113, 0.0107, 0.0110, 0.0106, 0.0111, 0.0106,\n",
      "         0.0107, 0.0114, 0.0108, 0.0108, 0.0113],\n",
      "        [0.0110, 0.0000, 0.0109, 0.0113, 0.0113, 0.0105, 0.0111, 0.0106, 0.0110,\n",
      "         0.0113, 0.0106, 0.0110, 0.0111, 0.0113],\n",
      "        [0.0114, 0.0109, 0.0000, 0.0111, 0.0108, 0.0114, 0.0107, 0.0111, 0.0111,\n",
      "         0.0109, 0.0109, 0.0111, 0.0112, 0.0113],\n",
      "        [0.0113, 0.0113, 0.0111, 0.0000, 0.0109, 0.0112, 0.0105, 0.0111, 0.0109,\n",
      "         0.0109, 0.0105, 0.0105, 0.0106, 0.0112],\n",
      "        [0.0107, 0.0113, 0.0108, 0.0109, 0.0000, 0.0112, 0.0110, 0.0112, 0.0108,\n",
      "         0.0109, 0.0111, 0.0106, 0.0110, 0.0111],\n",
      "        [0.0110, 0.0105, 0.0114, 0.0112, 0.0112, 0.0000, 0.0108, 0.0105, 0.0112,\n",
      "         0.0105, 0.0112, 0.0109, 0.0108, 0.0112],\n",
      "        [0.0106, 0.0111, 0.0107, 0.0105, 0.0110, 0.0108, 0.0000, 0.0109, 0.0108,\n",
      "         0.0107, 0.0110, 0.0105, 0.0112, 0.0104],\n",
      "        [0.0111, 0.0106, 0.0111, 0.0111, 0.0112, 0.0105, 0.0109, 0.0000, 0.0110,\n",
      "         0.0104, 0.0109, 0.0104, 0.0109, 0.0114],\n",
      "        [0.0106, 0.0110, 0.0111, 0.0109, 0.0108, 0.0112, 0.0108, 0.0110, 0.0000,\n",
      "         0.0114, 0.0113, 0.0108, 0.0108, 0.0109],\n",
      "        [0.0107, 0.0113, 0.0109, 0.0109, 0.0109, 0.0105, 0.0107, 0.0104, 0.0114,\n",
      "         0.0000, 0.0107, 0.0113, 0.0110, 0.0109],\n",
      "        [0.0114, 0.0106, 0.0109, 0.0105, 0.0111, 0.0112, 0.0110, 0.0109, 0.0113,\n",
      "         0.0107, 0.0000, 0.0112, 0.0110, 0.0113],\n",
      "        [0.0108, 0.0110, 0.0111, 0.0105, 0.0106, 0.0109, 0.0105, 0.0104, 0.0108,\n",
      "         0.0113, 0.0112, 0.0000, 0.0110, 0.0113],\n",
      "        [0.0108, 0.0111, 0.0112, 0.0106, 0.0110, 0.0108, 0.0112, 0.0109, 0.0108,\n",
      "         0.0110, 0.0110, 0.0110, 0.0000, 0.0114],\n",
      "        [0.0113, 0.0113, 0.0113, 0.0112, 0.0111, 0.0112, 0.0104, 0.0114, 0.0109,\n",
      "         0.0109, 0.0113, 0.0113, 0.0114, 0.0000]], grad_fn=<AddBackward0>)\n",
      " Min cost for edgeSub =  tensor([[0.0000, 0.0111, 0.0114],\n",
      "        [0.0111, 0.0000, 0.0114],\n",
      "        [0.0114, 0.0114, 0.0000]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39618/1074445101.py:35: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "print(len(Gs))\n",
    "nb_iter=5\n",
    "\n",
    "InsDel, nodeSub,edgeSub,loss_plt,loss_valid_plt,loss_train_plt=classification(model,Gs,nb_iter)\n",
    "\n",
    "# Plotting Node/Edge insertion/deletion costs\n",
    "plt.figure(0)\n",
    "plt.plot(InsDel[0:nb_iter,0],label=\"node\")\n",
    "plt.plot(InsDel[0:nb_iter,1],label=\"edge\")\n",
    "plt.title('Node/Edge insertion/deletion costs')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting Node Substitutions costs\n",
    "plt.figure(1)\n",
    "for k in range(nodeSub.shape[1]):\n",
    "    plt.plot(nodeSub[0:nb_iter,k])\n",
    "plt.title('Node Substitutions costs')\n",
    "\n",
    "# Plotting Edge Substitutions costs\n",
    "plt.figure(2)\n",
    "for k in range(edgeSub.shape[1]):\n",
    "    plt.plot(edgeSub[0:nb_iter,k])\n",
    "plt.title('Edge Substitutions costs')\n",
    "\n",
    "# Plotting the evolution of the train loss\n",
    "plt.figure(3)\n",
    "plt.plot(loss_plt)\n",
    "plt.title('Evolution of the train loss (loss_plt)')\n",
    "\n",
    "# Plotting the evolution of the validation loss\n",
    "plt.figure(4)\n",
    "plt.plot(loss_valid_plt)\n",
    "plt.title('Evolution of the valid loss')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save the losses into pickle files\n",
    "torch.save(loss_plt, 'pickle_files/loss_plt', pickle_module=pkl) \n",
    "torch.save(loss_valid_plt, 'pickle_files/loss_valid_plt', pickle_module=pkl) \n",
    "torch.save(loss_train_plt, 'pickle_files/loss_train_plt', pickle_module=pkl) \n",
    "\n",
    "# We save the costs into pickle files\n",
    "torch.save(InsDel,'pickle_files/InsDel', pickle_module=pkl) \n",
    "torch.save(edgeSub,'pickle_files/edgeSub', pickle_module=pkl) \n",
    "torch.save(nodeSub,'pickle_files/nodeSub', pickle_module=pkl) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "A=torch.tensor(nx.to_scipy_sparse_matrix(Gs[0],dtype=int,weight='bond_type').todense(),dtype=torch.int) \n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the train and the valid losses in a same graphic, to compare their evolution\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "plt.plot(loss_plt, label='train loss')\n",
    "plt.plot(loss_valid_plt, label='valid loss')\n",
    "plt.title('Train and valid losses')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "card = torch.tensor([G.order() for G in Gs]).to(device)\n",
    "print(Gs[0].order())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
