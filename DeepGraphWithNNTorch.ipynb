{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%load_ext autoreload\r\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import GPUtil\r\n",
    "from gklearn.utils.graphfiles import loadDataset\r\n",
    "import networkx as nx\r\n",
    "import matplotlib\r\n",
    "#matplotlib.use('TkAgg')\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import numpy as np\r\n",
    "import pickle as pkl\r\n",
    "import svd\r\n",
    "import rings\r\n",
    "import random\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from torch.utils.data import DataLoader, TensorDataset\r\n",
    "from triangular_losses import TriangularConstraint as triangular_constraint\r\n",
    "import torch.autograd.profiler as profiler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#We use a gpu \r\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\r\n",
    "\r\n",
    "# Getting the GPU status :\r\n",
    "GPUtil.showUtilization()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def label_to_color(label):\r\n",
    "    if label == 'C':\r\n",
    "        return 0.1\r\n",
    "    elif label == 'O':\r\n",
    "        return 0.8\r\n",
    "    \r\n",
    "def nodes_to_color_sequence(G):\r\n",
    "    return [label_to_color(c[1]['label'][0]) for c in G.nodes(data=True)]\r\n",
    "\r\n",
    "# Loading the dataset :\r\n",
    "Gs,y = loadDataset('DeepGED/MAO/dataset.ds')\r\n",
    "\r\n",
    "print(\"Length of Gs = \",len(Gs))\r\n",
    "#print('edge max label',max(max([[G[e[0]][e[1]]['bond_type'] for e in G.edges()] for G in Gs])))\r\n",
    "\r\n",
    "import extended_label\r\n",
    "for g in Gs:\r\n",
    "    extended_label.compute_extended_labels(g)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\r\n",
    "G1 = Gs[1]\r\n",
    "G2 = Gs[9]\r\n",
    "\r\n",
    "plt.figure(0)\r\n",
    "nx.draw_networkx(G1,with_labels=True,node_color = nodes_to_color_sequence(G1),cmap='autumn')\r\n",
    "\r\n",
    "plt.figure(1)\r\n",
    "nx.draw_networkx(G2,with_labels=True,node_color = nodes_to_color_sequence(G2),cmap='autumn')\r\n",
    "\r\n",
    "plt.show()\r\n",
    "'''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The class Net representing the neural network :\r\n",
    "\r\n",
    "class Net(nn.Module):\r\n",
    "        \r\n",
    "    def __init__(self,GraphList,normalize=False,node_label='label'):\r\n",
    "        super(Net, self).__init__()   \r\n",
    "        self.normalize=normalize\r\n",
    "        self.node_label=node_label\r\n",
    "        dict,self.nb_edge_labels=self.build_node_dictionnary(GraphList)\r\n",
    "        self.nb_labels=len(dict)\r\n",
    "        print(\"nb_edge_labels = \",self.nb_edge_labels)\r\n",
    "        self.device= torch.device(\"cuda:0\")  #'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
    "        nb_node_pair_label=self.nb_labels*(self.nb_labels-1)/2.0\r\n",
    "        nb_edge_pair_label=int(self.nb_edge_labels*(self.nb_edge_labels-1)/2)\r\n",
    "        \r\n",
    "        self.node_weighs=nn.Parameter(torch.tensor(1.0/(nb_node_pair_label+nb_edge_pair_label+2))+(1e-3)*torch.rand(int(self.nb_labels*(self.nb_labels-1)/2+1),requires_grad=True,device=self.device)) # all substitution costs+ nodeIns/Del. old version: 0 node subs, 1 nodeIns/Del, 2 : edgeSubs, 3 edgeIns/Del        \r\n",
    "        self.edge_weighs=nn.Parameter(torch.tensor(1.0/(nb_node_pair_label+nb_edge_pair_label+2))+(1e-3)*torch.rand(nb_edge_pair_label+1,requires_grad=True,device=self.device)) #edgeIns/Del\r\n",
    "        self.card=torch.tensor([G.order() for G in GraphList]).to(self.device)\r\n",
    "        card_max=self.card.max()\r\n",
    "        self.A=torch.zeros((len(GraphList),card_max*card_max),dtype=torch.int,device=self.device) \r\n",
    "        self.labels=torch.zeros((len(GraphList),card_max),dtype=torch.int,device=self.device) #node labels \r\n",
    "        for k in range(len(GraphList)):\r\n",
    "            A,l=self.from_networkx_to_tensor(GraphList[k],dict)             \r\n",
    "            self.A[k,0:A.shape[1]]=A[0]\r\n",
    "            self.labels[k,0:l.shape[0]]=l\r\n",
    "        print('adjacency matrices',self.A)\r\n",
    "        print('node labels',self.labels)\r\n",
    "        print('order of the graphs',self.card)\r\n",
    "        \r\n",
    "    # The forward pass of the neural network \r\n",
    "    def forward(self, input):  \r\n",
    "        self=self.to(self.device)\r\n",
    "        input=input.to(self.device)\r\n",
    "        ged=torch.zeros(len(input)).to(self.device) \r\n",
    "        node_costs,nodeInsDel,edge_costs,edgeInsDel=self.from_weighs_to_costs()\r\n",
    "        \r\n",
    "        # Here, we empty the cache so that the gpu doesn't keep all the information all along\r\n",
    "        torch.cuda.empty_cache()\r\n",
    "        GPUtil.showUtilization(all=True)\r\n",
    "        \r\n",
    "        for k in range(len(input)): \r\n",
    "            g1=input[k][0].to(self.device)\r\n",
    "            g2=input[k][1].to(self.device)\r\n",
    "            n=self.card[g1]\r\n",
    "            m=self.card[g2]\r\n",
    "            \r\n",
    "            # Building the rings around every couple of graphs \r\n",
    "            self.ring_g,self.ring_h = rings.build_rings(g1,edgeInsDel.size()), rings.build_rings(g2,edgeInsDel.size()) \r\n",
    "            \r\n",
    "            # Constructing a cost matrix C for every couple of graphs, given the different costs \r\n",
    "            C=self.construct_cost_matrix(g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel) \r\n",
    "            \r\n",
    "            # Calculating an optimal mapping S based on the cost matrix C \r\n",
    "            S=self.mapping_from_cost_sans_FW(n,m,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)\r\n",
    "            #S=self.mapping_from_cost(C,n,m)   \r\n",
    "            #S=self.new_mapping_from_cost(C,n,m,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)\r\n",
    "            \r\n",
    "            # Flattenning S by reshaping it into a one-dimensional tensor\r\n",
    "            v=torch.flatten(S).to(self.device)\r\n",
    "            \r\n",
    "            # Detaching from the current graph, so that the result will never require gradient\r\n",
    "            S=S.detach()\r\n",
    "            normalize_factor=1.0\r\n",
    "            if self.normalize:\r\n",
    "                nb_edge1=(self.A[g1][0:n*n] != torch.zeros(n*n,device=self.device)).int().sum()\r\n",
    "                nb_edge2=(self.A[g2][0:m*m] != torch.zeros(m*m,device=self.device)).int().sum()\r\n",
    "                normalize_factor=nodeInsDel*(n+m)+edgeInsDel*(nb_edge1+nb_edge2)\r\n",
    "            \r\n",
    "            # Getting the main diagonal of the cost matrix C\r\n",
    "            c=torch.diag(C)\r\n",
    "            D=C-torch.eye(C.shape[0],device=self.device)*c\r\n",
    "            \r\n",
    "            # Calculating the optimal ged for every couple of graphs, based on c, v and D \r\n",
    "            ged[k]=(.5*v.T@D@v+c.T@v)/normalize_factor\r\n",
    "            \r\n",
    "            # We delete C after every iteration so that it's not kept in memory, because torch keeps it automatically\r\n",
    "            del C\r\n",
    "            torch.cuda.empty_cache()\r\n",
    "\r\n",
    "        max=torch.max(ged)\r\n",
    "        min=torch.min(ged)\r\n",
    "        ged=(ged-min)/(max-min)\r\n",
    "        \r\n",
    "        return ged\r\n",
    "    \r\n",
    "    def from_weighs_to_costs(self):\r\n",
    "        # We apply the ReLU (rectified linear unit) function element-wise\r\n",
    "        relu=torch.nn.ReLU()\r\n",
    "        cn=relu(self.node_weighs)\r\n",
    "        ce=relu(self.edge_weighs)\r\n",
    "        edgeInsDel=ce[-1]\r\n",
    "        \r\n",
    "        # Or we can use the exponential function\r\n",
    "        # Returns a new tensor with the exponential of the elements of the input tensor \r\n",
    "        '''\r\n",
    "        #cn=torch.exp(self.node_weighs)\r\n",
    "        #ce=torch.exp(self.edge_weighs)\r\n",
    "        cn=self.node_weighs*self.node_weighs\r\n",
    "        ce=self.edge_weighs*self.edge_weighs\r\n",
    "        total_cost=cn.sum()+ce.sum()\r\n",
    "        cn=cn/total_cost #/max\r\n",
    "        ce=ce/total_cost\r\n",
    "        edgeInsDel=ce[-1]\r\n",
    "        '''\r\n",
    "        \r\n",
    "        # Initialization of the node costs\r\n",
    "        node_costs=torch.zeros((self.nb_labels,self.nb_labels),device=self.device)\r\n",
    "        upper_part=torch.triu_indices(node_costs.shape[0],node_costs.shape[1],offset=1,device=self.device)        \r\n",
    "        node_costs[upper_part[0],upper_part[1]]=cn[0:-1]\r\n",
    "        node_costs=node_costs+node_costs.T\r\n",
    "\r\n",
    "        if self.nb_edge_labels>1:\r\n",
    "            edge_costs=torch.zeros((self.nb_edge_labels,self.nb_edge_labels),device=self.device)\r\n",
    "            upper_part=torch.triu_indices(edge_costs.shape[0],edge_costs.shape[1],offset=1,device=self.device)        \r\n",
    "            edge_costs[upper_part[0],upper_part[1]]=ce[0:-1]\r\n",
    "            edge_costs=edge_costs+edge_costs.T\r\n",
    "            del upper_part\r\n",
    "            torch.cuda.empty_cache()\r\n",
    "        else:\r\n",
    "            edge_costs=torch.zeros(0,device=self.device)\r\n",
    "        \r\n",
    "        return node_costs,cn[-1],edge_costs,edgeInsDel\r\n",
    "    \r\n",
    "    # Extraction of all atom labels \r\n",
    "    def build_node_dictionnary(self,GraphList):\r\n",
    "        node_labels=[]\r\n",
    "        for G in Gs:\r\n",
    "            for v in nx.nodes(G):\r\n",
    "                if not G.nodes[v][self.node_label][0] in node_labels:\r\n",
    "                    node_labels.append(G.nodes[v][self.node_label][0])\r\n",
    "        node_labels.sort()\r\n",
    "        # Extraction of a dictionary allowing to number each label by a number.\r\n",
    "        dict={}\r\n",
    "        k=0\r\n",
    "        for label in node_labels:\r\n",
    "            dict[label]=k\r\n",
    "            k=k+1\r\n",
    "        print(\"node_labels : \",node_labels)\r\n",
    "    \r\n",
    "        return dict,max(max([[int(G[e[0]][e[1]]['bond_type']) for e in G.edges()] for G in GraphList]))\r\n",
    "    \r\n",
    "    # Transforming a networkx to a torch tensor\r\n",
    "    def from_networkx_to_tensor(self,G,dict):    \r\n",
    "        A=torch.tensor(nx.to_scipy_sparse_matrix(G,dtype=int,weight='bond_type').todense(),dtype=torch.int)        \r\n",
    "        lab=[dict[G.nodes[v][self.node_label][0]] for v in nx.nodes(G)]\r\n",
    "   \r\n",
    "        return (A.view(1,A.shape[0]*A.shape[1]),torch.tensor(lab))\r\n",
    "\r\n",
    "    # This function is used to construct a cost matrix C between two graphs g1 and g2, given the costs\r\n",
    "    def construct_cost_matrix(self,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel):\r\n",
    "        n = self.card[g1].item()\r\n",
    "        m = self.card[g2].item()\r\n",
    "        \r\n",
    "        # We use the no_grad to disable gradient calculation, that will reduce memory consumption \r\n",
    "        with torch.no_grad() : \r\n",
    "            A1=torch.zeros((n+1,n+1),dtype=torch.int,device=self.device)\r\n",
    "            A1[0:n,0:n]=self.A[g1][0:n*n].view(n,n)\r\n",
    "            A2=torch.zeros((m+1,m+1),dtype=torch.int,device=self.device)\r\n",
    "            A2[0:m,0:m]=self.A[g2][0:m*m].view(m,m)\r\n",
    "\r\n",
    "         # costs: 0 node subs, 1 nodeIns/Del, 2 : edgeSubs, 3 edgeIns/Del\r\n",
    "        \r\n",
    "        #C=cost[3]*torch.cat([torch.cat([C12[l][k] for k in range(n+1)],1) for l in range(n+1)])\r\n",
    "        C=edgeInsDel*self.matrix_edgeInsDel(A1,A2)\r\n",
    "        if self.nb_edge_labels>1:\r\n",
    "            for k in range(self.nb_edge_labels):\r\n",
    "                for l in range(self.nb_edge_labels):\r\n",
    "                    if k != l:\r\n",
    "                        C.add_(self.matrix_edgeSubst(A1,A2,k+1,l+1).multiply_(edge_costs[k][l]))\r\n",
    "                        C=C+edge_costs[k][l]*self.matrix_edgeSubst(A1,A2,k+1,l+1) \r\n",
    "        \r\n",
    "        l1=self.labels[g1][0:n]\r\n",
    "        l2=self.labels[g2][0:m]\r\n",
    "        D=torch.zeros((n+1)*(m+1),device=self.device)\r\n",
    "        D[n*(m+1):]=nodeInsDel\r\n",
    "        D[n*(m+1)+m]=0\r\n",
    "        D[[i*(m+1)+m for i in range(n)]]=nodeInsDel\r\n",
    "        for k in range(n*(m+1)):\r\n",
    "            if k%(m+1) != m:\r\n",
    "                D[k]=node_costs[l1[k//(m+1)]][l2[k%(m+1)]]\r\n",
    "        mask = torch.diag(torch.ones_like(D))\r\n",
    "        C=mask*torch.diag(D) #+ (1. - mask)*C\r\n",
    "\r\n",
    "        return C\r\n",
    "    \r\n",
    "    def matrix_edgeInsDel(self,A1,A2):\r\n",
    "        Abin1=(A1!=torch.zeros((A1.shape[0],A1.shape[1]),device=self.device))\r\n",
    "        Abin2=(A2!=torch.zeros((A2.shape[0],A2.shape[1]),device=self.device))\r\n",
    "        C1=torch.einsum('ij,kl->ijkl',torch.logical_not(Abin1),Abin2)\r\n",
    "        C2=torch.einsum('ij,kl->ijkl',Abin1,torch.logical_not(Abin2))\r\n",
    "        C12=torch.logical_or(C1,C2).int()\r\n",
    "        return torch.cat(torch.unbind(torch.cat(torch.unbind(C12,1),1),0),1)\r\n",
    "\r\n",
    "    def matrix_edgeSubst(self,A1,A2,lab1,lab2):\r\n",
    "        Abin1=(A1==lab1*torch.ones((A1.shape[0],A1.shape[1]),device=self.device)).int()\r\n",
    "        Abin2=(A2==lab2*torch.ones((A2.shape[0],A2.shape[1]),device=self.device)).int()\r\n",
    "        C=torch.einsum('ij,kl->ijkl',Abin1,Abin2)\r\n",
    "        return torch.cat(torch.unbind(torch.cat(torch.unbind(C,1),1),0),1).float()\r\n",
    "    \r\n",
    "    # ring_g, ring_h come from global ring with all graphs in so ring_g = rings['g'] and ring_h = rings['h']\r\n",
    "    def lsape_populate_instance(self,first_graph,second_graph,average_node_cost, average_edge_cost,alpha,lbda):\r\n",
    "        g,h = Gs[first_graph], Gs[second_graph]\r\n",
    "        self.average_cost =[average_node_cost, average_edge_cost]\r\n",
    "        self.first_graph, self.second_graph = first_graph,second_graph\r\n",
    "        \r\n",
    "        node_costs,nodeInsDel,edge_costs,edgeInsDel=self.from_weighs_to_costs()\r\n",
    "\r\n",
    "        lsape_instance = [[0 for _ in range(len(g) + 1)] for __ in range(len(h) + 1)]\r\n",
    "        for g_node_index in range(len(g) + 1):\r\n",
    "            for h_node_index in range(len(h) + 1):\r\n",
    "                lsape_instance[h_node_index][g_node_index] = rings.compute_ring_distance(g,h,self.ring_g,self.ring_h,g_node_index,h_node_index,alpha,lbda,node_costs,nodeInsDel,edge_costs,edgeInsDel,first_graph,second_graph)\r\n",
    "        for i in lsape_instance :\r\n",
    "            i = torch.as_tensor(i)\r\n",
    "        lsape_instance = torch.as_tensor(lsape_instance)\r\n",
    "        return lsape_instance\r\n",
    "    \r\n",
    "    # Calculating a mapping based on the cost matrix C, using the rings function and a derivable Hungarian approximation\r\n",
    "    def mapping_from_cost_sans_FW(self,n,m,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel): \r\n",
    "        c_0 =self.lsape_populate_instance(g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)\r\n",
    "        x0=svd.eps_assigment_from_mapping(torch.exp(-c_0),10).view((n+1)*(m+1),1)\r\n",
    "        return x0\r\n",
    "    \r\n",
    "    # Calculating a mapping based on the cost matrix C, not using the rings function and using a derivable Hungarian approximation\r\n",
    "    def new_mapping_from_cost(self,C,n,m,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel): \r\n",
    "        c=torch.diag(C)       \r\n",
    "        D=C-torch.eye(C.shape[0],device=self.device)*c\r\n",
    "        x0=svd.eps_assigment_from_mapping(torch.exp(-c),10).view((n+1)*(m+1),1)\r\n",
    "        return x0\r\n",
    "    \r\n",
    "    # Calculating a mapping based on the cost matrix C, not using the rings function and using the Frank Wolfe algorithm\r\n",
    "    def mapping_from_cost(self,C,n,m):\r\n",
    "        c=torch.diag(C)\r\n",
    "        D=C-torch.eye(C.shape[0],device=self.device)*c\r\n",
    "        x0=svd.eps_assigment_from_mapping(torch.exp(-.5*c.view(n+1,m+1)),10).view((n+1)*(m+1),1)\r\n",
    "        x=svd.franck_wolfe(x0,D,c,5,10,n,m)\r\n",
    "        def print_grad(grad):\r\n",
    "            if(grad.norm()!= 0.0):\r\n",
    "                print(grad)        \r\n",
    "        return x\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating our model, and sending it to the gpu\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model = Net(Gs,normalize=True,node_label='extended_label')\n",
    "model.to(device)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This function creates sets of couples of graphs from sets of graphs \r\n",
    "\r\n",
    "def creating_couples_after_splitting(train_D, valid_D,train_L,valid_L):\r\n",
    "    couples_train=[]\r\n",
    "    couples_test_train=[]\r\n",
    "    for i,g1_idx in enumerate(train_D): \r\n",
    "        for j,g2_idx in enumerate(train_D):\r\n",
    "            n=g1_idx\r\n",
    "            m=g2_idx\r\n",
    "            couples_train.append([n,m])\r\n",
    "    yt=np.ones(len(couples_train))\r\n",
    "    for k in couples_train:\r\n",
    "        if (y[k[0]]!=y[k[1]]):\r\n",
    "            yt[k]=-1.0  \r\n",
    "    for i,g1_idx in enumerate(valid_D):\r\n",
    "        for j,g2_idx in enumerate(train_D):\r\n",
    "            n=g1_idx\r\n",
    "            m=g2_idx\r\n",
    "            couples_test_train.append([n,m])\r\n",
    "            \r\n",
    "    yv=np.ones(len(couples_test_train))\r\n",
    "    for k in couples_test_train:\r\n",
    "        if (y[k[0]]!=y[k[1]]):\r\n",
    "            yv[k]=-1.0\r\n",
    "            \r\n",
    "    return torch.tensor(couples_train),yt,torch.tensor(couples_test_train),yv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Verifying that the two sets contain different graphs\r\n",
    "\r\n",
    "def different_sets(my_train_D,my_valid_D): \r\n",
    "    cp=my_valid_D\r\n",
    "    for i in range(len(my_valid_D)):\r\n",
    "        if my_valid_D[i] in my_train_D:\r\n",
    "            tmp=random.choice(Gs)\r\n",
    "            if tmp not in my_train_D: \r\n",
    "                cp[i]=tmp\r\n",
    "    my_valid_D=cp\r\n",
    "    \r\n",
    "    return my_train_D,my_valid_D"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This function splits the graph list Gs into two distinct sets of couples of graphs\r\n",
    "# One for training the model and one for testing it\r\n",
    "\r\n",
    "def splitting(Gs): \r\n",
    "    my_list=[i for i in range(len(Gs))] \r\n",
    "\r\n",
    "    [train_D, valid_D,train_L,valid_L]= train_test_split(my_list,y, test_size=0.20,train_size=0.80, shuffle=True, stratify=y) # we stratify so that y is used as the class labels\r\n",
    "    # We make sure that the two sets contain distinct graphs\r\n",
    "    train_D, valid_D=different_sets(train_D,valid_D)\r\n",
    "    \r\n",
    "    couples_train,yt,couples_test_train,yv = creating_couples_after_splitting(train_D, valid_D,train_L,valid_L)\r\n",
    "    yt=torch.tensor(yt)\r\n",
    "    yv=torch.tensor(yv)\r\n",
    "    DatasetTrain = TensorDataset(couples_train,yt) \r\n",
    "    DatasetValid=TensorDataset(couples_test_train, yv)\r\n",
    "\r\n",
    "    trainloader=torch.utils.data.DataLoader(DatasetTrain,batch_size=len(couples_train),shuffle=True,drop_last=True, num_workers=0) #128, len(couples_train)\r\n",
    "    validationloader=torch.utils.data.DataLoader(DatasetValid, batch_size=len(couples_test_train), drop_last=True,num_workers=0) #64,128,len(couples_test_train)\r\n",
    "    \r\n",
    "    print(len(trainloader),len(validationloader))\r\n",
    "    print(len(trainloader),len(validationloader))\r\n",
    "    \r\n",
    "    # We save our sets in pickle files\r\n",
    "    torch.save(train_D, 'pickle_files/train_D', pickle_module=pkl) \r\n",
    "    torch.save(valid_D, 'pickle_files/valid_D', pickle_module=pkl) \r\n",
    "    torch.save(train_L, 'pickle_files/train_L', pickle_module=pkl) \r\n",
    "    torch.save(valid_L, 'pickle_files/valid_L', pickle_module=pkl) \r\n",
    "    \r\n",
    "    return trainloader,validationloader,couples_train,yt,couples_test_train,yv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This is the function used for classification problems\r\n",
    "\r\n",
    "def classification(model,Gs,nb_iter):\r\n",
    "    \r\n",
    "    trainloader,validationloader,couples_train,yt,couples_test_train,yv=splitting(Gs)\r\n",
    "    criterion = torch.nn.HingeEmbeddingLoss(margin=1.0,reduction='mean')\r\n",
    "    criterionTri=triangular_constraint()\r\n",
    "    optimizer = torch.optim.Adam(model.parameters()) #, lr=1e-3\r\n",
    "    \r\n",
    "    train_input=couples_train.to(device)\r\n",
    "    #valid_input=couples_test_train.to(device)\r\n",
    "    \r\n",
    "    target=yt.to(device) \r\n",
    "    InsDel=np.empty((nb_iter,2))\r\n",
    "    node_costs,nodeInsDel,edge_costs,edgeInsDel=model.from_weighs_to_costs()\r\n",
    "    nodeSub=np.empty((nb_iter,int(node_costs.shape[0]*(node_costs.shape[0]-1)/2)))\r\n",
    "    edgeSub=np.empty((nb_iter,int(edge_costs.shape[0]*(edge_costs.shape[0]-1)/2)))\r\n",
    "    loss_plt=np.empty(nb_iter)\r\n",
    "    loss_train_plt=np.empty(nb_iter)\r\n",
    "    loss_valid_plt=np.empty(nb_iter)\r\n",
    "    min_valid_loss = np.inf\r\n",
    "    iter_min_valid_loss = 0\r\n",
    "    \r\n",
    "    \r\n",
    "    for t in range(nb_iter):   \r\n",
    "        print(t, torch.cuda.memory_allocated())\r\n",
    "        train_loss = 0.0\r\n",
    "        valid_loss = 0.0\r\n",
    "        tmp=np.inf\r\n",
    "        \r\n",
    "        # The training part :\r\n",
    "        for train_data,train_labels in trainloader:\r\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\r\n",
    "            optimizer.zero_grad()\r\n",
    "            #inputt=train_data.to(device)\r\n",
    "            \r\n",
    "            # Forward pass: Compute predicted y by passing data to the model\r\n",
    "            y_pred = model(train_data).to(device)  \r\n",
    "\r\n",
    "            # Computing and printing loss\r\n",
    "            train_labels=train_labels.to(device)\r\n",
    "            loss = criterion(y_pred, train_labels).to(device)\r\n",
    "            node_costs,nodeInsDel,edge_costs,edgeInsDel=model.from_weighs_to_costs()\r\n",
    "            triangularInq=criterionTri(node_costs,nodeInsDel,edge_costs,edgeInsDel)\r\n",
    "            loss=loss*(1+triangularInq)\r\n",
    "            loss.to(device)\r\n",
    "            loss.backward()\r\n",
    "            loss=loss.detach()\r\n",
    "            \r\n",
    "            optimizer.step()\r\n",
    "            print('loss.item of the train = ', t, loss.item())\r\n",
    "            train_loss =+ loss.item() #* train_data.size(0) \r\n",
    "            if (loss.item()<tmp): tmp=loss.item()\r\n",
    "                \r\n",
    "         # Getting the training loss\r\n",
    "        loss_plt[t]=loss.item()  \r\n",
    "        loss_train_plt[t]=train_loss /len(trainloader)\r\n",
    "        #loss_plt[t]=tmp\r\n",
    "          \r\n",
    "        # Getting the costs of the first iteration, to compare later\r\n",
    "        if t==0:\r\n",
    "            nodeInsDelInit=nodeInsDel\r\n",
    "            edgeInsDelInit=edgeInsDel\r\n",
    "            nodeSubInit=node_costs\r\n",
    "            edgeSubInit=edge_costs\r\n",
    "            torch.save(nodeInsDelInit, 'pickle_files/nodeInsDelInit', pickle_module=pkl) \r\n",
    "            torch.save(edgeInsDelInit, 'pickle_files/edgeInsDelInit', pickle_module=pkl) \r\n",
    "            torch.save(nodeSubInit, 'pickle_files/nodeSubInit', pickle_module=pkl) \r\n",
    "            torch.save(edgeSubInit, 'pickle_files/edgeSubInit', pickle_module=pkl) \r\n",
    "\r\n",
    "        # Getting some information every 100 iterations, to follow the evolution\r\n",
    "        if t % 100 == 99 or t==0:   \r\n",
    "            print('ged=',y_pred*target)  #train_labels\r\n",
    "            print('Distances: ',y_pred)\r\n",
    "            print('Loss Triangular:',triangularInq.item())\r\n",
    "            print('node_costs : \\n', node_costs)\r\n",
    "            print('nodeInsDel:',nodeInsDel.item())\r\n",
    "            print('edge_costs : \\n', edge_costs)\r\n",
    "            print('edgeInsDel:',edgeInsDel.item())\r\n",
    "        \r\n",
    "        print(f'Iteration {t+1} \\t\\t Training Loss: {train_loss / len(trainloader)}')\r\n",
    "        \r\n",
    "        # We delete to liberate some memory\r\n",
    "        del y_pred, train_loss,loss\r\n",
    "        torch.cuda.empty_cache()\r\n",
    "        \r\n",
    "        # The validation part :\r\n",
    "        for valid_data,valid_labels in validationloader:\r\n",
    "            inputt=valid_data.to(device)\r\n",
    "            y_pred = model(inputt).to(device)\r\n",
    "            # Compute and print loss\r\n",
    "            valid_labels=valid_labels.to(device)\r\n",
    "            loss = criterion(y_pred, valid_labels).to(device)    \r\n",
    "            loss.to(device)\r\n",
    "            print('loss.item of the valid = ', t, loss.item())  \r\n",
    "            valid_loss = valid_loss + loss.item() #* valid_data.size(0)\r\n",
    "\r\n",
    "        # Getting the validation loss\r\n",
    "        loss_valid_plt[t]=valid_loss / len(validationloader)   \r\n",
    "        \r\n",
    "        # Getting edges and nodes Insertion/Deletion costs\r\n",
    "        InsDel[t][0]=nodeInsDel.item()\r\n",
    "        InsDel[t][1]=edgeInsDel.item()\r\n",
    "        \r\n",
    "        k=0\r\n",
    "        for p in range(node_costs.shape[0]):\r\n",
    "            for q in range(p+1,node_costs.shape[0]):\r\n",
    "                nodeSub[t][k]=node_costs[p][q]\r\n",
    "                k=k+1\r\n",
    "        k=0\r\n",
    "        for p in range(edge_costs.shape[0]):\r\n",
    "            for q in range(p+1,edge_costs.shape[0]):\r\n",
    "                edgeSub[t][k]=edge_costs[p][q]\r\n",
    "                k=k+1\r\n",
    "        \r\n",
    "        print(f'Iteration {t+1} \\t\\t Validation Loss: {valid_loss/len(validationloader)}')\r\n",
    "        if min_valid_loss > valid_loss:\r\n",
    "            print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f})')\r\n",
    "            min_valid_loss = valid_loss\r\n",
    "            iter_min_valid_loss = t\r\n",
    "            nodeSub_min = node_costs\r\n",
    "            edgeSub_min = edge_costs\r\n",
    "            nodeInsDel_min = nodeInsDel\r\n",
    "            edgeInsDel_min = edgeInsDel\r\n",
    "        \r\n",
    "        # We delete to liberate some memory\r\n",
    "        del valid_loss,loss\r\n",
    "        torch.cuda.empty_cache()                \r\n",
    "            \r\n",
    "    print('iter and min_valid_loss = ',iter_min_valid_loss, min_valid_loss)\r\n",
    "    print(' Min cost for nodeInsDel = ', nodeInsDel_min)\r\n",
    "    print(' Min cost for edgeInsDel = ', edgeInsDel_min)\r\n",
    "    print(' Min cost for nodeSub = ', nodeSub_min)\r\n",
    "    print(' Min cost for edgeSub = ', edgeSub_min)\r\n",
    "    # Saving the minimum costs into pickle files\r\n",
    "    torch.save(nodeInsDel_min, 'pickle_files/nodeInsDel_min', pickle_module=pkl) \r\n",
    "    torch.save(edgeInsDel_min, 'pickle_files/edgeInsDel_min', pickle_module=pkl) \r\n",
    "    torch.save(nodeSub_min, 'pickle_files/nodeSub_min', pickle_module=pkl) \r\n",
    "    torch.save(edgeSub_min, 'pickle_files/edgeSub_min', pickle_module=pkl)\r\n",
    "    return InsDel,nodeSub,edgeSub,loss_plt,loss_valid_plt,loss_train_plt\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(len(Gs))\r\n",
    "nb_iter=100\r\n",
    "\r\n",
    "InsDel, nodeSub,edgeSub,loss_plt,loss_valid_plt,loss_train_plt=classification(model,Gs,nb_iter)\r\n",
    "\r\n",
    "# Plotting Node/Edge insertion/deletion costs\r\n",
    "plt.figure(0)\r\n",
    "plt.plot(InsDel[0:nb_iter,0],label=\"node\")\r\n",
    "plt.plot(InsDel[0:nb_iter,1],label=\"edge\")\r\n",
    "plt.title('Node/Edge insertion/deletion costs')\r\n",
    "plt.legend()\r\n",
    "\r\n",
    "# Plotting Node Substitutions costs\r\n",
    "plt.figure(1)\r\n",
    "for k in range(nodeSub.shape[1]):\r\n",
    "    plt.plot(nodeSub[0:nb_iter,k])\r\n",
    "plt.title('Node Substitutions costs')\r\n",
    "\r\n",
    "# Plotting Edge Substitutions costs\r\n",
    "plt.figure(2)\r\n",
    "for k in range(edgeSub.shape[1]):\r\n",
    "    plt.plot(edgeSub[0:nb_iter,k])\r\n",
    "plt.title('Edge Substitutions costs')\r\n",
    "\r\n",
    "# Plotting the evolution of the train loss\r\n",
    "plt.figure(3)\r\n",
    "plt.plot(loss_plt)\r\n",
    "plt.title('Evolution of the train loss (loss_plt)')\r\n",
    "\r\n",
    "# Plotting the evolution of the validation loss\r\n",
    "plt.figure(4)\r\n",
    "plt.plot(loss_valid_plt)\r\n",
    "plt.title('Evolution of the valid loss')\r\n",
    "\r\n",
    "plt.show()\r\n",
    "plt.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We save the losses into pickle files\r\n",
    "torch.save(loss_plt, 'pickle_files/loss_plt', pickle_module=pkl) \r\n",
    "torch.save(loss_valid_plt, 'pickle_files/loss_valid_plt', pickle_module=pkl) \r\n",
    "torch.save(loss_train_plt, 'pickle_files/loss_train_plt', pickle_module=pkl) \r\n",
    "\r\n",
    "# We save the costs into pickle files\r\n",
    "torch.save(InsDel,'pickle_files/InsDel', pickle_module=pkl) \r\n",
    "torch.save(edgeSub,'pickle_files/edgeSub', pickle_module=pkl) \r\n",
    "torch.save(nodeSub,'pickle_files/nodeSub', pickle_module=pkl) \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "A=torch.tensor(nx.to_scipy_sparse_matrix(Gs[0],dtype=int,weight='bond_type').todense(),dtype=torch.int) \r\n",
    "print(A)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plotting the train and the valid losses in a same graphic, to compare their evolution\r\n",
    "plt.plot(loss_plt, label='train loss')\r\n",
    "plt.plot(loss_valid_plt, label='valid loss')\r\n",
    "plt.title('Train and valid losses')\r\n",
    "plt.legend()\r\n",
    "plt.show()\r\n",
    "plt.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "card = torch.tensor([G.order() for G in Gs]).to(device)\r\n",
    "print(Gs[0].order())"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}