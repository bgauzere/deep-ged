{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gklearn.utils.graphfiles import loadDataset\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge max label 3\n",
      "0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "def label_to_color(label):\n",
    "    if label == 'C':\n",
    "        return 0.1\n",
    "    elif label == 'O':\n",
    "        return 0.8\n",
    "    \n",
    "def nodes_to_color_sequence(G):\n",
    "    return [label_to_color(c[1]['label'][0]) for c in G.nodes(data=True)]\n",
    "\n",
    "Gs,y = loadDataset('/home/ines/Documents/M2/Stage/stage_ged/Ines/DeepGED/MAO/dataset.ds')\n",
    "#for e in Gs[13].edges():\n",
    "#    print(Gs[13][e[0]][e[1]]['bond_type'])\n",
    "print('edge max label',max(max([[G[e[0]][e[1]]['bond_type'] for e in G.edges()] for G in Gs])))\n",
    "G1 = Gs[1]\n",
    "G2 = Gs[9]\n",
    "print(y[1],y[9])\n",
    "\n",
    "'''\n",
    "plt.figure(0)\n",
    "nx.draw_networkx(G1,with_labels=True,node_color = nodes_to_color_sequence(G1),cmap='autumn')\n",
    "\n",
    "plt.figure(1)\n",
    "nx.draw_networkx(G2,with_labels=True,node_color = nodes_to_color_sequence(G2),cmap='autumn')\n",
    "\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "import extended_label\n",
    "for g in Gs:\n",
    "    extended_label.compute_extended_labels(g)\n",
    "#for v in Gs[10].nodes():\n",
    "#        print(Gs[10].nodes[v])\n",
    "\n",
    "#print(nx.to_dict_of_lists(Gs[13]))\n",
    "\n",
    "\n",
    "\n",
    "#dict={'C':0,'N':1,'O':2}\n",
    "#A,labels=from_networkx_to_tensor2(Gs[13],dict)\n",
    "#print(A)\n",
    "#A1=(A==torch.ones(13,13)).int()\n",
    "#A2=(A==2*torch.ones(13,13)).int()\n",
    "#print(A1)\n",
    "#print(A2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gs= 68\n",
      "['C_1C', 'C_1C1C1N', 'C_1C1C2C', 'C_1C1N', 'C_1C1N2C', 'C_1C1O', 'C_1C1O2C', 'C_1C2C', 'C_1C3C', 'C_1N', 'C_1O', 'C_2C', 'C_2C2C', 'C_3C', 'N_1C', 'N_1C1C', 'N_1C1C1C', 'O_1C', 'O_1C1C']\n",
      "{'C_1C': 0, 'C_1C1C1N': 1, 'C_1C1C2C': 2, 'C_1C1N': 3, 'C_1C1N2C': 4, 'C_1C1O': 5, 'C_1C1O2C': 6, 'C_1C2C': 7, 'C_1C3C': 8, 'C_1N': 9, 'C_1O': 10, 'C_2C': 11, 'C_2C2C': 12, 'C_3C': 13, 'N_1C': 14, 'N_1C1C': 15, 'N_1C1C1C': 16, 'O_1C': 17, 'O_1C1C': 18} 19\n",
      "3\n",
      "torch.Size([68, 729])\n",
      "adjacency matrices tensor([[         0,          1,          0,  ...,          1,      32539,\n",
      "                165],\n",
      "        [         0,          1,          0,  ...,      32539,   78283592,\n",
      "                  0],\n",
      "        [         0,          1,          0,  ..., -652004368,      32539,\n",
      "         2115235952],\n",
      "        ...,\n",
      "        [         0,          1,          0,  ...,          0,          0,\n",
      "                  0],\n",
      "        [         0,          1,          0,  ...,          0,          0,\n",
      "                  0],\n",
      "        [         0,          1,          0,  ...,          0,          0,\n",
      "                  0]], dtype=torch.int32)\n",
      "node labels tensor([[        15,          4,          7,  ...,          0,          0,\n",
      "           78205760],\n",
      "        [        15,          4,          7,  ...,         46,          3,\n",
      "                 26],\n",
      "        [        15,          4,          7,  ...,        126,         47,\n",
      "                 24],\n",
      "        ...,\n",
      "        [        16,          4,          7,  ...,          0, -651994512,\n",
      "              32539],\n",
      "        [        16,          4,          7,  ...,          1,          0,\n",
      "           78213256],\n",
      "        [        16,          4,          7,  ...,         32,          1,\n",
      "                  0]], dtype=torch.int32)\n",
      "order of the graphs tensor([11, 12, 14, 14, 15, 17, 15, 16, 19, 15, 16, 19, 12, 13, 15, 18, 16, 16,\n",
      "        17, 16, 17, 17, 18, 19, 19, 18, 18, 22, 22, 15, 14, 13, 16, 17, 17, 21,\n",
      "        17, 18, 18, 21, 24, 25, 25, 14, 17, 18, 18, 22, 23, 23, 25, 27, 27, 15,\n",
      "        16, 23, 24, 24, 16, 17, 17, 20, 23, 24, 26, 16, 17, 21])\n",
      "2\n",
      "Parameter containing:\n",
      "tensor([0.0065, 0.0061, 0.0063, 0.0067, 0.0064, 0.0063, 0.0064, 0.0058, 0.0059,\n",
      "        0.0062, 0.0057, 0.0061, 0.0061, 0.0064, 0.0058, 0.0059, 0.0058, 0.0065,\n",
      "        0.0061, 0.0065, 0.0064, 0.0057, 0.0063, 0.0060, 0.0066, 0.0066, 0.0066,\n",
      "        0.0059, 0.0060, 0.0058, 0.0057, 0.0062, 0.0060, 0.0066, 0.0066, 0.0061,\n",
      "        0.0064, 0.0057, 0.0061, 0.0057, 0.0067, 0.0063, 0.0061, 0.0060, 0.0063,\n",
      "        0.0059, 0.0059, 0.0063, 0.0066, 0.0066, 0.0063, 0.0061, 0.0063, 0.0063,\n",
      "        0.0061, 0.0064, 0.0059, 0.0063, 0.0064, 0.0059, 0.0062, 0.0064, 0.0058,\n",
      "        0.0064, 0.0059, 0.0065, 0.0066, 0.0058, 0.0067, 0.0059, 0.0066, 0.0061,\n",
      "        0.0062, 0.0058, 0.0060, 0.0062, 0.0066, 0.0060, 0.0061, 0.0057, 0.0062,\n",
      "        0.0058, 0.0059, 0.0057, 0.0067, 0.0061, 0.0062, 0.0058, 0.0061, 0.0059,\n",
      "        0.0066, 0.0066, 0.0059, 0.0065, 0.0061, 0.0063, 0.0059, 0.0060, 0.0063,\n",
      "        0.0064, 0.0060, 0.0061, 0.0065, 0.0061, 0.0063, 0.0060, 0.0061, 0.0059,\n",
      "        0.0067, 0.0059, 0.0065, 0.0058, 0.0066, 0.0062, 0.0063, 0.0059, 0.0063,\n",
      "        0.0066, 0.0059, 0.0065, 0.0066, 0.0057, 0.0066, 0.0060, 0.0066, 0.0066,\n",
      "        0.0060, 0.0063, 0.0059, 0.0059, 0.0067, 0.0063, 0.0058, 0.0061, 0.0063,\n",
      "        0.0059, 0.0058, 0.0062, 0.0065, 0.0065, 0.0057, 0.0063, 0.0065, 0.0067,\n",
      "        0.0062, 0.0061, 0.0066, 0.0064, 0.0066, 0.0064, 0.0059, 0.0061, 0.0062,\n",
      "        0.0065, 0.0065, 0.0057, 0.0066, 0.0064, 0.0060, 0.0061, 0.0058, 0.0058,\n",
      "        0.0066, 0.0063, 0.0057, 0.0058, 0.0062, 0.0064, 0.0057, 0.0059, 0.0062,\n",
      "        0.0061], requires_grad=True)\n",
      "27 68\n",
      "toto\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import svd\n",
    "import rings\n",
    "from svd import iterated_power as compute_major_axis\n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "        \n",
    "    def __init__(self,GraphList,normalize=False,node_label='label'):\n",
    "        super(Net, self).__init__()   \n",
    "        self.normalize=normalize\n",
    "        self.node_label=node_label\n",
    "        dict,self.nb_edge_labels=self.build_node_dictionnary(GraphList)\n",
    "        self.nb_labels=len(dict)\n",
    "        print(self.nb_edge_labels)\n",
    "        self.device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        nb_node_pair_label=self.nb_labels*(self.nb_labels-1)/2.0\n",
    "        nb_edge_pair_label=int(self.nb_edge_labels*(self.nb_edge_labels-1)/2)\n",
    "        \n",
    "        self.node_weighs=nn.Parameter(torch.tensor(1.0/(nb_node_pair_label+nb_edge_pair_label+2))+(1e-3)*torch.rand(int(self.nb_labels*(self.nb_labels-1)/2+1),requires_grad=True,device=self.device)) # all substitution costs+ nodeIns/Del. old version: 0 node subs, 1 nodeIns/Del, 2 : edgeSubs, 3 edgeIns/Del        \n",
    "        self.edge_weighs=nn.Parameter(torch.tensor(1.0/(nb_node_pair_label+nb_edge_pair_label+2))+(1e-3)*torch.rand(nb_edge_pair_label+1,requires_grad=True,device=self.device)) #edgeIns/Del\n",
    "        \n",
    "        self.card=torch.tensor([G.order() for G in GraphList]).to(self.device)\n",
    "        card_max=self.card.max()\n",
    "        self.A=torch.empty((len(GraphList),card_max*card_max),dtype=torch.int,device=self.device)\n",
    "        self.labels=torch.empty((len(GraphList),card_max),dtype=torch.int,device=self.device)\n",
    "        print(self.A.shape)\n",
    "        for k in range(len(GraphList)):\n",
    "            A,l=self.from_networkx_to_tensor(GraphList[k],dict)             \n",
    "            self.A[k,0:A.shape[1]]=A[0]\n",
    "            self.labels[k,0:l.shape[0]]=l\n",
    "        print('adjacency matrices',self.A)\n",
    "        print('node labels',self.labels)\n",
    "        print('order of the graphs',self.card)\n",
    "        \n",
    "    def forward(self, input):        \n",
    "        ged=torch.zeros(len(input)).to(self.device) \n",
    "        node_costs,nodeInsDel,edge_costs,edgeInsDel=self.from_weighs_to_costs()\n",
    "        \n",
    "            \n",
    "        \n",
    "        #print('weighs:',self.weighs.device,'device:',self.device,'card:',self.card.device,'A:',self.A.device,'labels:',self.labels.device)\n",
    "        for k in range(len(input)):            \n",
    "            g1=input[k][0]\n",
    "            g2=input[k][1]\n",
    "            n=self.card[g1]\n",
    "            m=self.card[g2]\n",
    "            \n",
    "            self.ring_g,self.ring_h = rings.build_rings(g1,edgeInsDel.size()), rings.build_rings(g2,edgeInsDel.size()) \n",
    "            \n",
    "            C=self.construct_cost_matrix(g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)      \n",
    "            #S=self.mapping_from_similarity(C,n,m)\n",
    "            #S=self.mapping_from_cost(C,n,m)   \n",
    "            #S=self.new_mapping_from_cost(C,n,m,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)\n",
    "            S=self.mapping_from_cost_sans_FW(n,m,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)\n",
    "            \n",
    "            v=torch.flatten(S)\n",
    "            \n",
    "            normalize_factor=1.0\n",
    "            if self.normalize:\n",
    "                nb_edge1=(self.A[g1][0:n*n] != torch.zeros(n*n,device=self.device)).int().sum()\n",
    "                nb_edge2=(self.A[g2][0:m*m] != torch.zeros(m*m,device=self.device)).int().sum()\n",
    "                normalize_factor=nodeInsDel*(n+m)+edgeInsDel*(nb_edge1+nb_edge2)\n",
    "            c=torch.diag(C)\n",
    "            D=C-torch.eye(C.shape[0],device=self.device)*c\n",
    "            ged[k]=(.5*v.T@D@v+c.T@v)/normalize_factor\n",
    "        max=torch.max(ged)\n",
    "        min=torch.min(ged)\n",
    "        ged=(ged-min)/(max-min)\n",
    "        \n",
    "        return ged\n",
    "    \n",
    "    def from_weighs_to_costs(self):\n",
    "        \n",
    "        #cn=torch.exp(self.node_weighs)\n",
    "        #ce=torch.exp(self.edge_weighs)\n",
    "        cn=self.node_weighs*self.node_weighs\n",
    "        ce=self.edge_weighs*self.edge_weighs\n",
    "        total_cost=cn.sum()+ce.sum()\n",
    "        cn=cn/total_cost #/max\n",
    "        ce=ce/total_cost\n",
    "        edgeInsDel=ce[-1]\n",
    "\n",
    "        node_costs=torch.zeros((self.nb_labels,self.nb_labels),device=self.device)\n",
    "        upper_part=torch.triu_indices(node_costs.shape[0],node_costs.shape[1],offset=1,device=self.device)        \n",
    "        node_costs[upper_part[0],upper_part[1]]=cn[0:-1]\n",
    "        node_costs=node_costs+node_costs.T\n",
    "\n",
    "        if self.nb_edge_labels>1:\n",
    "            edge_costs=torch.zeros((self.nb_edge_labels,self.nb_edge_labels),device=self.device)\n",
    "            upper_part=torch.triu_indices(edge_costs.shape[0],edge_costs.shape[1],offset=1,device=self.device)        \n",
    "            edge_costs[upper_part[0],upper_part[1]]=ce[0:-1]\n",
    "            edge_costs=edge_costs+edge_costs.T\n",
    "        else:\n",
    "            edge_costs=torch.zeros(0,device=self.device)\n",
    "        \n",
    "        return node_costs,cn[-1],edge_costs,edgeInsDel\n",
    "    \n",
    "    def build_node_dictionnary(self,GraphList):\n",
    "        #extraction de tous les labels d'atomes\n",
    "        node_labels=[]\n",
    "        for G in Gs:\n",
    "            for v in nx.nodes(G):\n",
    "                if not G.nodes[v][self.node_label][0] in node_labels:\n",
    "                    node_labels.append(G.nodes[v][self.node_label][0])\n",
    "        node_labels.sort()\n",
    "        #extraction d'un dictionnaire permettant de numéroter chaque label par un numéro.\n",
    "        dict={}\n",
    "        k=0\n",
    "        for label in node_labels:\n",
    "            dict[label]=k\n",
    "            k=k+1\n",
    "        print(node_labels)\n",
    "        print(dict,len(dict))\n",
    "    \n",
    "        return dict,max(max([[int(G[e[0]][e[1]]['bond_type']) for e in G.edges()] for G in GraphList]))\n",
    "    \n",
    "    def from_networkx_to_tensor(self,G,dict):    \n",
    "        A=torch.tensor(nx.to_scipy_sparse_matrix(G,dtype=int,weight='bond_type').todense(),dtype=torch.int)        \n",
    "        lab=[dict[G.nodes[v][self.node_label][0]] for v in nx.nodes(G)]\n",
    "   \n",
    "        return (A.view(1,A.shape[0]*A.shape[1]),torch.tensor(lab))\n",
    "\n",
    "    def construct_cost_matrix(self,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel):\n",
    "        n = self.card[g1].item()\n",
    "        m = self.card[g2].item()\n",
    "        \n",
    "        A1=torch.zeros((n+1,n+1),dtype=torch.int,device=self.device)\n",
    "        A1[0:n,0:n]=self.A[g1][0:n*n].view(n,n)\n",
    "        A2=torch.zeros((m+1,m+1),dtype=torch.int,device=self.device)\n",
    "        A2[0:m,0:m]=self.A[g2][0:m*m].view(m,m)\n",
    "        \n",
    "        \n",
    "         # costs: 0 node subs, 1 nodeIns/Del, 2 : edgeSubs, 3 edgeIns/Del\n",
    "        \n",
    "        #C=cost[3]*torch.cat([torch.cat([C12[l][k] for k in range(n+1)],1) for l in range(n+1)])\n",
    "        #Pas bien sur mais cela semble fonctionner.\n",
    "        C=edgeInsDel*self.matrix_edgeInsDel(A1,A2)\n",
    "        if self.nb_edge_labels>1:\n",
    "            for k in range(self.nb_edge_labels):\n",
    "                for l in range(self.nb_edge_labels):\n",
    "                    if k != l:\n",
    "#                    C.add_(self.matrix_edgeSubst(A1,A2,k+1,l+1),alpha=edge_costs[k][l])\n",
    "                        C=C+edge_costs[k][l]*self.matrix_edgeSubst(A1,A2,k+1,l+1)\n",
    "        #C=cost[3]*torch.tensor(np.array([ [  k!=l and A1[k//(m+1),l//(m+1)]^A2[k%(m+1),l%(m+1)] for k in range((n+1)*(m+1))] for l in range((n+1)*(m+1))]),device=self.device)        \n",
    "\n",
    "        l1=self.labels[g1][0:n]\n",
    "        l2=self.labels[g2][0:m]\n",
    "        D=torch.zeros((n+1)*(m+1),device=self.device)\n",
    "        D[n*(m+1):]=nodeInsDel\n",
    "        D[n*(m+1)+m]=0\n",
    "        D[[i*(m+1)+m for i in range(n)]]=nodeInsDel\n",
    "        D[[k for k in range(n*(m+1)) if k%(m+1) != m]]=torch.tensor([node_costs[l1[k//(m+1)],l2[k%(m+1)]] for k in range(n*(m+1)) if k%(m+1) != m],device=self.device )\n",
    "        mask = torch.diag(torch.ones_like(D))\n",
    "        C=mask*torch.diag(D) + (1. - mask)*C\n",
    "        \n",
    "        #C[range(len(C)),range(len(C))]=D\n",
    "      \n",
    "        return C\n",
    "    def matrix_edgeInsDel(self,A1,A2):\n",
    "        Abin1=(A1!=torch.zeros((A1.shape[0],A1.shape[1]),device=self.device))\n",
    "        Abin2=(A2!=torch.zeros((A2.shape[0],A2.shape[1]),device=self.device))\n",
    "        C1=torch.einsum('ij,kl->ijkl',torch.logical_not(Abin1),Abin2)\n",
    "        C2=torch.einsum('ij,kl->ijkl',Abin1,torch.logical_not(Abin2))\n",
    "        C12=torch.logical_or(C1,C2).int()\n",
    "    \n",
    "        return torch.cat(torch.unbind(torch.cat(torch.unbind(C12,1),1),0),1)\n",
    "\n",
    "    def matrix_edgeSubst(self,A1,A2,lab1,lab2):\n",
    "        Abin1=(A1==lab1*torch.ones((A1.shape[0],A1.shape[1]),device=self.device)).int()\n",
    "        Abin2=(A2==lab2*torch.ones((A2.shape[0],A2.shape[1]),device=self.device)).int()\n",
    "        C=torch.einsum('ij,kl->ijkl',Abin1,Abin2)\n",
    "        \n",
    "        return torch.cat(torch.unbind(torch.cat(torch.unbind(C,1),1),0),1)\n",
    "    \n",
    "    def similarity_from_cost(self,C):\n",
    "        N=C.shape[0]\n",
    "             \n",
    "        #return (torch.norm(C,p='fro')*torch.eye(N,device=self.device) -C)\n",
    "        return (C.max()*torch.eye(N,device=self.device) -C)\n",
    "    \n",
    "    def lsape_populate_instance(self,first_graph,second_graph,average_node_cost, average_edge_cost,alpha,lbda):       #ring_g, ring_h come from global ring with all graphs in so ring_g = rings['g'] and ring_h = rings['h']\n",
    "        g,h = Gs[first_graph], Gs[second_graph]\n",
    "        self.average_cost =[average_node_cost, average_edge_cost]\n",
    "        self.first_graph, self.second_graph = first_graph,second_graph\n",
    "        \n",
    "        node_costs,nodeInsDel,edge_costs,edgeInsDel=self.from_weighs_to_costs()\n",
    "\n",
    "        lsape_instance = [[0 for _ in range(len(g) + 1)] for __ in range(len(h) + 1)]\n",
    "        for g_node_index in range(len(g) + 1):\n",
    "            for h_node_index in range(len(h) + 1):\n",
    "                lsape_instance[h_node_index][g_node_index] = rings.compute_ring_distance(g,h,self.ring_g,self.ring_h,g_node_index,h_node_index,alpha,lbda,node_costs,nodeInsDel,edge_costs,edgeInsDel,first_graph,second_graph)\n",
    "        for i in lsape_instance :\n",
    "            i = torch.as_tensor(i)\n",
    "        lsape_instance = torch.as_tensor(lsape_instance)\n",
    "        #print(type(lsape_instance))\n",
    "        return lsape_instance\n",
    "    \n",
    "  \n",
    "    def mapping_from_cost_sans_FW(self,n,m,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel): \n",
    "        c_0 =self.lsape_populate_instance(g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)\n",
    "        x0=svd.eps_assigment_from_mapping(torch.exp(-c_0),10).view((n+1)*(m+1),1)\n",
    "        return x0\n",
    "    \n",
    "    def new_mapping_from_cost(self,C,n,m,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel): \n",
    "        c=torch.diag(C)       \n",
    "        c_0 =self.lsape_populate_instance(g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)\n",
    "        D=C-torch.eye(C.shape[0],device=self.device)*c\n",
    "        x0=svd.eps_assigment_from_mapping(torch.exp(-c_0),10).view((n+1)*(m+1),1)\n",
    "        return svd.franck_wolfe(x0,D,c,5,15,n,m)\n",
    "    \n",
    "    \n",
    "    def mapping_from_cost(self,C,n,m):\n",
    "        c=torch.diag(C)\n",
    "        D=C-torch.eye(C.shape[0],device=self.device)*c\n",
    "        x0=svd.eps_assigment_from_mapping(torch.exp(-.5*c.view(n+1,m+1)),10).view((n+1)*(m+1),1)\n",
    "        x=svd.franck_wolfe(x0,D,c,5,10,n,m)\n",
    "        def print_grad(grad):\n",
    "            if(grad.norm()!= 0.0):\n",
    "                print(grad)\n",
    "        \n",
    "#        x.register_hook(print_grad)\n",
    "        return x\n",
    "\n",
    "print('Gs=',len(Gs))\n",
    "model = Net(Gs,normalize=True,node_label='extended_label')\n",
    "\n",
    "params = list(model.parameters())\n",
    "print(len(params))\n",
    "print(params[0])\n",
    "#print(model(input))\n",
    "print(max([G.order() for G in Gs]),len(Gs))\n",
    "print('toto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20, 14, 14, 29, 28, 27, 11,  1, 23, 16, 11, 14])\n",
      "train graphs: tensor([60,  4, 19, 27, 66, 25, 36, 59,  7, 23, 14, 45, 40, 57, 47, 35,  0])\n",
      "couples= tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "          3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,\n",
      "          5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,\n",
      "          9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         12, 12, 12, 12, 13, 13, 13, 14, 14, 15],\n",
      "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  2,  3,\n",
      "          4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  3,  4,  5,  6,  7,\n",
      "          8,  9, 10, 11, 12, 13, 14, 15, 16,  4,  5,  6,  7,  8,  9, 10, 11, 12,\n",
      "         13, 14, 15, 16,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  6,  7,\n",
      "          8,  9, 10, 11, 12, 13, 14, 15, 16,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
      "         16,  8,  9, 10, 11, 12, 13, 14, 15, 16,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "         10, 11, 12, 13, 14, 15, 16, 11, 12, 13, 14, 15, 16, 12, 13, 14, 15, 16,\n",
      "         13, 14, 15, 16, 14, 15, 16, 15, 16, 16]])\n",
      "nb_class1/nb_class2= 12 5\n",
      "couples restreints: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "          3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,\n",
      "          5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,\n",
      "          9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11],\n",
      "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  2,  3,\n",
      "          4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  3,  4,  5,  6,  7,\n",
      "          8,  9, 10, 11, 12, 13, 14, 15, 16,  4,  5,  6,  7,  8,  9, 10, 11, 12,\n",
      "         13, 14, 15, 16,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  6,  7,\n",
      "          8,  9, 10, 11, 12, 13, 14, 15, 16,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
      "         16,  8,  9, 10, 11, 12, 13, 14, 15, 16,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "         10, 11, 12, 13, 14, 15, 16, 11, 12, 13, 14, 15, 16, 12, 13, 14, 15, 16]])\n",
      "old_size, new size= 126 126.0\n",
      "126\n",
      "data= tensor([[60,  4],\n",
      "        [60, 19],\n",
      "        [60, 27],\n",
      "        [60, 66],\n",
      "        [60, 25],\n",
      "        [60, 36],\n",
      "        [60, 59],\n",
      "        [60,  7],\n",
      "        [60, 23],\n",
      "        [60, 14],\n",
      "        [60, 45],\n",
      "        [60, 40],\n",
      "        [60, 57],\n",
      "        [60, 47],\n",
      "        [60, 35],\n",
      "        [60,  0],\n",
      "        [ 4, 19],\n",
      "        [ 4, 27],\n",
      "        [ 4, 66],\n",
      "        [ 4, 25],\n",
      "        [ 4, 36],\n",
      "        [ 4, 59],\n",
      "        [ 4,  7],\n",
      "        [ 4, 23],\n",
      "        [ 4, 14],\n",
      "        [ 4, 45],\n",
      "        [ 4, 40],\n",
      "        [ 4, 57],\n",
      "        [ 4, 47],\n",
      "        [ 4, 35],\n",
      "        [ 4,  0],\n",
      "        [19, 27],\n",
      "        [19, 66],\n",
      "        [19, 25],\n",
      "        [19, 36],\n",
      "        [19, 59],\n",
      "        [19,  7],\n",
      "        [19, 23],\n",
      "        [19, 14],\n",
      "        [19, 45],\n",
      "        [19, 40],\n",
      "        [19, 57],\n",
      "        [19, 47],\n",
      "        [19, 35],\n",
      "        [19,  0],\n",
      "        [27, 66],\n",
      "        [27, 25],\n",
      "        [27, 36],\n",
      "        [27, 59],\n",
      "        [27,  7],\n",
      "        [27, 23],\n",
      "        [27, 14],\n",
      "        [27, 45],\n",
      "        [27, 40],\n",
      "        [27, 57],\n",
      "        [27, 47],\n",
      "        [27, 35],\n",
      "        [27,  0],\n",
      "        [66, 25],\n",
      "        [66, 36],\n",
      "        [66, 59],\n",
      "        [66,  7],\n",
      "        [66, 23],\n",
      "        [66, 14],\n",
      "        [66, 45],\n",
      "        [66, 40],\n",
      "        [66, 57],\n",
      "        [66, 47],\n",
      "        [66, 35],\n",
      "        [66,  0],\n",
      "        [25, 36],\n",
      "        [25, 59],\n",
      "        [25,  7],\n",
      "        [25, 23],\n",
      "        [25, 14],\n",
      "        [25, 45],\n",
      "        [25, 40],\n",
      "        [25, 57],\n",
      "        [25, 47],\n",
      "        [25, 35],\n",
      "        [25,  0],\n",
      "        [36, 59],\n",
      "        [36,  7],\n",
      "        [36, 23],\n",
      "        [36, 14],\n",
      "        [36, 45],\n",
      "        [36, 40],\n",
      "        [36, 57],\n",
      "        [36, 47],\n",
      "        [36, 35],\n",
      "        [36,  0],\n",
      "        [59,  7],\n",
      "        [59, 23],\n",
      "        [59, 14],\n",
      "        [59, 45],\n",
      "        [59, 40],\n",
      "        [59, 57],\n",
      "        [59, 47],\n",
      "        [59, 35],\n",
      "        [59,  0],\n",
      "        [ 7, 23],\n",
      "        [ 7, 14],\n",
      "        [ 7, 45],\n",
      "        [ 7, 40],\n",
      "        [ 7, 57],\n",
      "        [ 7, 47],\n",
      "        [ 7, 35],\n",
      "        [ 7,  0],\n",
      "        [23, 14],\n",
      "        [23, 45],\n",
      "        [23, 40],\n",
      "        [23, 57],\n",
      "        [23, 47],\n",
      "        [23, 35],\n",
      "        [23,  0],\n",
      "        [14, 45],\n",
      "        [14, 40],\n",
      "        [14, 57],\n",
      "        [14, 47],\n",
      "        [14, 35],\n",
      "        [14,  0],\n",
      "        [45, 40],\n",
      "        [45, 57],\n",
      "        [45, 47],\n",
      "        [45, 35],\n",
      "        [45,  0]], dtype=torch.int32) 126\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "nb=len(Gs)\n",
    "class1=torch.tensor([k for k in range(len(y)) if y[k]==1])\n",
    "class2=torch.tensor([k for k in range(len(y)) if y[k]==0])\n",
    "\n",
    "nb_class1=12\n",
    "nb_class2=int((nb_class1-1)/2)\n",
    "train_size=nb_class1+nb_class2\n",
    "#train_size=20\n",
    "\n",
    "#if train_size % 2 == 0:\n",
    "#    nb_class1=int(train_size/2)\n",
    "#    nb_class2=int(train_size/2)\n",
    "#else:\n",
    "#    nb_class1=int(train_size/2)+1\n",
    "#    nb_class2=int(train_size/2)\n",
    "    \n",
    "print((torch.abs(10000*torch.randn(nb_class1)).int()%class1.size()[0]).long())\n",
    "random_class1=class1[(torch.abs(10000*torch.randn(nb_class1)).int()%class1.size()[0]).long()]\n",
    "random_class2=class2[(torch.abs(10000*torch.randn(nb_class2)).int()%class2.size()[0]).long()]\n",
    "train_graphs=torch.cat((random_class1,random_class2),0)\n",
    "print('train graphs:',train_graphs)\n",
    "\n",
    "\n",
    "couples=torch.triu_indices(train_size,train_size,offset=1)\n",
    "print('couples=',couples)\n",
    "print('nb_class1/nb_class2=',nb_class1,nb_class2)\n",
    "#combinations=itertools.combinations(range(nb),2)\n",
    "\n",
    "nb_elt=int(nb_class1*(nb_class1+2*nb_class2-1)/2)\n",
    "print('couples restreints:',couples[:,0:nb_elt])\n",
    "\n",
    "#nb_elt=int(train_size*(train_size-1)/2)\n",
    "data=torch.empty((nb_elt,2),dtype=torch.int)\n",
    "yt=torch.ones(nb_elt)\n",
    "print('old_size, new size=',nb_elt,.5*nb_class1*(nb_class1+2*nb_class2-1))\n",
    "data[0:nb_elt,0]=train_graphs[couples[0,0:nb_elt]]\n",
    "data[0:nb_elt,1]=train_graphs[couples[1,0:nb_elt]]\n",
    "\n",
    "\n",
    "#data[0:nb_elt,0]=train_graphs[couples[0]]\n",
    "#data[0:nb_elt,1]=train_graphs[couples[1]]\n",
    "print(nb_elt)\n",
    "#couples=[]\n",
    "for k in range(nb_elt):\n",
    "    if (y[data[k][0]]!=y[data[k][1]]):\n",
    "        yt[k]=-1.0        \n",
    "\n",
    "print('data=',data,len(data))\n",
    "\n",
    "#print(couples[1:2])\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")          # a CUDA device object    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[train_D, valid_D,train_L,valid_L]= train_test_split(Gs,y, test_size=0.25,train_size=0.75, shuffle=True) #, stratify=yt)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[train_D, valid_D,train_L,valid_L]= train_test_split(Gs,y, test_size=0.25,train_size=0.75, shuffle=True) #, stratify=yt)\n",
    "  \n",
    "\n",
    "def creating_couples_after_splitting(train_D, valid_D,train_L,valid_L):\n",
    "    couples_train=[]\n",
    "    couples_test_train=[]\n",
    "    for i,g1_idx in enumerate(train_D): \n",
    "        for j,g2_idx in enumerate(train_D):\n",
    "            n=g1_idx.order()\n",
    "            m=g2_idx.order()\n",
    "            #print([n,m])\n",
    "            couples_train.append([n,m])\n",
    "    yt=np.ones(len(couples_train))\n",
    "    #print(yt,len(yt))\n",
    "    for k in couples_train:\n",
    "        if (y[k[0]]!=y[k[1]]):\n",
    "            yt[k]=-1.0  \n",
    "    for i,g1_idx in enumerate(valid_D):\n",
    "        for j,g2_idx in enumerate(train_D):\n",
    "            n=g1_idx.order()\n",
    "            m=g2_idx.order()\n",
    "            couples_test_train.append([n,m])\n",
    "            \n",
    "    yv=np.ones(len(couples_test_train))\n",
    "    #print(yt,len(yt))\n",
    "    for k in couples_test_train:\n",
    "        if (y[k[0]]!=y[k[1]]):\n",
    "            yv[k]=-1.0\n",
    "            \n",
    "    return torch.tensor(couples_train),yt,torch.tensor(couples_test_train),yv\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[11]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def different_sets(my_train_D,my_valid_D):\n",
    "    #my_list=[i for i in range(68)] \n",
    "    cp=my_valid_D\n",
    "\n",
    "    for i in range(len(my_valid_D)):\n",
    "        if my_valid_D[i] in my_train_D:\n",
    "            tmp=random.choice(Gs)\n",
    "            if tmp not in my_train_D: \n",
    "                cp[i]=tmp\n",
    "        #else: cp[i]=my_valid_D[i]\n",
    "    my_valid_D=cp\n",
    "    \n",
    "    return my_train_D,my_valid_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def different_sets_ints(my_train_D,my_valid_D):\n",
    "    my_list=[i for i in range(68)] \n",
    "    cp=my_valid_D\n",
    "\n",
    "    for i in range(len(my_valid_D)):\n",
    "        if my_valid_D[i] in my_train_D:\n",
    "            tmp=random.choice(my_list)\n",
    "            if tmp not in my_train_D: \n",
    "                cp[i]=tmp\n",
    "        #else: cp[i]=my_valid_D[i]\n",
    "    my_valid_D=cp\n",
    "    \n",
    "    return my_train_D,my_valid_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x7f1b292ea280>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gs[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_data(train_D, valid_D,train_L,valid_L):\n",
    "    my_train_D=[]\n",
    "    my_valid_D=[]\n",
    "    for i,g1_idx in enumerate(train_D): \n",
    "        n=g1_idx.order()\n",
    "        my_train_D.append(n)\n",
    "    for i,g1_idx in enumerate(valid_D): \n",
    "        n=g1_idx.order()\n",
    "        my_valid_D.append(n)\n",
    "        \n",
    "    my_train_D,my_valid_D=different_sets_ints(my_train_D,my_valid_D)\n",
    "    \n",
    "    return my_train_D,my_valid_D\n",
    "\n",
    "#my_train_D,my_valid_D,train_L,valid_L=getting_data(train_D, valid_D,train_L,valid_L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "\n",
    "def splitting(data):\n",
    "    \n",
    "    #[train_D, valid_D,train_L,valid_L]= train_test_split(data,yt, test_size=0.25,train_size=0.75, shuffle=True) #, stratify=yt)\n",
    "    [train_D, valid_D,train_L,valid_L]= train_test_split(Gs,y, test_size=0.25,train_size=0.75, shuffle=True, stratify=y)\n",
    "    train_D, valid_D=different_sets(train_D,valid_D)\n",
    "    \n",
    "    couples_train,yt,couples_test_train,yv = creating_couples_after_splitting(train_D, valid_D,train_L,valid_L)\n",
    "    yt=torch.tensor(yt)\n",
    "    yv=torch.tensor(yv)\n",
    "    #DatasetTrain = TensorDataset(train_D, train_L)\n",
    "    DatasetTrain = TensorDataset(couples_train,yt) \n",
    "    \n",
    "    #DatasetValid=TensorDataset(valid_D, valid_L)\n",
    "    DatasetValid=TensorDataset(couples_test_train, yv)\n",
    "\n",
    "    trainloader=torch.utils.data.DataLoader(DatasetTrain,batch_size=len(couples_train),shuffle=True,drop_last=True, num_workers=0)\n",
    "\n",
    "    validationloader=torch.utils.data.DataLoader(DatasetValid, batch_size=128, drop_last=True,num_workers=0)\n",
    "\n",
    "    '''\n",
    "    print(len(train_D), len(valid_D))\n",
    "    print(len(train_L), len(valid_L))\n",
    "    print(len(trainloader),len(validationloader))\n",
    "    for i,j in validationloader:\n",
    "        print('i : ',i,'\\n j : ',j)\n",
    "    '''\n",
    "    \n",
    "    print(len(trainloader),len(validationloader))\n",
    "    print(len(trainloader),len(validationloader))\n",
    "    \n",
    "    train_D,valid_D=getting_data(train_D, valid_D,train_L,valid_L)\n",
    "    torch.save(train_D, 'train_D', pickle_module=pkl) \n",
    "    torch.save(valid_D, 'valid_D', pickle_module=pkl) \n",
    "    torch.save(train_L, 'train_L', pickle_module=pkl) \n",
    "    torch.save(valid_L, 'valid_L', pickle_module=pkl) \n",
    "    \n",
    "    return trainloader,validationloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if torch.cuda.device_count() > 1:\n",
    "#  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "#  model = nn.DataParallel(model)\n",
    "from triangular_losses import TriangularConstraint as triangular_constraint\n",
    "\n",
    "model.to(device)\n",
    "def classification(model,data,yt,nb_iter):\n",
    "    \n",
    "    trainloader,validationloader=splitting(data)\n",
    "    criterion = torch.nn.HingeEmbeddingLoss(margin=1.0,reduction='mean')\n",
    "    criterionTri=triangular_constraint()\n",
    "    optimizer = torch.optim.Adam(model.parameters()) #, lr=1e-3\n",
    "    #print(device)\n",
    "\n",
    "    #torch.cat((same_class[0:20],diff_class[0:20]),0).to(device)\n",
    "    whole_input=data.to(device)\n",
    "    target=yt.to(device) \n",
    "    #torch.ones(40,device=device)\n",
    "    #target[20:]=-1.0\n",
    "    #target=(yt[0:20]).to(device)\n",
    "    InsDel=np.empty((nb_iter,2))\n",
    "    node_costs,nodeInsDel,edge_costs,edgeInsDel=model.from_weighs_to_costs()\n",
    "    nodeSub=np.empty((nb_iter,int(node_costs.shape[0]*(node_costs.shape[0]-1)/2)))\n",
    "    edgeSub=np.empty((nb_iter,int(edge_costs.shape[0]*(edge_costs.shape[0]-1)/2)))\n",
    "    loss_plt=np.empty(nb_iter)\n",
    "    loss_train_plt=np.empty(nb_iter)\n",
    "    loss_valid_plt=np.empty(nb_iter)\n",
    "    min_valid_loss = np.inf\n",
    "    iter_min_valid_loss = 0\n",
    "    \n",
    "    for t in range(nb_iter):    \n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        tmp=np.inf\n",
    "        for train_data,train_labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            inputt=train_data.to(device)\n",
    "            \n",
    "            # Forward pass: Compute predicted y by passing data to the model\n",
    "            y_pred = model(whole_input).to(device)\n",
    "\n",
    "            # Compute and print loss\n",
    "            loss = criterion(y_pred, target).to(device)\n",
    "            node_costs,nodeInsDel,edge_costs,edgeInsDel=model.from_weighs_to_costs()\n",
    "            triangularInq=criterionTri(node_costs,nodeInsDel,edge_costs,edgeInsDel)\n",
    "            loss=loss*(1+triangularInq)\n",
    "            loss.to(device)\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            #optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print('loss.item of the train = ', t, loss.item())\n",
    "            train_loss =+ loss.item() #* train_data.size(0) \n",
    "            if (loss.item()<tmp): tmp=loss.item()\n",
    "        loss_plt[t]=loss.item()  \n",
    "        loss_train_plt[t]=train_loss /len(trainloader)\n",
    "        #loss_plt[t]=tmp\n",
    "            \n",
    "        if t % 100 == 99 or t==0:   \n",
    "            print('ged=',y_pred*target)  #train_labels\n",
    "            print('Distances: ',y_pred)\n",
    "            print('Loss Triangular:',triangularInq.item())\n",
    "            print('node_costs :')\n",
    "            print(node_costs)\n",
    "            print('nodeInsDel:',nodeInsDel.item())\n",
    "            print('edge_costs :')\n",
    "            print(edge_costs)\n",
    "            print('edgeInsDel:',edgeInsDel.item())\n",
    "            \n",
    "            \n",
    "            \n",
    "        for valid_data,valid_labels in validationloader:\n",
    "            \n",
    "            inputt=valid_data.to(device)\n",
    "            y_pred = model(inputt).to(device)\n",
    "            # Compute and print loss\n",
    "            loss = criterion(y_pred, valid_labels).to(device)    \n",
    "            loss.to(device)\n",
    "            \n",
    "            print('loss.item of the valid = ', t, loss.item())  \n",
    "            valid_loss = valid_loss + loss.item() #* valid_data.size(0)\n",
    "            \n",
    "        loss_valid_plt[t]=valid_loss / len(validationloader)   \n",
    "        \n",
    "        InsDel[t][0]=nodeInsDel.item()\n",
    "        InsDel[t][1]=edgeInsDel.item()\n",
    "        \n",
    "        \n",
    "        k=0\n",
    "        for p in range(node_costs.shape[0]):\n",
    "            for q in range(p+1,node_costs.shape[0]):\n",
    "                nodeSub[t][k]=node_costs[p][q]\n",
    "                k=k+1\n",
    "        k=0\n",
    "        for p in range(edge_costs.shape[0]):\n",
    "            for q in range(p+1,edge_costs.shape[0]):\n",
    "                edgeSub[t][k]=edge_costs[p][q]\n",
    "                k=k+1\n",
    "        \n",
    "            \n",
    "        print(f'Iteration {t+1} \\t\\t Training Loss: {train_loss / len(trainloader)} \\t\\t Validation Loss: {valid_loss/len(validationloader)}')\n",
    "        if min_valid_loss > valid_loss:\n",
    "            print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f})')\n",
    "            min_valid_loss = valid_loss\n",
    "            iter_min_valid_loss = t\n",
    "            nodeSub_min = node_costs\n",
    "            edgeSub_min = edge_costs\n",
    "            nodeInsDel_min = nodeInsDel\n",
    "            edgeInsDel_min = edgeInsDel\n",
    "            \n",
    "            \n",
    "    print('iter and min_valid_loss = ',iter_min_valid_loss, min_valid_loss)\n",
    "    '''\n",
    "    nodeInsDel_min = InsDel[iter_min_valid_loss][0]\n",
    "    edgeInsDel_min = InsDel[iter_min_valid_loss][1]\n",
    "    nodeSub_min = nodeSub[iter_min_valid_loss]\n",
    "    edgeSub_min = edgeSub[iter_min_valid_loss]\n",
    "    '''\n",
    "    print(' Min cost for nodeInsDel = ', nodeInsDel_min)\n",
    "    print(' Min cost for edgeInsDel = ', edgeInsDel_min)\n",
    "    print(' Min cost for nodeSub = ', nodeSub_min)\n",
    "    print(' Min cost for edgeSub = ', edgeSub_min)\n",
    "    torch.save(nodeInsDel_min, 'nodeInsDel_min', pickle_module=pkl) \n",
    "    torch.save(edgeInsDel_min, 'edgeInsDel_min', pickle_module=pkl) \n",
    "    torch.save(nodeSub_min, 'nodeSub_min', pickle_module=pkl) \n",
    "    torch.save(edgeSub_min, 'edgeSub_min', pickle_module=pkl)\n",
    "    return InsDel,nodeSub,edgeSub,loss_plt,loss_valid_plt,loss_train_plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 6\n",
      "1 6\n",
      "loss.item of the train =  0 0.3707982003688812\n",
      "ged= tensor([ 0.1730,  0.1821,  0.4786,  0.0155,  0.2515,  0.0271,  0.0280,  0.1985,\n",
      "         0.2855,  0.1730,  0.2527, -0.5683, -0.5647, -0.4285, -0.3781, -0.3958,\n",
      "         0.2351,  0.6524,  0.2649,  0.3830,  0.2820,  0.2799,  0.2542,  0.2852,\n",
      "         0.0000,  0.3740, -0.8625, -0.8641, -0.5820, -0.5229, -0.2655,  0.5676,\n",
      "         0.2204,  0.2773,  0.2409,  0.2486,  0.0029,  0.3307,  0.1529,  0.2779,\n",
      "        -0.6091, -0.6056, -0.5165, -0.4322, -0.3459,  0.2638,  0.2125,  0.2674,\n",
      "         0.2530,  0.3150,  0.2132,  0.3888,  0.2011, -0.2597, -0.2695, -0.0199,\n",
      "        -0.2028, -1.0000,  0.2509,  0.0271,  0.0280,  0.1925,  0.2815,  0.1704,\n",
      "         0.2527, -0.5655, -0.5619, -0.4180, -0.3781, -0.3946,  0.1967,  0.2029,\n",
      "         0.1945,  0.2355,  0.2146,  0.0264, -0.4665, -0.4705, -0.3672, -0.3238,\n",
      "        -0.5402,  0.0327,  0.1906,  0.2798,  0.1850,  0.2585, -0.5997, -0.5991,\n",
      "        -0.4307, -0.3960, -0.3953,  0.1888,  0.2993,  0.1846,  0.2479, -0.5798,\n",
      "        -0.5827, -0.4422, -0.3948, -0.4095,  0.3456,  0.1592,  0.2744, -0.6285,\n",
      "        -0.6281, -0.5122, -0.4994, -0.3600,  0.2059,  0.1984, -0.2313, -0.2349,\n",
      "        -0.3141, -0.2634, -0.6069,  0.3718, -0.8392, -0.8408, -0.5780, -0.5129,\n",
      "        -0.2599, -0.4554, -0.4646, -0.3659, -0.3324, -0.5486],\n",
      "       grad_fn=<MulBackward0>)\n",
      "Distances:  tensor([0.1730, 0.1821, 0.4786, 0.0155, 0.2515, 0.0271, 0.0280, 0.1985, 0.2855,\n",
      "        0.1730, 0.2527, 0.5683, 0.5647, 0.4285, 0.3781, 0.3958, 0.2351, 0.6524,\n",
      "        0.2649, 0.3830, 0.2820, 0.2799, 0.2542, 0.2852, 0.0000, 0.3740, 0.8625,\n",
      "        0.8641, 0.5820, 0.5229, 0.2655, 0.5676, 0.2204, 0.2773, 0.2409, 0.2486,\n",
      "        0.0029, 0.3307, 0.1529, 0.2779, 0.6091, 0.6056, 0.5165, 0.4322, 0.3459,\n",
      "        0.2638, 0.2125, 0.2674, 0.2530, 0.3150, 0.2132, 0.3888, 0.2011, 0.2597,\n",
      "        0.2695, 0.0199, 0.2028, 1.0000, 0.2509, 0.0271, 0.0280, 0.1925, 0.2815,\n",
      "        0.1704, 0.2527, 0.5655, 0.5619, 0.4180, 0.3781, 0.3946, 0.1967, 0.2029,\n",
      "        0.1945, 0.2355, 0.2146, 0.0264, 0.4665, 0.4705, 0.3672, 0.3238, 0.5402,\n",
      "        0.0327, 0.1906, 0.2798, 0.1850, 0.2585, 0.5997, 0.5991, 0.4307, 0.3960,\n",
      "        0.3953, 0.1888, 0.2993, 0.1846, 0.2479, 0.5798, 0.5827, 0.4422, 0.3948,\n",
      "        0.4095, 0.3456, 0.1592, 0.2744, 0.6285, 0.6281, 0.5122, 0.4994, 0.3600,\n",
      "        0.2059, 0.1984, 0.2313, 0.2349, 0.3141, 0.2634, 0.6069, 0.3718, 0.8392,\n",
      "        0.8408, 0.5780, 0.5129, 0.2599, 0.4554, 0.4646, 0.3659, 0.3324, 0.5486],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Loss Triangular: 0.0\n",
      "node_costs :\n",
      "tensor([[0.0000, 0.0062, 0.0056, 0.0058, 0.0066, 0.0061, 0.0059, 0.0061, 0.0050,\n",
      "         0.0052, 0.0057, 0.0049, 0.0056, 0.0056, 0.0060, 0.0050, 0.0052, 0.0050,\n",
      "         0.0062],\n",
      "        [0.0062, 0.0000, 0.0055, 0.0063, 0.0061, 0.0049, 0.0060, 0.0053, 0.0065,\n",
      "         0.0065, 0.0064, 0.0052, 0.0054, 0.0050, 0.0048, 0.0057, 0.0053, 0.0065,\n",
      "         0.0065],\n",
      "        [0.0056, 0.0055, 0.0000, 0.0056, 0.0061, 0.0048, 0.0054, 0.0048, 0.0066,\n",
      "         0.0059, 0.0054, 0.0053, 0.0058, 0.0052, 0.0052, 0.0058, 0.0064, 0.0064,\n",
      "         0.0059],\n",
      "        [0.0058, 0.0063, 0.0056, 0.0000, 0.0056, 0.0060, 0.0059, 0.0056, 0.0061,\n",
      "         0.0052, 0.0060, 0.0061, 0.0051, 0.0057, 0.0060, 0.0049, 0.0060, 0.0052,\n",
      "         0.0063],\n",
      "        [0.0066, 0.0061, 0.0061, 0.0056, 0.0000, 0.0064, 0.0050, 0.0066, 0.0051,\n",
      "         0.0064, 0.0054, 0.0056, 0.0049, 0.0054, 0.0056, 0.0064, 0.0053, 0.0055,\n",
      "         0.0048],\n",
      "        [0.0061, 0.0049, 0.0048, 0.0060, 0.0064, 0.0000, 0.0058, 0.0051, 0.0052,\n",
      "         0.0049, 0.0066, 0.0056, 0.0057, 0.0051, 0.0055, 0.0051, 0.0064, 0.0064,\n",
      "         0.0051],\n",
      "        [0.0059, 0.0060, 0.0054, 0.0059, 0.0050, 0.0058, 0.0000, 0.0063, 0.0056,\n",
      "         0.0059, 0.0051, 0.0053, 0.0059, 0.0061, 0.0054, 0.0055, 0.0062, 0.0055,\n",
      "         0.0059],\n",
      "        [0.0061, 0.0053, 0.0048, 0.0056, 0.0066, 0.0051, 0.0063, 0.0000, 0.0053,\n",
      "         0.0055, 0.0052, 0.0066, 0.0051, 0.0063, 0.0050, 0.0064, 0.0056, 0.0058,\n",
      "         0.0051],\n",
      "        [0.0050, 0.0065, 0.0066, 0.0061, 0.0051, 0.0052, 0.0056, 0.0053, 0.0000,\n",
      "         0.0059, 0.0065, 0.0051, 0.0062, 0.0065, 0.0048, 0.0066, 0.0054, 0.0064,\n",
      "         0.0064],\n",
      "        [0.0052, 0.0065, 0.0059, 0.0052, 0.0064, 0.0049, 0.0059, 0.0055, 0.0059,\n",
      "         0.0000, 0.0053, 0.0060, 0.0052, 0.0052, 0.0066, 0.0059, 0.0050, 0.0055,\n",
      "         0.0059],\n",
      "        [0.0057, 0.0064, 0.0054, 0.0060, 0.0054, 0.0066, 0.0051, 0.0052, 0.0065,\n",
      "         0.0053, 0.0000, 0.0051, 0.0050, 0.0057, 0.0063, 0.0062, 0.0049, 0.0058,\n",
      "         0.0063],\n",
      "        [0.0049, 0.0052, 0.0053, 0.0061, 0.0056, 0.0056, 0.0053, 0.0066, 0.0051,\n",
      "         0.0060, 0.0051, 0.0000, 0.0066, 0.0057, 0.0055, 0.0065, 0.0060, 0.0064,\n",
      "         0.0060],\n",
      "        [0.0056, 0.0054, 0.0058, 0.0051, 0.0049, 0.0057, 0.0059, 0.0051, 0.0062,\n",
      "         0.0052, 0.0050, 0.0066, 0.0000, 0.0051, 0.0056, 0.0058, 0.0062, 0.0062,\n",
      "         0.0048],\n",
      "        [0.0056, 0.0050, 0.0052, 0.0057, 0.0054, 0.0051, 0.0061, 0.0063, 0.0065,\n",
      "         0.0052, 0.0057, 0.0057, 0.0051, 0.0000, 0.0064, 0.0060, 0.0053, 0.0055,\n",
      "         0.0050],\n",
      "        [0.0060, 0.0048, 0.0052, 0.0060, 0.0056, 0.0055, 0.0054, 0.0050, 0.0048,\n",
      "         0.0066, 0.0063, 0.0055, 0.0056, 0.0064, 0.0000, 0.0050, 0.0065, 0.0059,\n",
      "         0.0049],\n",
      "        [0.0050, 0.0057, 0.0058, 0.0049, 0.0064, 0.0051, 0.0055, 0.0064, 0.0066,\n",
      "         0.0059, 0.0062, 0.0065, 0.0058, 0.0060, 0.0050, 0.0000, 0.0050, 0.0057,\n",
      "         0.0060],\n",
      "        [0.0052, 0.0053, 0.0064, 0.0060, 0.0053, 0.0064, 0.0062, 0.0056, 0.0054,\n",
      "         0.0050, 0.0049, 0.0060, 0.0062, 0.0053, 0.0065, 0.0050, 0.0000, 0.0048,\n",
      "         0.0052],\n",
      "        [0.0050, 0.0065, 0.0064, 0.0052, 0.0055, 0.0064, 0.0055, 0.0058, 0.0064,\n",
      "         0.0055, 0.0058, 0.0064, 0.0062, 0.0055, 0.0059, 0.0057, 0.0048, 0.0000,\n",
      "         0.0058],\n",
      "        [0.0062, 0.0065, 0.0059, 0.0063, 0.0048, 0.0051, 0.0059, 0.0051, 0.0064,\n",
      "         0.0059, 0.0063, 0.0060, 0.0048, 0.0050, 0.0049, 0.0060, 0.0052, 0.0058,\n",
      "         0.0000]], grad_fn=<AddBackward0>)\n",
      "nodeInsDel: 0.005506515968590975\n",
      "edge_costs :\n",
      "tensor([[0.0000, 0.0057, 0.0057],\n",
      "        [0.0057, 0.0000, 0.0052],\n",
      "        [0.0057, 0.0052, 0.0000]], grad_fn=<AddBackward0>)\n",
      "edgeInsDel: 0.005254096817225218\n",
      "loss.item of the valid =  0 0.3361877202987671\n",
      "loss.item of the valid =  0 0.37919220328330994\n",
      "loss.item of the valid =  0 0.3368178606033325\n",
      "loss.item of the valid =  0 0.28624415397644043\n",
      "loss.item of the valid =  0 0.3065773546695709\n",
      "loss.item of the valid =  0 0.45578062534332275\n",
      "Iteration 1 \t\t Training Loss: 0.3707982003688812 \t\t Validation Loss: 0.3501333196957906\n",
      "Validation Loss Decreased(inf--->2.100800)\n",
      "loss.item of the train =  1 0.36260899901390076\n",
      "loss.item of the valid =  1 0.34250664710998535\n",
      "loss.item of the valid =  1 0.3832415044307709\n",
      "loss.item of the valid =  1 0.3429710268974304\n",
      "loss.item of the valid =  1 0.2912682592868805\n",
      "loss.item of the valid =  1 0.31464219093322754\n",
      "loss.item of the valid =  1 0.4607783854007721\n",
      "Iteration 2 \t\t Training Loss: 0.36260899901390076 \t\t Validation Loss: 0.3559013356765111\n",
      "loss.item of the train =  2 0.35798153281211853\n",
      "loss.item of the valid =  2 0.34599483013153076\n",
      "loss.item of the valid =  2 0.38553687930107117\n",
      "loss.item of the valid =  2 0.3462661802768707\n",
      "loss.item of the valid =  2 0.29355740547180176\n",
      "loss.item of the valid =  2 0.31812068819999695\n",
      "loss.item of the valid =  2 0.4640900194644928\n",
      "Iteration 3 \t\t Training Loss: 0.35798153281211853 \t\t Validation Loss: 0.3589276671409607\n",
      "loss.item of the train =  3 0.3553257882595062\n",
      "loss.item of the valid =  3 0.34819021821022034\n",
      "loss.item of the valid =  3 0.3869793117046356\n",
      "loss.item of the valid =  3 0.3482922315597534\n",
      "loss.item of the valid =  3 0.29489171504974365\n",
      "loss.item of the valid =  3 0.3199198544025421\n",
      "loss.item of the valid =  3 0.46649169921875\n",
      "Iteration 4 \t\t Training Loss: 0.3553257882595062 \t\t Validation Loss: 0.36079417169094086\n",
      "loss.item of the train =  4 0.3537997305393219\n",
      "loss.item of the valid =  4 0.3497619330883026\n",
      "loss.item of the valid =  4 0.38803625106811523\n",
      "loss.item of the valid =  4 0.34979501366615295\n",
      "loss.item of the valid =  4 0.295908123254776\n",
      "loss.item of the valid =  4 0.32128584384918213\n",
      "loss.item of the valid =  4 0.46846309304237366\n",
      "Iteration 5 \t\t Training Loss: 0.3537997305393219 \t\t Validation Loss: 0.36220837632815045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item of the train =  5 0.3529166281223297\n",
      "loss.item of the valid =  5 0.3509775698184967\n",
      "loss.item of the valid =  5 0.38898828625679016\n",
      "loss.item of the valid =  5 0.3509843647480011\n",
      "loss.item of the valid =  5 0.29665690660476685\n",
      "loss.item of the valid =  5 0.3224833011627197\n",
      "loss.item of the valid =  5 0.4702318012714386\n",
      "Iteration 6 \t\t Training Loss: 0.3529166281223297 \t\t Validation Loss: 0.36338703831036884\n",
      "loss.item of the train =  6 0.35251033306121826\n",
      "loss.item of the valid =  6 0.3520623743534088\n",
      "loss.item of the valid =  6 0.3898161053657532\n",
      "loss.item of the valid =  6 0.3519844710826874\n",
      "loss.item of the valid =  6 0.29715192317962646\n",
      "loss.item of the valid =  6 0.3231700360774994\n",
      "loss.item of the valid =  6 0.472003310918808\n",
      "Iteration 7 \t\t Training Loss: 0.35251033306121826 \t\t Validation Loss: 0.3643647034962972\n",
      "loss.item of the train =  7 0.3523334264755249\n",
      "loss.item of the valid =  7 0.35301119089126587\n",
      "loss.item of the valid =  7 0.3912966549396515\n",
      "loss.item of the valid =  7 0.35295355319976807\n",
      "loss.item of the valid =  7 0.29739874601364136\n",
      "loss.item of the valid =  7 0.3238648474216461\n",
      "loss.item of the valid =  7 0.4740588963031769\n",
      "Iteration 8 \t\t Training Loss: 0.3523334264755249 \t\t Validation Loss: 0.36543064812819165\n",
      "loss.item of the train =  8 0.3517526686191559\n",
      "loss.item of the valid =  8 0.353970468044281\n",
      "loss.item of the valid =  8 0.3932919204235077\n",
      "loss.item of the valid =  8 0.35416102409362793\n",
      "loss.item of the valid =  8 0.29762378334999084\n",
      "loss.item of the valid =  8 0.3249697685241699\n",
      "loss.item of the valid =  8 0.4763200879096985\n",
      "Iteration 9 \t\t Training Loss: 0.3517526686191559 \t\t Validation Loss: 0.36672284205754596\n",
      "loss.item of the train =  9 0.35143187642097473\n",
      "loss.item of the valid =  9 0.3550416827201843\n",
      "loss.item of the valid =  9 0.39539825916290283\n",
      "loss.item of the valid =  9 0.3554384112358093\n",
      "loss.item of the valid =  9 0.29785650968551636\n",
      "loss.item of the valid =  9 0.3265252113342285\n",
      "loss.item of the valid =  9 0.4787052869796753\n",
      "Iteration 10 \t\t Training Loss: 0.35143187642097473 \t\t Validation Loss: 0.3681608935197194\n",
      "loss.item of the train =  10 0.3559626340866089\n",
      "loss.item of the valid =  10 0.35372209548950195\n",
      "loss.item of the valid =  10 0.3925134539604187\n",
      "loss.item of the valid =  10 0.3540171980857849\n",
      "loss.item of the valid =  10 0.2977614104747772\n",
      "loss.item of the valid =  10 0.32573214173316956\n",
      "loss.item of the valid =  10 0.47550857067108154\n",
      "Iteration 11 \t\t Training Loss: 0.3559626340866089 \t\t Validation Loss: 0.36654247840245563\n",
      "loss.item of the train =  11 0.35165050625801086\n",
      "loss.item of the valid =  11 0.3528117537498474\n",
      "loss.item of the valid =  11 0.39103108644485474\n",
      "loss.item of the valid =  11 0.3534257113933563\n",
      "loss.item of the valid =  11 0.29778358340263367\n",
      "loss.item of the valid =  11 0.3250811696052551\n",
      "loss.item of the valid =  11 0.47362759709358215\n",
      "Iteration 12 \t\t Training Loss: 0.35165050625801086 \t\t Validation Loss: 0.3656268169482549\n",
      "loss.item of the train =  12 0.35205936431884766\n",
      "loss.item of the valid =  12 0.35222214460372925\n",
      "loss.item of the valid =  12 0.3902899920940399\n",
      "loss.item of the valid =  12 0.3530266284942627\n",
      "loss.item of the valid =  12 0.29766926169395447\n",
      "loss.item of the valid =  12 0.32410481572151184\n",
      "loss.item of the valid =  12 0.47255945205688477\n",
      "Iteration 13 \t\t Training Loss: 0.35205936431884766 \t\t Validation Loss: 0.36497871577739716\n",
      "loss.item of the train =  13 0.3523653745651245\n",
      "loss.item of the valid =  13 0.3518495559692383\n",
      "loss.item of the valid =  13 0.38978180289268494\n",
      "loss.item of the valid =  13 0.3527795672416687\n",
      "loss.item of the valid =  13 0.29759299755096436\n",
      "loss.item of the valid =  13 0.32339537143707275\n",
      "loss.item of the valid =  13 0.47201645374298096\n",
      "Iteration 14 \t\t Training Loss: 0.3523653745651245 \t\t Validation Loss: 0.364569291472435\n",
      "loss.item of the train =  14 0.35273826122283936\n",
      "loss.item of the valid =  14 0.35156452655792236\n",
      "loss.item of the valid =  14 0.3894272446632385\n",
      "loss.item of the valid =  14 0.3526608943939209\n",
      "loss.item of the valid =  14 0.29748302698135376\n",
      "loss.item of the valid =  14 0.3229827880859375\n",
      "loss.item of the valid =  14 0.47165513038635254\n",
      "Iteration 15 \t\t Training Loss: 0.35273826122283936 \t\t Validation Loss: 0.3642956018447876\n",
      "loss.item of the train =  15 0.35294514894485474\n",
      "loss.item of the valid =  15 0.35131269693374634\n",
      "loss.item of the valid =  15 0.38915956020355225\n",
      "loss.item of the valid =  15 0.35251399874687195\n",
      "loss.item of the valid =  15 0.29750344157218933\n",
      "loss.item of the valid =  15 0.32254964113235474\n",
      "loss.item of the valid =  15 0.47135254740715027\n",
      "Iteration 16 \t\t Training Loss: 0.35294514894485474 \t\t Validation Loss: 0.36406531433264416\n",
      "loss.item of the train =  16 0.35311391949653625\n",
      "loss.item of the valid =  16 0.35112595558166504\n",
      "loss.item of the valid =  16 0.38901379704475403\n",
      "loss.item of the valid =  16 0.35250821709632874\n",
      "loss.item of the valid =  16 0.29757973551750183\n",
      "loss.item of the valid =  16 0.3222636580467224\n",
      "loss.item of the valid =  16 0.47131219506263733\n",
      "Iteration 17 \t\t Training Loss: 0.35311391949653625 \t\t Validation Loss: 0.3639672597249349\n",
      "loss.item of the train =  17 0.35320937633514404\n",
      "loss.item of the valid =  17 0.35101792216300964\n",
      "loss.item of the valid =  17 0.38896501064300537\n",
      "loss.item of the valid =  17 0.3525952100753784\n",
      "loss.item of the valid =  17 0.29767411947250366\n",
      "loss.item of the valid =  17 0.3221297562122345\n",
      "loss.item of the valid =  17 0.4713832139968872\n",
      "Iteration 18 \t\t Training Loss: 0.35320937633514404 \t\t Validation Loss: 0.3639608720938365\n",
      "loss.item of the train =  18 0.35325857996940613\n",
      "loss.item of the valid =  18 0.35097864270210266\n",
      "loss.item of the valid =  18 0.3889996111392975\n",
      "loss.item of the valid =  18 0.352760374546051\n",
      "loss.item of the valid =  18 0.29778817296028137\n",
      "loss.item of the valid =  18 0.32213926315307617\n",
      "loss.item of the valid =  18 0.4715413451194763\n",
      "Iteration 19 \t\t Training Loss: 0.35325857996940613 \t\t Validation Loss: 0.3640345682700475\n",
      "loss.item of the train =  19 0.35326746106147766\n",
      "loss.item of the valid =  19 0.35100361704826355\n",
      "loss.item of the valid =  19 0.38910797238349915\n",
      "loss.item of the valid =  19 0.35299360752105713\n",
      "loss.item of the valid =  19 0.29792121052742004\n",
      "loss.item of the valid =  19 0.32228100299835205\n",
      "loss.item of the valid =  19 0.4717712700366974\n",
      "Iteration 20 \t\t Training Loss: 0.35326746106147766 \t\t Validation Loss: 0.36417978008588153\n",
      "loss.item of the train =  20 0.3532413840293884\n",
      "loss.item of the valid =  20 0.35108935832977295\n",
      "loss.item of the valid =  20 0.38925808668136597\n",
      "loss.item of the valid =  20 0.35325944423675537\n",
      "loss.item of the valid =  20 0.29806676506996155\n",
      "loss.item of the valid =  20 0.32253116369247437\n",
      "loss.item of the valid =  20 0.47206342220306396\n",
      "Iteration 21 \t\t Training Loss: 0.3532413840293884 \t\t Validation Loss: 0.3643780400355657\n",
      "loss.item of the train =  21 0.3531873822212219\n",
      "loss.item of the valid =  21 0.3512451648712158\n",
      "loss.item of the valid =  21 0.389350026845932\n",
      "loss.item of the valid =  21 0.3534129559993744\n",
      "loss.item of the valid =  21 0.2980877161026001\n",
      "loss.item of the valid =  21 0.32281357049942017\n",
      "loss.item of the valid =  21 0.4723489284515381\n",
      "Iteration 22 \t\t Training Loss: 0.3531873822212219 \t\t Validation Loss: 0.3645430604616801\n",
      "loss.item of the train =  22 0.35308581590652466\n",
      "loss.item of the valid =  22 0.35140281915664673\n",
      "loss.item of the valid =  22 0.38940081000328064\n",
      "loss.item of the valid =  22 0.3535653352737427\n",
      "loss.item of the valid =  22 0.2980950176715851\n",
      "loss.item of the valid =  22 0.3230189085006714\n",
      "loss.item of the valid =  22 0.47247251868247986\n",
      "Iteration 23 \t\t Training Loss: 0.35308581590652466 \t\t Validation Loss: 0.36465923488140106\n",
      "loss.item of the train =  23 0.352975457906723\n",
      "loss.item of the valid =  23 0.35145264863967896\n",
      "loss.item of the valid =  23 0.38979044556617737\n",
      "loss.item of the valid =  23 0.35390394926071167\n",
      "loss.item of the valid =  23 0.2980188727378845\n",
      "loss.item of the valid =  23 0.3231653571128845\n",
      "loss.item of the valid =  23 0.4728166162967682\n",
      "Iteration 24 \t\t Training Loss: 0.352975457906723 \t\t Validation Loss: 0.3648579816023509\n",
      "loss.item of the train =  24 0.3528459072113037\n",
      "loss.item of the valid =  24 0.3515430688858032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item of the valid =  24 0.39017343521118164\n",
      "loss.item of the valid =  24 0.3539820611476898\n",
      "loss.item of the valid =  24 0.2979559600353241\n",
      "loss.item of the valid =  24 0.3232364356517792\n",
      "loss.item of the valid =  24 0.4731743037700653\n",
      "Iteration 25 \t\t Training Loss: 0.3528459072113037 \t\t Validation Loss: 0.3650108774503072\n",
      "loss.item of the train =  25 0.3527354300022125\n",
      "loss.item of the valid =  25 0.35173794627189636\n",
      "loss.item of the valid =  25 0.3903728425502777\n",
      "loss.item of the valid =  25 0.353945255279541\n",
      "loss.item of the valid =  25 0.2978944182395935\n",
      "loss.item of the valid =  25 0.32333728671073914\n",
      "loss.item of the valid =  25 0.47355926036834717\n",
      "Iteration 26 \t\t Training Loss: 0.3527354300022125 \t\t Validation Loss: 0.3651411682367325\n",
      "loss.item of the train =  26 0.35263559222221375\n",
      "loss.item of the valid =  26 0.3519977331161499\n",
      "loss.item of the valid =  26 0.39053773880004883\n",
      "loss.item of the valid =  26 0.35394299030303955\n",
      "loss.item of the valid =  26 0.29786205291748047\n",
      "loss.item of the valid =  26 0.32351154088974\n",
      "loss.item of the valid =  26 0.47399869561195374\n",
      "Iteration 27 \t\t Training Loss: 0.35263559222221375 \t\t Validation Loss: 0.3653084586064021\n",
      "loss.item of the train =  27 0.35255855321884155\n",
      "loss.item of the valid =  27 0.3523862063884735\n",
      "loss.item of the valid =  27 0.39083585143089294\n",
      "loss.item of the valid =  27 0.35399529337882996\n",
      "loss.item of the valid =  27 0.29764196276664734\n",
      "loss.item of the valid =  27 0.3237539529800415\n",
      "loss.item of the valid =  27 0.4746076762676239\n",
      "Iteration 28 \t\t Training Loss: 0.35255855321884155 \t\t Validation Loss: 0.3655368238687515\n",
      "loss.item of the train =  28 0.35238906741142273\n",
      "loss.item of the valid =  28 0.35289931297302246\n",
      "loss.item of the valid =  28 0.3909568190574646\n",
      "loss.item of the valid =  28 0.3539827764034271\n",
      "loss.item of the valid =  28 0.29747265577316284\n",
      "loss.item of the valid =  28 0.32406872510910034\n",
      "loss.item of the valid =  28 0.4750440716743469\n",
      "Iteration 29 \t\t Training Loss: 0.35238906741142273 \t\t Validation Loss: 0.3657373934984207\n",
      "loss.item of the train =  29 0.35217997431755066\n",
      "loss.item of the valid =  29 0.3534354865550995\n",
      "loss.item of the valid =  29 0.39214515686035156\n",
      "loss.item of the valid =  29 0.3541485667228699\n",
      "loss.item of the valid =  29 0.29736509919166565\n",
      "loss.item of the valid =  29 0.3244791626930237\n",
      "loss.item of the valid =  29 0.4757806956768036\n",
      "Iteration 30 \t\t Training Loss: 0.35217997431755066 \t\t Validation Loss: 0.3662256946166356\n",
      "loss.item of the train =  30 0.3519574701786041\n",
      "loss.item of the valid =  30 0.3539956212043762\n",
      "loss.item of the valid =  30 0.3933965265750885\n",
      "loss.item of the valid =  30 0.35444292426109314\n",
      "loss.item of the valid =  30 0.29729965329170227\n",
      "loss.item of the valid =  30 0.3250965476036072\n",
      "loss.item of the valid =  30 0.4766477942466736\n",
      "Iteration 31 \t\t Training Loss: 0.3519574701786041 \t\t Validation Loss: 0.36681317786375683\n",
      "loss.item of the train =  31 0.35164517164230347\n",
      "loss.item of the valid =  31 0.3546234369277954\n",
      "loss.item of the valid =  31 0.39472949504852295\n",
      "loss.item of the valid =  31 0.3548794388771057\n",
      "loss.item of the valid =  31 0.29730579257011414\n",
      "loss.item of the valid =  31 0.3258205056190491\n",
      "loss.item of the valid =  31 0.4777062237262726\n",
      "Iteration 32 \t\t Training Loss: 0.35164517164230347 \t\t Validation Loss: 0.3675108154614766\n",
      "loss.item of the train =  32 0.352818101644516\n",
      "loss.item of the valid =  32 0.3543160557746887\n",
      "loss.item of the valid =  32 0.3938479423522949\n",
      "loss.item of the valid =  32 0.35432136058807373\n",
      "loss.item of the valid =  32 0.2974376678466797\n",
      "loss.item of the valid =  32 0.3255331218242645\n",
      "loss.item of the valid =  32 0.47660112380981445\n",
      "Iteration 33 \t\t Training Loss: 0.352818101644516 \t\t Validation Loss: 0.36700954536596936\n",
      "loss.item of the train =  33 0.35147422552108765\n",
      "loss.item of the valid =  33 0.3540169298648834\n",
      "loss.item of the valid =  33 0.39309221506118774\n",
      "loss.item of the valid =  33 0.35391291975975037\n",
      "loss.item of the valid =  33 0.29762402176856995\n",
      "loss.item of the valid =  33 0.325156033039093\n",
      "loss.item of the valid =  33 0.47588586807250977\n",
      "Iteration 34 \t\t Training Loss: 0.35147422552108765 \t\t Validation Loss: 0.3666146645943324\n",
      "loss.item of the train =  34 0.35153764486312866\n",
      "loss.item of the valid =  34 0.35382893681526184\n",
      "loss.item of the valid =  34 0.3925688862800598\n",
      "loss.item of the valid =  34 0.35363298654556274\n",
      "loss.item of the valid =  34 0.29771608114242554\n",
      "loss.item of the valid =  34 0.32502996921539307\n",
      "loss.item of the valid =  34 0.4754590392112732\n",
      "Iteration 35 \t\t Training Loss: 0.35153764486312866 \t\t Validation Loss: 0.36637264986832935\n",
      "loss.item of the train =  35 0.3516112267971039\n",
      "loss.item of the valid =  35 0.35374194383621216\n",
      "loss.item of the valid =  35 0.3924902677536011\n",
      "loss.item of the valid =  35 0.3536478877067566\n",
      "loss.item of the valid =  35 0.297713965177536\n",
      "loss.item of the valid =  35 0.3248511254787445\n",
      "loss.item of the valid =  35 0.47551271319389343\n",
      "Iteration 36 \t\t Training Loss: 0.3516112267971039 \t\t Validation Loss: 0.36632631719112396\n",
      "loss.item of the train =  36 0.3516194820404053\n",
      "loss.item of the valid =  36 0.3537692129611969\n",
      "loss.item of the valid =  36 0.3927074074745178\n",
      "loss.item of the valid =  36 0.3538590371608734\n",
      "loss.item of the valid =  36 0.29764044284820557\n",
      "loss.item of the valid =  36 0.3247824013233185\n",
      "loss.item of the valid =  36 0.47590136528015137\n",
      "Iteration 37 \t\t Training Loss: 0.3516194820404053 \t\t Validation Loss: 0.3664433111747106\n",
      "loss.item of the train =  37 0.35158655047416687\n",
      "loss.item of the valid =  37 0.35387009382247925\n",
      "loss.item of the valid =  37 0.39300814270973206\n",
      "loss.item of the valid =  37 0.35402408242225647\n",
      "loss.item of the valid =  37 0.29759466648101807\n",
      "loss.item of the valid =  37 0.32484331727027893\n",
      "loss.item of the valid =  37 0.47625309228897095\n",
      "Iteration 38 \t\t Training Loss: 0.35158655047416687 \t\t Validation Loss: 0.3665988991657893\n",
      "loss.item of the train =  38 0.35151493549346924\n",
      "loss.item of the valid =  38 0.35405486822128296\n",
      "loss.item of the valid =  38 0.3934613764286041\n",
      "loss.item of the valid =  38 0.3542974591255188\n",
      "loss.item of the valid =  38 0.29756343364715576\n",
      "loss.item of the valid =  38 0.32505810260772705\n",
      "loss.item of the valid =  38 0.47672998905181885\n",
      "Iteration 39 \t\t Training Loss: 0.35151493549346924 \t\t Validation Loss: 0.36686087151368457\n",
      "loss.item of the train =  39 0.3514101505279541\n",
      "loss.item of the valid =  39 0.35434818267822266\n",
      "loss.item of the valid =  39 0.3940914571285248\n",
      "loss.item of the valid =  39 0.3545968532562256\n",
      "loss.item of the valid =  39 0.2975463271141052\n",
      "loss.item of the valid =  39 0.3253817558288574\n",
      "loss.item of the valid =  39 0.4773359000682831\n",
      "Iteration 40 \t\t Training Loss: 0.3514101505279541 \t\t Validation Loss: 0.3672167460123698\n",
      "loss.item of the train =  40 0.3516477346420288\n",
      "loss.item of the valid =  40 0.35342565178871155\n",
      "loss.item of the valid =  40 0.39224961400032043\n",
      "loss.item of the valid =  40 0.35348454117774963\n",
      "loss.item of the valid =  40 0.29733189940452576\n",
      "loss.item of the valid =  40 0.3242797553539276\n",
      "loss.item of the valid =  40 0.47516149282455444\n",
      "Iteration 41 \t\t Training Loss: 0.3516477346420288 \t\t Validation Loss: 0.3659888257582982\n",
      "loss.item of the train =  41 0.35163596272468567\n",
      "loss.item of the valid =  41 0.35271045565605164\n",
      "loss.item of the valid =  41 0.3908056616783142\n",
      "loss.item of the valid =  41 0.35258111357688904\n",
      "loss.item of the valid =  41 0.2972046732902527\n",
      "loss.item of the valid =  41 0.3234242796897888\n",
      "loss.item of the valid =  41 0.4734177589416504\n",
      "Iteration 42 \t\t Training Loss: 0.35163596272468567 \t\t Validation Loss: 0.3650239904721578\n",
      "loss.item of the train =  42 0.3519288897514343\n",
      "loss.item of the valid =  42 0.35213667154312134\n",
      "loss.item of the valid =  42 0.38971641659736633\n",
      "loss.item of the valid =  42 0.35188013315200806\n",
      "loss.item of the valid =  42 0.2971678078174591\n",
      "loss.item of the valid =  42 0.32272958755493164\n",
      "loss.item of the valid =  42 0.4720344841480255\n",
      "Iteration 43 \t\t Training Loss: 0.3519288897514343 \t\t Validation Loss: 0.364277516802152\n",
      "loss.item of the train =  43 0.35215723514556885\n",
      "loss.item of the valid =  43 0.3516663610935211\n",
      "loss.item of the valid =  43 0.38909611105918884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item of the valid =  43 0.35137632489204407\n",
      "loss.item of the valid =  43 0.2971450984477997\n",
      "loss.item of the valid =  43 0.322230726480484\n",
      "loss.item of the valid =  43 0.4710312783718109\n",
      "Iteration 44 \t\t Training Loss: 0.35215723514556885 \t\t Validation Loss: 0.3637576500574748\n",
      "loss.item of the train =  44 0.3523136079311371\n",
      "loss.item of the valid =  44 0.3512944281101227\n",
      "loss.item of the valid =  44 0.3886096179485321\n",
      "loss.item of the valid =  44 0.3509925901889801\n",
      "loss.item of the valid =  44 0.2971130609512329\n",
      "loss.item of the valid =  44 0.3218202292919159\n",
      "loss.item of the valid =  44 0.47024717926979065\n",
      "Iteration 45 \t\t Training Loss: 0.3523136079311371 \t\t Validation Loss: 0.3633461842934291\n",
      "loss.item of the train =  45 0.3524223268032074\n",
      "loss.item of the valid =  45 0.35101643204689026\n",
      "loss.item of the valid =  45 0.3882302939891815\n",
      "loss.item of the valid =  45 0.350691556930542\n",
      "loss.item of the valid =  45 0.2970832884311676\n",
      "loss.item of the valid =  45 0.3215077519416809\n",
      "loss.item of the valid =  45 0.4696281850337982\n",
      "Iteration 46 \t\t Training Loss: 0.3524223268032074 \t\t Validation Loss: 0.3630262513955434\n",
      "loss.item of the train =  46 0.3525353968143463\n",
      "loss.item of the valid =  46 0.3508080542087555\n",
      "loss.item of the valid =  46 0.3879939615726471\n",
      "loss.item of the valid =  46 0.35049259662628174\n",
      "loss.item of the valid =  46 0.297031968832016\n",
      "loss.item of the valid =  46 0.32124653458595276\n",
      "loss.item of the valid =  46 0.46917450428009033\n",
      "Iteration 47 \t\t Training Loss: 0.3525353968143463 \t\t Validation Loss: 0.3627912700176239\n",
      "loss.item of the train =  47 0.3526126444339752\n",
      "loss.item of the valid =  47 0.35065311193466187\n",
      "loss.item of the valid =  47 0.3878442645072937\n",
      "loss.item of the valid =  47 0.35035741329193115\n",
      "loss.item of the valid =  47 0.29697385430336\n",
      "loss.item of the valid =  47 0.32108762860298157\n",
      "loss.item of the valid =  47 0.4688837230205536\n",
      "Iteration 48 \t\t Training Loss: 0.3526126444339752 \t\t Validation Loss: 0.3626333326101303\n",
      "loss.item of the train =  48 0.3526514768600464\n",
      "loss.item of the valid =  48 0.35054701566696167\n",
      "loss.item of the valid =  48 0.3876955211162567\n",
      "loss.item of the valid =  48 0.3502178490161896\n",
      "loss.item of the valid =  48 0.29693931341171265\n",
      "loss.item of the valid =  48 0.32094958424568176\n",
      "loss.item of the valid =  48 0.4686422049999237\n",
      "Iteration 49 \t\t Training Loss: 0.3526514768600464 \t\t Validation Loss: 0.36249858140945435\n",
      "loss.item of the train =  49 0.3526860177516937\n",
      "loss.item of the valid =  49 0.3504844009876251\n",
      "loss.item of the valid =  49 0.38756516575813293\n",
      "loss.item of the valid =  49 0.3501007556915283\n",
      "loss.item of the valid =  49 0.2969322204589844\n",
      "loss.item of the valid =  49 0.32087743282318115\n",
      "loss.item of the valid =  49 0.46847617626190186\n",
      "Iteration 50 \t\t Training Loss: 0.3526860177516937 \t\t Validation Loss: 0.36240602533022565\n",
      "loss.item of the train =  50 0.3527139127254486\n",
      "loss.item of the valid =  50 0.35045185685157776\n",
      "loss.item of the valid =  50 0.3874624967575073\n",
      "loss.item of the valid =  50 0.3500245213508606\n",
      "loss.item of the valid =  50 0.29692381620407104\n",
      "loss.item of the valid =  50 0.32084810733795166\n",
      "loss.item of the valid =  50 0.4683947265148163\n",
      "Iteration 51 \t\t Training Loss: 0.3527139127254486 \t\t Validation Loss: 0.3623509208361308\n",
      "loss.item of the train =  51 0.35273265838623047\n",
      "loss.item of the valid =  51 0.3504316210746765\n",
      "loss.item of the valid =  51 0.38745033740997314\n",
      "loss.item of the valid =  51 0.35002437233924866\n",
      "loss.item of the valid =  51 0.2969093322753906\n",
      "loss.item of the valid =  51 0.3208256661891937\n",
      "loss.item of the valid =  51 0.46840542554855347\n",
      "Iteration 52 \t\t Training Loss: 0.35273265838623047 \t\t Validation Loss: 0.36234112580617267\n",
      "loss.item of the train =  52 0.3527277410030365\n",
      "loss.item of the valid =  52 0.35043051838874817\n",
      "loss.item of the valid =  52 0.3875173330307007\n",
      "loss.item of the valid =  52 0.35007044672966003\n",
      "loss.item of the valid =  52 0.2968883216381073\n",
      "loss.item of the valid =  52 0.3208155632019043\n",
      "loss.item of the valid =  52 0.468472957611084\n",
      "Iteration 53 \t\t Training Loss: 0.3527277410030365 \t\t Validation Loss: 0.36236585676670074\n",
      "loss.item of the train =  53 0.3527168035507202\n",
      "loss.item of the valid =  53 0.35046833753585815\n",
      "loss.item of the valid =  53 0.38759806752204895\n",
      "loss.item of the valid =  53 0.3501288592815399\n",
      "loss.item of the valid =  53 0.2968790829181671\n",
      "loss.item of the valid =  53 0.3208520710468292\n",
      "loss.item of the valid =  53 0.46859151124954224\n",
      "Iteration 54 \t\t Training Loss: 0.3527168035507202 \t\t Validation Loss: 0.36241965492566425\n",
      "loss.item of the train =  54 0.35270214080810547\n",
      "loss.item of the valid =  54 0.35053861141204834\n",
      "loss.item of the valid =  54 0.3876909017562866\n",
      "loss.item of the valid =  54 0.3502066731452942\n",
      "loss.item of the valid =  54 0.2968815565109253\n",
      "loss.item of the valid =  54 0.32092735171318054\n",
      "loss.item of the valid =  54 0.4687599241733551\n",
      "Iteration 55 \t\t Training Loss: 0.35270214080810547 \t\t Validation Loss: 0.36250083645184833\n",
      "loss.item of the train =  55 0.3526764512062073\n",
      "loss.item of the valid =  55 0.35063958168029785\n",
      "loss.item of the valid =  55 0.3878058195114136\n",
      "loss.item of the valid =  55 0.3503037691116333\n",
      "loss.item of the valid =  55 0.2968940734863281\n",
      "loss.item of the valid =  55 0.32104039192199707\n",
      "loss.item of the valid =  55 0.4689735174179077\n",
      "Iteration 56 \t\t Training Loss: 0.3526764512062073 \t\t Validation Loss: 0.36260952552159625\n",
      "loss.item of the train =  56 0.35264232754707336\n",
      "loss.item of the valid =  56 0.35076457262039185\n",
      "loss.item of the valid =  56 0.3879462480545044\n",
      "loss.item of the valid =  56 0.3504261076450348\n",
      "loss.item of the valid =  56 0.29692789912223816\n",
      "loss.item of the valid =  56 0.32118546962738037\n",
      "loss.item of the valid =  56 0.46922922134399414\n",
      "Iteration 57 \t\t Training Loss: 0.35264232754707336 \t\t Validation Loss: 0.36274658640225727\n",
      "loss.item of the train =  57 0.3526005744934082\n",
      "loss.item of the valid =  57 0.35090965032577515\n",
      "loss.item of the valid =  57 0.3881441056728363\n",
      "loss.item of the valid =  57 0.35059496760368347\n",
      "loss.item of the valid =  57 0.29696348309516907\n",
      "loss.item of the valid =  57 0.32135602831840515\n",
      "loss.item of the valid =  57 0.4695377051830292\n",
      "Iteration 58 \t\t Training Loss: 0.3526005744934082 \t\t Validation Loss: 0.3629176566998164\n",
      "loss.item of the train =  58 0.3525480628013611\n",
      "loss.item of the valid =  58 0.3510771095752716\n",
      "loss.item of the valid =  58 0.38837072253227234\n",
      "loss.item of the valid =  58 0.3507908582687378\n",
      "loss.item of the valid =  58 0.2969988286495209\n",
      "loss.item of the valid =  58 0.32155346870422363\n",
      "loss.item of the valid =  58 0.46989333629608154\n",
      "Iteration 59 \t\t Training Loss: 0.3525480628013611 \t\t Validation Loss: 0.3631140540043513\n",
      "loss.item of the train =  59 0.3524881601333618\n",
      "loss.item of the valid =  59 0.351276695728302\n",
      "loss.item of the valid =  59 0.38865554332733154\n",
      "loss.item of the valid =  59 0.3510333299636841\n",
      "loss.item of the valid =  59 0.2970469892024994\n",
      "loss.item of the valid =  59 0.32176563143730164\n",
      "loss.item of the valid =  59 0.47031641006469727\n",
      "Iteration 60 \t\t Training Loss: 0.3524881601333618 \t\t Validation Loss: 0.3633490999539693\n",
      "loss.item of the train =  60 0.35241594910621643\n",
      "loss.item of the valid =  60 0.3515065014362335\n",
      "loss.item of the valid =  60 0.3889700472354889\n",
      "loss.item of the valid =  60 0.3512963056564331\n",
      "loss.item of the valid =  60 0.297100692987442\n",
      "loss.item of the valid =  60 0.3220207691192627\n",
      "loss.item of the valid =  60 0.470797598361969\n",
      "Iteration 61 \t\t Training Loss: 0.35241594910621643 \t\t Validation Loss: 0.36361531913280487\n",
      "loss.item of the train =  61 0.3523353040218353\n",
      "loss.item of the valid =  61 0.35177376866340637\n",
      "loss.item of the valid =  61 0.38930436968803406\n",
      "loss.item of the valid =  61 0.3515802323818207\n",
      "loss.item of the valid =  61 0.2971571683883667\n",
      "loss.item of the valid =  61 0.3223133683204651\n",
      "loss.item of the valid =  61 0.47133877873420715\n",
      "Iteration 62 \t\t Training Loss: 0.3523353040218353 \t\t Validation Loss: 0.36391128102938336\n",
      "loss.item of the train =  62 0.35224470496177673\n",
      "loss.item of the valid =  62 0.35205137729644775\n",
      "loss.item of the valid =  62 0.3896797299385071\n",
      "loss.item of the valid =  62 0.3519032299518585\n",
      "loss.item of the valid =  62 0.2972129285335541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item of the valid =  62 0.32269221544265747\n",
      "loss.item of the valid =  62 0.47194889187812805\n",
      "Iteration 63 \t\t Training Loss: 0.35224470496177673 \t\t Validation Loss: 0.3642480621735255\n",
      "loss.item of the train =  63 0.3521175682544708\n",
      "loss.item of the valid =  63 0.352382630109787\n",
      "loss.item of the valid =  63 0.39011192321777344\n",
      "loss.item of the valid =  63 0.35225197672843933\n",
      "loss.item of the valid =  63 0.29727447032928467\n",
      "loss.item of the valid =  63 0.32309022545814514\n",
      "loss.item of the valid =  63 0.47268030047416687\n",
      "Iteration 64 \t\t Training Loss: 0.3521175682544708 \t\t Validation Loss: 0.36463192105293274\n",
      "loss.item of the train =  64 0.3519856035709381\n",
      "loss.item of the valid =  64 0.35273614525794983\n",
      "loss.item of the valid =  64 0.3908405601978302\n",
      "loss.item of the valid =  64 0.3526584804058075\n",
      "loss.item of the valid =  64 0.29733747243881226\n",
      "loss.item of the valid =  64 0.3235118091106415\n",
      "loss.item of the valid =  64 0.4734836518764496\n",
      "Iteration 65 \t\t Training Loss: 0.3519856035709381 \t\t Validation Loss: 0.36509468654791516\n",
      "loss.item of the train =  65 0.3518424928188324\n",
      "loss.item of the valid =  65 0.3531407415866852\n",
      "loss.item of the valid =  65 0.39167356491088867\n",
      "loss.item of the valid =  65 0.35310494899749756\n",
      "loss.item of the valid =  65 0.2973916530609131\n",
      "loss.item of the valid =  65 0.3239717483520508\n",
      "loss.item of the valid =  65 0.4743935167789459\n",
      "Iteration 66 \t\t Training Loss: 0.3518424928188324 \t\t Validation Loss: 0.3656126956144969\n",
      "loss.item of the train =  66 0.3516848087310791\n",
      "loss.item of the valid =  66 0.35357865691185\n",
      "loss.item of the valid =  66 0.3925440013408661\n",
      "loss.item of the valid =  66 0.35358864068984985\n",
      "loss.item of the valid =  66 0.2974744737148285\n",
      "loss.item of the valid =  66 0.32450199127197266\n",
      "loss.item of the valid =  66 0.47537365555763245\n",
      "Iteration 67 \t\t Training Loss: 0.3516848087310791 \t\t Validation Loss: 0.36617690324783325\n",
      "loss.item of the train =  67 0.35151833295822144\n",
      "loss.item of the valid =  67 0.3540659546852112\n",
      "loss.item of the valid =  67 0.39350762963294983\n",
      "loss.item of the valid =  67 0.35413357615470886\n",
      "loss.item of the valid =  67 0.29756808280944824\n",
      "loss.item of the valid =  67 0.3250935673713684\n",
      "loss.item of the valid =  67 0.47645723819732666\n",
      "Iteration 68 \t\t Training Loss: 0.35151833295822144 \t\t Validation Loss: 0.3668043414751689\n",
      "loss.item of the train =  68 0.35133248567581177\n",
      "loss.item of the valid =  68 0.3546156883239746\n",
      "loss.item of the valid =  68 0.3945867717266083\n",
      "loss.item of the valid =  68 0.3547368347644806\n",
      "loss.item of the valid =  68 0.2976723313331604\n",
      "loss.item of the valid =  68 0.3257484436035156\n",
      "loss.item of the valid =  68 0.477655827999115\n",
      "Iteration 69 \t\t Training Loss: 0.35133248567581177 \t\t Validation Loss: 0.3675026496251424\n",
      "loss.item of the train =  69 0.3529646098613739\n",
      "loss.item of the valid =  69 0.35380569100379944\n",
      "loss.item of the valid =  69 0.3929610550403595\n",
      "loss.item of the valid =  69 0.35379910469055176\n",
      "loss.item of the valid =  69 0.2975442409515381\n",
      "loss.item of the valid =  69 0.3248256742954254\n",
      "loss.item of the valid =  69 0.4758448004722595\n",
      "Iteration 70 \t\t Training Loss: 0.3529646098613739 \t\t Validation Loss: 0.36646342774232227\n",
      "loss.item of the train =  70 0.35145407915115356\n",
      "loss.item of the valid =  70 0.35316920280456543\n",
      "loss.item of the valid =  70 0.3916555941104889\n",
      "loss.item of the valid =  70 0.3530992269515991\n",
      "loss.item of the valid =  70 0.29741349816322327\n",
      "loss.item of the valid =  70 0.3240549862384796\n",
      "loss.item of the valid =  70 0.4744587242603302\n",
      "Iteration 71 \t\t Training Loss: 0.35145407915115356 \t\t Validation Loss: 0.36564187208811444\n",
      "loss.item of the train =  71 0.35169142484664917\n",
      "loss.item of the valid =  71 0.3526499271392822\n",
      "loss.item of the valid =  71 0.3906141221523285\n",
      "loss.item of the valid =  71 0.352581262588501\n",
      "loss.item of the valid =  71 0.2973114252090454\n",
      "loss.item of the valid =  71 0.32342809438705444\n",
      "loss.item of the valid =  71 0.47335752844810486\n",
      "Iteration 72 \t\t Training Loss: 0.35169142484664917 \t\t Validation Loss: 0.3649903933207194\n",
      "loss.item of the train =  72 0.35187050700187683\n",
      "loss.item of the valid =  72 0.3522629737854004\n",
      "loss.item of the valid =  72 0.3899928629398346\n",
      "loss.item of the valid =  72 0.3521689176559448\n",
      "loss.item of the valid =  72 0.29723572731018066\n",
      "loss.item of the valid =  72 0.3229544162750244\n",
      "loss.item of the valid =  72 0.47252875566482544\n",
      "Iteration 73 \t\t Training Loss: 0.35187050700187683 \t\t Validation Loss: 0.3645239422718684\n",
      "loss.item of the train =  73 0.35202011466026306\n",
      "loss.item of the valid =  73 0.3519804775714874\n",
      "loss.item of the valid =  73 0.38961872458457947\n",
      "loss.item of the valid =  73 0.3518548011779785\n",
      "loss.item of the valid =  73 0.2971739172935486\n",
      "loss.item of the valid =  73 0.32260096073150635\n",
      "loss.item of the valid =  73 0.4719085097312927\n",
      "Iteration 74 \t\t Training Loss: 0.35202011466026306 \t\t Validation Loss: 0.3641895651817322\n",
      "loss.item of the train =  74 0.35213834047317505\n",
      "loss.item of the valid =  74 0.3517884314060211\n",
      "loss.item of the valid =  74 0.3893600404262543\n",
      "loss.item of the valid =  74 0.35162970423698425\n",
      "loss.item of the valid =  74 0.29713159799575806\n",
      "loss.item of the valid =  74 0.3223336935043335\n",
      "loss.item of the valid =  74 0.47149091958999634\n",
      "Iteration 75 \t\t Training Loss: 0.35213834047317505 \t\t Validation Loss: 0.3639557311932246\n",
      "loss.item of the train =  75 0.3522258400917053\n",
      "loss.item of the valid =  75 0.35165172815322876\n",
      "loss.item of the valid =  75 0.389178067445755\n",
      "loss.item of the valid =  75 0.35146626830101013\n",
      "loss.item of the valid =  75 0.2971106767654419\n",
      "loss.item of the valid =  75 0.3221498727798462\n",
      "loss.item of the valid =  75 0.4711912274360657\n",
      "Iteration 76 \t\t Training Loss: 0.3522258400917053 \t\t Validation Loss: 0.3637913068135579\n",
      "loss.item of the train =  76 0.35228386521339417\n",
      "loss.item of the valid =  76 0.3515656590461731\n",
      "loss.item of the valid =  76 0.38906165957450867\n",
      "loss.item of the valid =  76 0.35135892033576965\n",
      "loss.item of the valid =  76 0.2970992624759674\n",
      "loss.item of the valid =  76 0.32202914357185364\n",
      "loss.item of the valid =  76 0.4709988236427307\n",
      "Iteration 77 \t\t Training Loss: 0.35228386521339417 \t\t Validation Loss: 0.36368557810783386\n",
      "loss.item of the train =  77 0.35232168436050415\n",
      "loss.item of the valid =  77 0.3515247702598572\n",
      "loss.item of the valid =  77 0.3890007734298706\n",
      "loss.item of the valid =  77 0.3512974977493286\n",
      "loss.item of the valid =  77 0.29709821939468384\n",
      "loss.item of the valid =  77 0.3219672441482544\n",
      "loss.item of the valid =  77 0.4709031581878662\n",
      "Iteration 78 \t\t Training Loss: 0.35232168436050415 \t\t Validation Loss: 0.3636319438616435\n",
      "loss.item of the train =  78 0.35234102606773376\n",
      "loss.item of the valid =  78 0.3515438735485077\n",
      "loss.item of the valid =  78 0.3889796733856201\n",
      "loss.item of the valid =  78 0.35129761695861816\n",
      "loss.item of the valid =  78 0.2971018850803375\n",
      "loss.item of the valid =  78 0.32196635007858276\n",
      "loss.item of the valid =  78 0.4709019660949707\n",
      "Iteration 79 \t\t Training Loss: 0.35234102606773376 \t\t Validation Loss: 0.36363189419110614\n",
      "loss.item of the train =  79 0.35234537720680237\n",
      "loss.item of the valid =  79 0.35158634185791016\n",
      "loss.item of the valid =  79 0.38901156187057495\n",
      "loss.item of the valid =  79 0.35133522748947144\n",
      "loss.item of the valid =  79 0.2971111834049225\n",
      "loss.item of the valid =  79 0.322042852640152\n",
      "loss.item of the valid =  79 0.47097426652908325\n",
      "Iteration 80 \t\t Training Loss: 0.35234537720680237 \t\t Validation Loss: 0.36367690563201904\n",
      "loss.item of the train =  80 0.3523158133029938\n",
      "loss.item of the valid =  80 0.35165682435035706\n",
      "loss.item of the valid =  80 0.3890954852104187\n",
      "loss.item of the valid =  80 0.35140395164489746\n",
      "loss.item of the valid =  80 0.29713138937950134\n",
      "loss.item of the valid =  80 0.3221595883369446\n",
      "loss.item of the valid =  80 0.47112229466438293\n",
      "Iteration 81 \t\t Training Loss: 0.3523158133029938 \t\t Validation Loss: 0.3637615889310837\n",
      "loss.item of the train =  81 0.352275550365448\n",
      "loss.item of the valid =  81 0.35175424814224243\n",
      "loss.item of the valid =  81 0.3892279267311096\n",
      "loss.item of the valid =  81 0.35150736570358276\n",
      "loss.item of the valid =  81 0.2971615791320801\n",
      "loss.item of the valid =  81 0.32231372594833374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item of the valid =  81 0.4713403582572937\n",
      "Iteration 82 \t\t Training Loss: 0.352275550365448 \t\t Validation Loss: 0.36388420065244037\n",
      "loss.item of the train =  82 0.3522289991378784\n",
      "loss.item of the valid =  82 0.35189563035964966\n",
      "loss.item of the valid =  82 0.38939008116722107\n",
      "loss.item of the valid =  82 0.35165831446647644\n",
      "loss.item of the valid =  82 0.2971841096878052\n",
      "loss.item of the valid =  82 0.32249167561531067\n",
      "loss.item of the valid =  82 0.4716228246688843\n",
      "Iteration 83 \t\t Training Loss: 0.3522289991378784 \t\t Validation Loss: 0.36404043932755786\n",
      "loss.item of the train =  83 0.35217487812042236\n",
      "loss.item of the valid =  83 0.35206544399261475\n",
      "loss.item of the valid =  83 0.3896031081676483\n",
      "loss.item of the valid =  83 0.3518378436565399\n",
      "loss.item of the valid =  83 0.2972147464752197\n",
      "loss.item of the valid =  83 0.3226996660232544\n",
      "loss.item of the valid =  83 0.4719713032245636\n",
      "Iteration 84 \t\t Training Loss: 0.35217487812042236 \t\t Validation Loss: 0.36423201858997345\n",
      "loss.item of the train =  84 0.35211366415023804\n",
      "loss.item of the valid =  84 0.35225605964660645\n",
      "loss.item of the valid =  84 0.38986653089523315\n",
      "loss.item of the valid =  84 0.35206538438796997\n",
      "loss.item of the valid =  84 0.2972484827041626\n",
      "loss.item of the valid =  84 0.3229328393936157\n",
      "loss.item of the valid =  84 0.47238585352897644\n",
      "Iteration 85 \t\t Training Loss: 0.35211366415023804 \t\t Validation Loss: 0.36445919175942737\n",
      "loss.item of the train =  85 0.3520403206348419\n",
      "loss.item of the valid =  85 0.3524766266345978\n",
      "loss.item of the valid =  85 0.39021334052085876\n",
      "loss.item of the valid =  85 0.3523314595222473\n",
      "loss.item of the valid =  85 0.2972932755947113\n",
      "loss.item of the valid =  85 0.32319778203964233\n",
      "loss.item of the valid =  85 0.47287774085998535\n",
      "Iteration 86 \t\t Training Loss: 0.3520403206348419 \t\t Validation Loss: 0.36473170419534046\n",
      "loss.item of the train =  86 0.3519531786441803\n",
      "loss.item of the valid =  86 0.3527315557003021\n",
      "loss.item of the valid =  86 0.39073696732521057\n",
      "loss.item of the valid =  86 0.35264432430267334\n",
      "loss.item of the valid =  86 0.29734712839126587\n",
      "loss.item of the valid =  86 0.3234948217868805\n",
      "loss.item of the valid =  86 0.47345319390296936\n",
      "Iteration 87 \t\t Training Loss: 0.3519531786441803 \t\t Validation Loss: 0.365067998568217\n",
      "loss.item of the train =  87 0.35185790061950684\n",
      "loss.item of the valid =  87 0.3530219793319702\n",
      "loss.item of the valid =  87 0.39132991433143616\n",
      "loss.item of the valid =  87 0.3529849052429199\n",
      "loss.item of the valid =  87 0.29740628600120544\n",
      "loss.item of the valid =  87 0.3238394856452942\n",
      "loss.item of the valid =  87 0.474104106426239\n",
      "Iteration 88 \t\t Training Loss: 0.35185790061950684 \t\t Validation Loss: 0.3654477794965108\n",
      "loss.item of the train =  88 0.35174560546875\n",
      "loss.item of the valid =  88 0.35335487127304077\n",
      "loss.item of the valid =  88 0.392000675201416\n",
      "loss.item of the valid =  88 0.3533608019351959\n",
      "loss.item of the valid =  88 0.2974725663661957\n",
      "loss.item of the valid =  88 0.32423701882362366\n",
      "loss.item of the valid =  88 0.4748392403125763\n",
      "Iteration 89 \t\t Training Loss: 0.35174560546875 \t\t Validation Loss: 0.36587752898534137\n",
      "loss.item of the train =  89 0.3516194820404053\n",
      "loss.item of the valid =  89 0.353730171918869\n",
      "loss.item of the valid =  89 0.39276379346847534\n",
      "loss.item of the valid =  89 0.3537765443325043\n",
      "loss.item of the valid =  89 0.2975408732891083\n",
      "loss.item of the valid =  89 0.3246854245662689\n",
      "loss.item of the valid =  89 0.4756792187690735\n",
      "Iteration 90 \t\t Training Loss: 0.3516194820404053 \t\t Validation Loss: 0.36636267105738324\n",
      "loss.item of the train =  90 0.35147660970687866\n",
      "loss.item of the valid =  90 0.3541613817214966\n",
      "loss.item of the valid =  90 0.39360904693603516\n",
      "loss.item of the valid =  90 0.35423916578292847\n",
      "loss.item of the valid =  90 0.29761385917663574\n",
      "loss.item of the valid =  90 0.3251912295818329\n",
      "loss.item of the valid =  90 0.476607084274292\n",
      "Iteration 91 \t\t Training Loss: 0.35147660970687866 \t\t Validation Loss: 0.3669036279122035\n",
      "loss.item of the train =  91 0.3513150215148926\n",
      "loss.item of the valid =  91 0.35463184118270874\n",
      "loss.item of the valid =  91 0.39455172419548035\n",
      "loss.item of the valid =  91 0.35476306080818176\n",
      "loss.item of the valid =  91 0.29769885540008545\n",
      "loss.item of the valid =  91 0.32575589418411255\n",
      "loss.item of the valid =  91 0.4776453375816345\n",
      "Iteration 92 \t\t Training Loss: 0.3513150215148926 \t\t Validation Loss: 0.36750778555870056\n",
      "loss.item of the train =  92 0.35306796431541443\n",
      "loss.item of the valid =  92 0.35380107164382935\n",
      "loss.item of the valid =  92 0.39287301898002625\n",
      "loss.item of the valid =  92 0.3538290560245514\n",
      "loss.item of the valid =  92 0.2975568175315857\n",
      "loss.item of the valid =  92 0.3247723877429962\n",
      "loss.item of the valid =  92 0.47580215334892273\n",
      "Iteration 93 \t\t Training Loss: 0.35306796431541443 \t\t Validation Loss: 0.3664390842119853\n",
      "loss.item of the train =  93 0.3514566719532013\n",
      "loss.item of the valid =  93 0.35315120220184326\n",
      "loss.item of the valid =  93 0.39158034324645996\n",
      "loss.item of the valid =  93 0.3531101644039154\n",
      "loss.item of the valid =  93 0.2974308133125305\n",
      "loss.item of the valid =  93 0.3239997625350952\n",
      "loss.item of the valid =  93 0.4743846654891968\n",
      "Iteration 94 \t\t Training Loss: 0.3514566719532013 \t\t Validation Loss: 0.3656094918648402\n",
      "loss.item of the train =  94 0.3516957461833954\n",
      "loss.item of the valid =  94 0.35264497995376587\n",
      "loss.item of the valid =  94 0.3905840218067169\n",
      "loss.item of the valid =  94 0.35256221890449524\n",
      "loss.item of the valid =  94 0.29732584953308105\n",
      "loss.item of the valid =  94 0.3233998715877533\n",
      "loss.item of the valid =  94 0.47330212593078613\n",
      "Iteration 95 \t\t Training Loss: 0.3516957461833954 \t\t Validation Loss: 0.3649698446194331\n",
      "loss.item of the train =  95 0.3518793284893036\n",
      "loss.item of the valid =  95 0.35226157307624817\n",
      "loss.item of the valid =  95 0.3899496793746948\n",
      "loss.item of the valid =  95 0.35214194655418396\n",
      "loss.item of the valid =  95 0.29724687337875366\n",
      "loss.item of the valid =  95 0.3229575753211975\n",
      "loss.item of the valid =  95 0.4724718928337097\n",
      "Iteration 96 \t\t Training Loss: 0.3518793284893036 \t\t Validation Loss: 0.3645049234231313\n",
      "loss.item of the train =  96 0.35202229022979736\n",
      "loss.item of the valid =  96 0.3519783616065979\n",
      "loss.item of the valid =  96 0.3895634412765503\n",
      "loss.item of the valid =  96 0.3518172800540924\n",
      "loss.item of the valid =  96 0.29719358682632446\n",
      "loss.item of the valid =  96 0.3226280212402344\n",
      "loss.item of the valid =  96 0.471846342086792\n",
      "Iteration 97 \t\t Training Loss: 0.35202229022979736 \t\t Validation Loss: 0.36417117218176526\n",
      "loss.item of the train =  97 0.35213008522987366\n",
      "loss.item of the valid =  97 0.35177081823349\n",
      "loss.item of the valid =  97 0.3892683982849121\n",
      "loss.item of the valid =  97 0.3515748381614685\n",
      "loss.item of the valid =  97 0.29715728759765625\n",
      "loss.item of the valid =  97 0.3223831355571747\n",
      "loss.item of the valid =  97 0.47136884927749634\n",
      "Iteration 98 \t\t Training Loss: 0.35213008522987366 \t\t Validation Loss: 0.36392055451869965\n",
      "loss.item of the train =  98 0.3522108495235443\n",
      "loss.item of the valid =  98 0.35161373019218445\n",
      "loss.item of the valid =  98 0.38905251026153564\n",
      "loss.item of the valid =  98 0.35138803720474243\n",
      "loss.item of the valid =  98 0.29712435603141785\n",
      "loss.item of the valid =  98 0.3221901059150696\n",
      "loss.item of the valid =  98 0.47101029753685\n",
      "Iteration 99 \t\t Training Loss: 0.3522108495235443 \t\t Validation Loss: 0.3637298395236333\n",
      "loss.item of the train =  99 0.35226815938949585\n",
      "ged= tensor([ 0.2109,  0.2413,  0.5212,  0.0186,  0.2741,  0.0131,  0.0131,  0.2355,\n",
      "         0.3389,  0.2109,  0.2782, -0.6720, -0.6825, -0.5272, -0.4616, -0.4009,\n",
      "         0.2767,  0.6848,  0.3177,  0.4134,  0.3249,  0.3147,  0.2746,  0.3610,\n",
      "         0.0000,  0.4072, -0.9674, -0.9773, -0.6848, -0.6014, -0.2857,  0.6190,\n",
      "         0.2988,  0.3514,  0.2893,  0.2930,  0.0106,  0.4148,  0.2252,  0.3521,\n",
      "        -0.7602, -0.7678, -0.6105, -0.5620, -0.3775,  0.2590,  0.2307,  0.2675,\n",
      "         0.2492,  0.3108,  0.2355,  0.3712,  0.2138, -0.3121, -0.3224, -0.0198,\n",
      "        -0.2337, -1.0000,  0.2743,  0.0131,  0.0131,  0.2300,  0.3340,  0.2064,\n",
      "         0.2785, -0.6643, -0.6749, -0.5161, -0.4616, -0.3989,  0.2218,  0.2218,\n",
      "         0.2235,  0.2788,  0.2192,  0.0090, -0.5496, -0.5592, -0.4212, -0.3909,\n",
      "        -0.5275,  0.0064,  0.2284,  0.3274,  0.2033,  0.2716, -0.6919, -0.7023,\n",
      "        -0.5238, -0.4464, -0.3831,  0.2249,  0.3295,  0.2027,  0.2667, -0.6609,\n",
      "        -0.6721, -0.5196, -0.4457, -0.3950,  0.4038,  0.2216,  0.3411, -0.7673,\n",
      "        -0.7781, -0.6022, -0.5676, -0.3701,  0.2216,  0.2317, -0.3035, -0.3116,\n",
      "        -0.3879, -0.3363, -0.6100,  0.4019, -0.9423, -0.9523, -0.6857, -0.5857,\n",
      "        -0.2783, -0.5433, -0.5529, -0.4168, -0.3929, -0.5276],\n",
      "       grad_fn=<MulBackward0>)\n",
      "Distances:  tensor([0.2109, 0.2413, 0.5212, 0.0186, 0.2741, 0.0131, 0.0131, 0.2355, 0.3389,\n",
      "        0.2109, 0.2782, 0.6720, 0.6825, 0.5272, 0.4616, 0.4009, 0.2767, 0.6848,\n",
      "        0.3177, 0.4134, 0.3249, 0.3147, 0.2746, 0.3610, 0.0000, 0.4072, 0.9674,\n",
      "        0.9773, 0.6848, 0.6014, 0.2857, 0.6190, 0.2988, 0.3514, 0.2893, 0.2930,\n",
      "        0.0106, 0.4148, 0.2252, 0.3521, 0.7602, 0.7678, 0.6105, 0.5620, 0.3775,\n",
      "        0.2590, 0.2307, 0.2675, 0.2492, 0.3108, 0.2355, 0.3712, 0.2138, 0.3121,\n",
      "        0.3224, 0.0198, 0.2337, 1.0000, 0.2743, 0.0131, 0.0131, 0.2300, 0.3340,\n",
      "        0.2064, 0.2785, 0.6643, 0.6749, 0.5161, 0.4616, 0.3989, 0.2218, 0.2218,\n",
      "        0.2235, 0.2788, 0.2192, 0.0090, 0.5496, 0.5592, 0.4212, 0.3909, 0.5275,\n",
      "        0.0064, 0.2284, 0.3274, 0.2033, 0.2716, 0.6919, 0.7023, 0.5238, 0.4464,\n",
      "        0.3831, 0.2249, 0.3295, 0.2027, 0.2667, 0.6609, 0.6721, 0.5196, 0.4457,\n",
      "        0.3950, 0.4038, 0.2216, 0.3411, 0.7673, 0.7781, 0.6022, 0.5676, 0.3701,\n",
      "        0.2216, 0.2317, 0.3035, 0.3116, 0.3879, 0.3363, 0.6100, 0.4019, 0.9423,\n",
      "        0.9523, 0.6857, 0.5857, 0.2783, 0.5433, 0.5529, 0.4168, 0.3929, 0.5276],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Loss Triangular: 6.275838131841738e-06\n",
      "node_costs :\n",
      "tensor([[0.0000e+00, 6.7400e-06, 9.2896e-05, 2.4116e-04, 6.4270e-05, 9.0184e-05,\n",
      "         2.5647e-04, 1.5915e-05, 3.4901e-05, 2.7437e-05, 7.8573e-05, 7.9099e-06,\n",
      "         5.3278e-05, 9.1454e-05, 3.8482e-05, 1.2039e-04, 2.4381e-04, 2.0774e-04,\n",
      "         4.1298e-06],\n",
      "        [6.7400e-06, 0.0000e+00, 1.4964e-04, 9.1905e-05, 1.2567e-04, 2.6791e-04,\n",
      "         1.8364e-04, 9.8007e-05, 5.1903e-05, 5.2644e-05, 1.7868e-04, 1.2865e-04,\n",
      "         1.4298e-04, 2.8096e-04, 2.0292e-06, 8.0655e-05, 1.2504e-06, 5.1577e-05,\n",
      "         4.1213e-05],\n",
      "        [9.2896e-05, 1.4964e-04, 0.0000e+00, 2.2727e-05, 1.5909e-05, 1.9979e-06,\n",
      "         1.8943e-05, 2.0325e-06, 6.3661e-05, 1.6751e-04, 1.1984e-08, 3.2445e-05,\n",
      "         2.2929e-04, 2.7514e-05, 1.0867e-04, 2.3568e-04, 1.9061e-04, 3.5784e-06,\n",
      "         1.6659e-04],\n",
      "        [2.4116e-04, 9.1905e-05, 2.2727e-05, 0.0000e+00, 9.0886e-05, 5.4037e-05,\n",
      "         3.6322e-05, 3.7212e-05, 9.0025e-05, 2.4406e-04, 8.4243e-05, 1.5973e-05,\n",
      "         2.1779e-04, 6.6353e-05, 1.2290e-04, 1.8048e-04, 3.5774e-05, 1.1305e-04,\n",
      "         5.5466e-06],\n",
      "        [6.4270e-05, 1.2567e-04, 1.5909e-05, 9.0886e-05, 0.0000e+00, 1.7812e-04,\n",
      "         2.9620e-05, 6.4366e-05, 2.2082e-04, 1.8047e-04, 1.2958e-04, 8.3392e-05,\n",
      "         4.0465e-04, 1.5154e-04, 2.3521e-04, 1.7808e-04, 1.2628e-04, 1.5754e-04,\n",
      "         1.8947e-04],\n",
      "        [9.0184e-05, 2.6791e-04, 1.9979e-06, 5.4037e-05, 1.7812e-04, 0.0000e+00,\n",
      "         6.4031e-05, 4.3250e-04, 2.4820e-04, 3.3187e-04, 6.4060e-05, 2.7736e-05,\n",
      "         7.8874e-05, 1.0564e-04, 5.1639e-05, 4.2429e-04, 1.8026e-04, 1.9020e-04,\n",
      "         4.2348e-04],\n",
      "        [2.5647e-04, 1.8364e-04, 1.8943e-05, 3.6322e-05, 2.9620e-05, 6.4031e-05,\n",
      "         0.0000e+00, 1.1332e-04, 2.4311e-05, 1.8266e-04, 2.1643e-04, 1.0548e-04,\n",
      "         2.5173e-04, 3.0316e-07, 3.7098e-06, 2.6734e-04, 6.9932e-06, 5.9542e-05,\n",
      "         1.7861e-04],\n",
      "        [1.5915e-05, 9.8007e-05, 2.0325e-06, 3.7212e-05, 6.4366e-05, 4.3250e-04,\n",
      "         1.1332e-04, 0.0000e+00, 2.2925e-05, 1.4613e-04, 3.6215e-06, 6.4575e-05,\n",
      "         2.0949e-04, 1.6682e-04, 2.8426e-05, 1.7880e-04, 8.2915e-05, 2.2476e-04,\n",
      "         1.1294e-04],\n",
      "        [3.4901e-05, 5.1903e-05, 6.3661e-05, 9.0025e-05, 2.2082e-04, 2.4820e-04,\n",
      "         2.4311e-05, 2.2925e-05, 0.0000e+00, 2.5518e-04, 4.9018e-05, 4.3062e-04,\n",
      "         4.1125e-06, 5.2182e-05, 3.7234e-04, 4.8570e-05, 2.0343e-04, 3.8418e-06,\n",
      "         1.9002e-04],\n",
      "        [2.7437e-05, 5.2644e-05, 1.6751e-04, 2.4406e-04, 1.8047e-04, 3.3187e-04,\n",
      "         1.8266e-04, 1.4613e-04, 2.5518e-04, 0.0000e+00, 7.3954e-05, 1.8182e-04,\n",
      "         1.7631e-04, 8.8094e-05, 6.3348e-05, 1.6828e-04, 2.1803e-04, 3.0802e-05,\n",
      "         1.7080e-04],\n",
      "        [7.8573e-05, 1.7868e-04, 1.1984e-08, 8.4243e-05, 1.2958e-04, 6.4060e-05,\n",
      "         2.1643e-04, 3.6215e-06, 4.9018e-05, 7.3954e-05, 0.0000e+00, 2.1898e-04,\n",
      "         1.5305e-04, 7.8825e-05, 1.3426e-06, 7.0049e-06, 2.4387e-04, 2.3050e-04,\n",
      "         1.1177e-04],\n",
      "        [7.9099e-06, 1.2865e-04, 3.2445e-05, 1.5973e-05, 8.3392e-05, 2.7736e-05,\n",
      "         1.0548e-04, 6.4575e-05, 4.3062e-04, 1.8182e-04, 2.1898e-04, 0.0000e+00,\n",
      "         6.3838e-05, 7.9875e-05, 2.0230e-05, 5.0088e-05, 3.6847e-05, 3.4929e-06,\n",
      "         1.9110e-04],\n",
      "        [5.3278e-05, 1.4298e-04, 2.2929e-04, 2.1779e-04, 4.0465e-04, 7.8874e-05,\n",
      "         2.5173e-04, 2.0949e-04, 4.1125e-06, 1.7631e-04, 1.5305e-04, 6.3838e-05,\n",
      "         0.0000e+00, 2.3608e-04, 7.5086e-05, 6.4564e-05, 7.0053e-06, 4.1214e-06,\n",
      "         7.9438e-06],\n",
      "        [9.1454e-05, 2.8096e-04, 2.7514e-05, 6.6353e-05, 1.5154e-04, 1.0564e-04,\n",
      "         3.0316e-07, 1.6682e-04, 5.2182e-05, 8.8094e-05, 7.8825e-05, 7.9875e-05,\n",
      "         2.3608e-04, 0.0000e+00, 1.8133e-04, 1.9006e-04, 6.2603e-07, 1.7346e-05,\n",
      "         1.7304e-04],\n",
      "        [3.8482e-05, 2.0292e-06, 1.0867e-04, 1.2290e-04, 2.3521e-04, 5.1639e-05,\n",
      "         3.7098e-06, 2.8426e-05, 3.7234e-04, 6.3348e-05, 1.3426e-06, 2.0230e-05,\n",
      "         7.5086e-05, 1.8133e-04, 0.0000e+00, 1.3939e-07, 5.2853e-05, 1.7897e-04,\n",
      "         2.8770e-04],\n",
      "        [1.2039e-04, 8.0655e-05, 2.3568e-04, 1.8048e-04, 1.7808e-04, 4.2429e-04,\n",
      "         2.6734e-04, 1.7880e-04, 4.8570e-05, 1.6828e-04, 7.0049e-06, 5.0088e-05,\n",
      "         6.4564e-05, 1.9006e-04, 1.3939e-07, 0.0000e+00, 2.7093e-04, 7.8753e-05,\n",
      "         4.1227e-05],\n",
      "        [2.4381e-04, 1.2504e-06, 1.9061e-04, 3.5774e-05, 1.2628e-04, 1.8026e-04,\n",
      "         6.9932e-06, 8.2915e-05, 2.0343e-04, 2.1803e-04, 2.4387e-04, 3.6847e-05,\n",
      "         7.0053e-06, 6.2603e-07, 5.2853e-05, 2.7093e-04, 0.0000e+00, 1.3120e-05,\n",
      "         2.3279e-04],\n",
      "        [2.0774e-04, 5.1577e-05, 3.5784e-06, 1.1305e-04, 1.5754e-04, 1.9020e-04,\n",
      "         5.9542e-05, 2.2476e-04, 3.8418e-06, 3.0802e-05, 2.3050e-04, 3.4929e-06,\n",
      "         4.1214e-06, 1.7346e-05, 1.7897e-04, 7.8753e-05, 1.3120e-05, 0.0000e+00,\n",
      "         6.4491e-05],\n",
      "        [4.1298e-06, 4.1213e-05, 1.6659e-04, 5.5466e-06, 1.8947e-04, 4.2348e-04,\n",
      "         1.7861e-04, 1.1294e-04, 1.9002e-04, 1.7080e-04, 1.1177e-04, 1.9110e-04,\n",
      "         7.9438e-06, 1.7304e-04, 2.8770e-04, 4.1227e-05, 2.3279e-04, 6.4491e-05,\n",
      "         0.0000e+00]], grad_fn=<AddBackward0>)\n",
      "nodeInsDel: 0.00013005947403144091\n",
      "edge_costs :\n",
      "tensor([[0.0000e+00, 5.8193e-01, 5.7148e-07],\n",
      "        [5.8193e-01, 0.0000e+00, 1.9471e-05],\n",
      "        [5.7148e-07, 1.9471e-05, 0.0000e+00]], grad_fn=<AddBackward0>)\n",
      "edgeInsDel: 0.3975837826728821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item of the valid =  99 0.3515052795410156\n",
      "loss.item of the valid =  99 0.3889026641845703\n",
      "loss.item of the valid =  99 0.35125741362571716\n",
      "loss.item of the valid =  99 0.2971002161502838\n",
      "loss.item of the valid =  99 0.32206645607948303\n",
      "loss.item of the valid =  99 0.4707724153995514\n",
      "Iteration 100 \t\t Training Loss: 0.35226815938949585 \t\t Validation Loss: 0.3636007408301036\n",
      "iter and min_valid_loss =  0 2.1007999181747437\n",
      " Min cost for nodeInsDel =  tensor(0.0055, grad_fn=<SelectBackward>)\n",
      " Min cost for edgeInsDel =  tensor(0.0053, grad_fn=<SelectBackward>)\n",
      " Min cost for nodeSub =  tensor([[0.0000, 0.0062, 0.0056, 0.0058, 0.0066, 0.0061, 0.0059, 0.0061, 0.0050,\n",
      "         0.0052, 0.0057, 0.0049, 0.0056, 0.0056, 0.0060, 0.0050, 0.0052, 0.0050,\n",
      "         0.0062],\n",
      "        [0.0062, 0.0000, 0.0055, 0.0063, 0.0061, 0.0049, 0.0060, 0.0053, 0.0065,\n",
      "         0.0065, 0.0064, 0.0052, 0.0054, 0.0050, 0.0048, 0.0057, 0.0053, 0.0065,\n",
      "         0.0065],\n",
      "        [0.0056, 0.0055, 0.0000, 0.0056, 0.0061, 0.0048, 0.0054, 0.0048, 0.0066,\n",
      "         0.0059, 0.0054, 0.0053, 0.0058, 0.0052, 0.0052, 0.0058, 0.0064, 0.0064,\n",
      "         0.0059],\n",
      "        [0.0058, 0.0063, 0.0056, 0.0000, 0.0056, 0.0060, 0.0059, 0.0056, 0.0061,\n",
      "         0.0052, 0.0060, 0.0061, 0.0051, 0.0057, 0.0060, 0.0049, 0.0060, 0.0052,\n",
      "         0.0063],\n",
      "        [0.0066, 0.0061, 0.0061, 0.0056, 0.0000, 0.0064, 0.0050, 0.0066, 0.0051,\n",
      "         0.0064, 0.0054, 0.0056, 0.0049, 0.0054, 0.0056, 0.0064, 0.0053, 0.0055,\n",
      "         0.0048],\n",
      "        [0.0061, 0.0049, 0.0048, 0.0060, 0.0064, 0.0000, 0.0058, 0.0051, 0.0052,\n",
      "         0.0049, 0.0066, 0.0056, 0.0057, 0.0051, 0.0055, 0.0051, 0.0064, 0.0064,\n",
      "         0.0051],\n",
      "        [0.0059, 0.0060, 0.0054, 0.0059, 0.0050, 0.0058, 0.0000, 0.0063, 0.0056,\n",
      "         0.0059, 0.0051, 0.0053, 0.0059, 0.0061, 0.0054, 0.0055, 0.0062, 0.0055,\n",
      "         0.0059],\n",
      "        [0.0061, 0.0053, 0.0048, 0.0056, 0.0066, 0.0051, 0.0063, 0.0000, 0.0053,\n",
      "         0.0055, 0.0052, 0.0066, 0.0051, 0.0063, 0.0050, 0.0064, 0.0056, 0.0058,\n",
      "         0.0051],\n",
      "        [0.0050, 0.0065, 0.0066, 0.0061, 0.0051, 0.0052, 0.0056, 0.0053, 0.0000,\n",
      "         0.0059, 0.0065, 0.0051, 0.0062, 0.0065, 0.0048, 0.0066, 0.0054, 0.0064,\n",
      "         0.0064],\n",
      "        [0.0052, 0.0065, 0.0059, 0.0052, 0.0064, 0.0049, 0.0059, 0.0055, 0.0059,\n",
      "         0.0000, 0.0053, 0.0060, 0.0052, 0.0052, 0.0066, 0.0059, 0.0050, 0.0055,\n",
      "         0.0059],\n",
      "        [0.0057, 0.0064, 0.0054, 0.0060, 0.0054, 0.0066, 0.0051, 0.0052, 0.0065,\n",
      "         0.0053, 0.0000, 0.0051, 0.0050, 0.0057, 0.0063, 0.0062, 0.0049, 0.0058,\n",
      "         0.0063],\n",
      "        [0.0049, 0.0052, 0.0053, 0.0061, 0.0056, 0.0056, 0.0053, 0.0066, 0.0051,\n",
      "         0.0060, 0.0051, 0.0000, 0.0066, 0.0057, 0.0055, 0.0065, 0.0060, 0.0064,\n",
      "         0.0060],\n",
      "        [0.0056, 0.0054, 0.0058, 0.0051, 0.0049, 0.0057, 0.0059, 0.0051, 0.0062,\n",
      "         0.0052, 0.0050, 0.0066, 0.0000, 0.0051, 0.0056, 0.0058, 0.0062, 0.0062,\n",
      "         0.0048],\n",
      "        [0.0056, 0.0050, 0.0052, 0.0057, 0.0054, 0.0051, 0.0061, 0.0063, 0.0065,\n",
      "         0.0052, 0.0057, 0.0057, 0.0051, 0.0000, 0.0064, 0.0060, 0.0053, 0.0055,\n",
      "         0.0050],\n",
      "        [0.0060, 0.0048, 0.0052, 0.0060, 0.0056, 0.0055, 0.0054, 0.0050, 0.0048,\n",
      "         0.0066, 0.0063, 0.0055, 0.0056, 0.0064, 0.0000, 0.0050, 0.0065, 0.0059,\n",
      "         0.0049],\n",
      "        [0.0050, 0.0057, 0.0058, 0.0049, 0.0064, 0.0051, 0.0055, 0.0064, 0.0066,\n",
      "         0.0059, 0.0062, 0.0065, 0.0058, 0.0060, 0.0050, 0.0000, 0.0050, 0.0057,\n",
      "         0.0060],\n",
      "        [0.0052, 0.0053, 0.0064, 0.0060, 0.0053, 0.0064, 0.0062, 0.0056, 0.0054,\n",
      "         0.0050, 0.0049, 0.0060, 0.0062, 0.0053, 0.0065, 0.0050, 0.0000, 0.0048,\n",
      "         0.0052],\n",
      "        [0.0050, 0.0065, 0.0064, 0.0052, 0.0055, 0.0064, 0.0055, 0.0058, 0.0064,\n",
      "         0.0055, 0.0058, 0.0064, 0.0062, 0.0055, 0.0059, 0.0057, 0.0048, 0.0000,\n",
      "         0.0058],\n",
      "        [0.0062, 0.0065, 0.0059, 0.0063, 0.0048, 0.0051, 0.0059, 0.0051, 0.0064,\n",
      "         0.0059, 0.0063, 0.0060, 0.0048, 0.0050, 0.0049, 0.0060, 0.0052, 0.0058,\n",
      "         0.0000]], grad_fn=<AddBackward0>)\n",
      " Min cost for edgeSub =  tensor([[0.0000, 0.0057, 0.0057],\n",
      "        [0.0057, 0.0000, 0.0052],\n",
      "        [0.0057, 0.0052, 0.0000]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nb_iter=100\n",
    "InsDel, nodeSub,edgeSub,loss_plt,loss_valid_plt,loss_train_plt=classification(model,data,yt,nb_iter)\n",
    "plt.figure(0)\n",
    "plt.plot(InsDel[0:nb_iter,0],label=\"node\")\n",
    "plt.plot(InsDel[0:nb_iter,1],label=\"edge\")\n",
    "plt.title('Node/Edge insertion/deletion costs')\n",
    "plt.legend()\n",
    "plt.figure(1)\n",
    "for k in range(nodeSub.shape[1]):\n",
    "    plt.plot(nodeSub[0:nb_iter,k])\n",
    "plt.title('Node Substitutions costs')\n",
    "plt.figure(2)\n",
    "for k in range(edgeSub.shape[1]):\n",
    "    plt.plot(edgeSub[0:nb_iter,k])\n",
    "plt.title('Edge Substitutions costs')\n",
    "plt.figure(3)\n",
    "plt.plot(loss_plt)\n",
    "plt.title('Evolution of the train loss (loss_plt)')\n",
    "\n",
    "plt.figure(4)\n",
    "plt.plot(loss_valid_plt)\n",
    "plt.title('Evolution of the valid loss')\n",
    "'''\n",
    "plt.figure(5)\n",
    "plt.plot(loss_train_plt)\n",
    "plt.title('Evolution of the loss_train_plt')\n",
    "'''\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=torch.tensor(nx.to_scipy_sparse_matrix(Gs[0],dtype=int,weight='bond_type').todense(),dtype=torch.int) \n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plt, label='train loss')\n",
    "plt.plot(loss_valid_plt, label='valid loss')\n",
    "plt.title('Train and valid losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card = torch.tensor([G.order() for G in Gs]).to(device)\n",
    "print(Gs[0].order())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.plot(InsDel[0:500,0],label=\"node\")\n",
    "plt.plot(InsDel[0:500,1],label=\"edge\")\n",
    "plt.title('Node/Edge insertion/deletion costs')\n",
    "plt.legend()\n",
    "plt.figure(1)\n",
    "for k in range(nodeSub.shape[1]):\n",
    "    plt.plot(nodeSub[0:500,k])\n",
    "plt.title('node Substitution costs')\n",
    "plt.figure(2)\n",
    "for k in range(edgeSub.shape[1]):\n",
    "    plt.plot(edgeSub[0:500,k])\n",
    "plt.title('edge Substitution costs')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
