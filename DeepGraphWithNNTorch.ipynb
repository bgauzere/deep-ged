{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gklearn.utils.graphfiles import loadDataset\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge max label 3\n",
      "0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "def label_to_color(label):\n",
    "    if label == 'C':\n",
    "        return 0.1\n",
    "    elif label == 'O':\n",
    "        return 0.8\n",
    "    \n",
    "def nodes_to_color_sequence(G):\n",
    "    return [label_to_color(c[1]['label'][0]) for c in G.nodes(data=True)]\n",
    "\n",
    "Gs,y = loadDataset('/home/ines/Documents/M2/Stage/stage_ged/Ines/DeepGED/MAO/dataset.ds')\n",
    "#for e in Gs[13].edges():\n",
    "#    print(Gs[13][e[0]][e[1]]['bond_type'])\n",
    "print('edge max label',max(max([[G[e[0]][e[1]]['bond_type'] for e in G.edges()] for G in Gs])))\n",
    "G1 = Gs[1]\n",
    "G2 = Gs[9]\n",
    "print(y[1],y[9])\n",
    "\n",
    "'''\n",
    "plt.figure(0)\n",
    "nx.draw_networkx(G1,with_labels=True,node_color = nodes_to_color_sequence(G1),cmap='autumn')\n",
    "\n",
    "plt.figure(1)\n",
    "nx.draw_networkx(G2,with_labels=True,node_color = nodes_to_color_sequence(G2),cmap='autumn')\n",
    "\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "import extended_label\n",
    "for g in Gs:\n",
    "    extended_label.compute_extended_labels(g)\n",
    "#for v in Gs[10].nodes():\n",
    "#        print(Gs[10].nodes[v])\n",
    "\n",
    "#print(nx.to_dict_of_lists(Gs[13]))\n",
    "\n",
    "\n",
    "\n",
    "#dict={'C':0,'N':1,'O':2}\n",
    "#A,labels=from_networkx_to_tensor2(Gs[13],dict)\n",
    "#print(A)\n",
    "#A1=(A==torch.ones(13,13)).int()\n",
    "#A2=(A==2*torch.ones(13,13)).int()\n",
    "#print(A1)\n",
    "#print(A2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gs= 68\n",
      "['C_1C', 'C_1C1C1N', 'C_1C1C2C', 'C_1C1N', 'C_1C1N2C', 'C_1C1O', 'C_1C1O2C', 'C_1C2C', 'C_1C3C', 'C_1N', 'C_1O', 'C_2C', 'C_2C2C', 'C_3C', 'N_1C', 'N_1C1C', 'N_1C1C1C', 'O_1C', 'O_1C1C']\n",
      "{'C_1C': 0, 'C_1C1C1N': 1, 'C_1C1C2C': 2, 'C_1C1N': 3, 'C_1C1N2C': 4, 'C_1C1O': 5, 'C_1C1O2C': 6, 'C_1C2C': 7, 'C_1C3C': 8, 'C_1N': 9, 'C_1O': 10, 'C_2C': 11, 'C_2C2C': 12, 'C_3C': 13, 'N_1C': 14, 'N_1C1C': 15, 'N_1C1C1C': 16, 'O_1C': 17, 'O_1C1C': 18} 19\n",
      "3\n",
      "torch.Size([68, 729])\n",
      "adjacency matrices tensor([[         0,          1,          0,  ..., 1600481124, 1953722211,\n",
      "         1869491315],\n",
      "        [         0,          1,          0,  ...,  694969668,  538970682,\n",
      "          538976288],\n",
      "        [         0,          1,          0,  ...,  540876903, 1735289202,\n",
      "         1730632563],\n",
      "        ...,\n",
      "        [         0,          1,          0,  ...,  778464357, 1685217635,\n",
      "          538970665],\n",
      "        [         0,          1,          0,  ..., 1701585011, 1634476147,\n",
      "         1936483682],\n",
      "        [         0,          1,          0,  ...,  538970665,  538976288,\n",
      "          826482720]], dtype=torch.int32)\n",
      "node labels tensor([[        15,          4,          7,  ...,   93915040,          0,\n",
      "                  2],\n",
      "        [        15,          4,          7,  ...,         12,         28,\n",
      "         1784769887],\n",
      "        [        15,          4,          7,  ..., 1768843615,  677338996,\n",
      "                 53],\n",
      "        ...,\n",
      "        [        16,          4,          7,  ..., 1700749934, 1666698576,\n",
      "              32647],\n",
      "        [        16,          4,          7,  ...,         70,         23,\n",
      "                  2],\n",
      "        [        16,          4,          7,  ..., 1031299437,   93922024,\n",
      "                  0]], dtype=torch.int32)\n",
      "order of the graphs tensor([11, 12, 14, 14, 15, 17, 15, 16, 19, 15, 16, 19, 12, 13, 15, 18, 16, 16,\n",
      "        17, 16, 17, 17, 18, 19, 19, 18, 18, 22, 22, 15, 14, 13, 16, 17, 17, 21,\n",
      "        17, 18, 18, 21, 24, 25, 25, 14, 17, 18, 18, 22, 23, 23, 25, 27, 27, 15,\n",
      "        16, 23, 24, 24, 16, 17, 17, 20, 23, 24, 26, 16, 17, 21])\n",
      "2\n",
      "Parameter containing:\n",
      "tensor([0.0064, 0.0062, 0.0065, 0.0062, 0.0062, 0.0065, 0.0061, 0.0063, 0.0060,\n",
      "        0.0063, 0.0061, 0.0064, 0.0060, 0.0062, 0.0066, 0.0066, 0.0058, 0.0057,\n",
      "        0.0058, 0.0061, 0.0059, 0.0061, 0.0057, 0.0066, 0.0058, 0.0059, 0.0065,\n",
      "        0.0063, 0.0057, 0.0059, 0.0065, 0.0058, 0.0067, 0.0061, 0.0060, 0.0058,\n",
      "        0.0066, 0.0060, 0.0061, 0.0060, 0.0058, 0.0065, 0.0060, 0.0065, 0.0064,\n",
      "        0.0064, 0.0063, 0.0058, 0.0060, 0.0064, 0.0057, 0.0060, 0.0063, 0.0063,\n",
      "        0.0060, 0.0062, 0.0066, 0.0057, 0.0059, 0.0062, 0.0066, 0.0064, 0.0057,\n",
      "        0.0058, 0.0058, 0.0058, 0.0059, 0.0062, 0.0064, 0.0064, 0.0060, 0.0060,\n",
      "        0.0061, 0.0066, 0.0060, 0.0057, 0.0062, 0.0064, 0.0062, 0.0061, 0.0065,\n",
      "        0.0061, 0.0058, 0.0064, 0.0058, 0.0058, 0.0061, 0.0061, 0.0065, 0.0059,\n",
      "        0.0059, 0.0063, 0.0064, 0.0064, 0.0061, 0.0063, 0.0066, 0.0065, 0.0064,\n",
      "        0.0061, 0.0060, 0.0057, 0.0066, 0.0060, 0.0061, 0.0062, 0.0062, 0.0064,\n",
      "        0.0065, 0.0065, 0.0067, 0.0066, 0.0064, 0.0058, 0.0065, 0.0061, 0.0063,\n",
      "        0.0059, 0.0057, 0.0057, 0.0064, 0.0057, 0.0065, 0.0062, 0.0061, 0.0062,\n",
      "        0.0061, 0.0062, 0.0059, 0.0058, 0.0061, 0.0065, 0.0059, 0.0065, 0.0065,\n",
      "        0.0065, 0.0066, 0.0058, 0.0057, 0.0061, 0.0057, 0.0059, 0.0058, 0.0059,\n",
      "        0.0061, 0.0061, 0.0065, 0.0059, 0.0059, 0.0057, 0.0064, 0.0065, 0.0059,\n",
      "        0.0057, 0.0061, 0.0066, 0.0062, 0.0058, 0.0063, 0.0063, 0.0064, 0.0058,\n",
      "        0.0064, 0.0062, 0.0064, 0.0063, 0.0058, 0.0065, 0.0057, 0.0057, 0.0059,\n",
      "        0.0059], requires_grad=True)\n",
      "27 68\n",
      "toto\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import svd\n",
    "import rings\n",
    "from svd import iterated_power as compute_major_axis\n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "        \n",
    "    def __init__(self,GraphList,normalize=False,node_label='label'):\n",
    "        super(Net, self).__init__()   \n",
    "        self.normalize=normalize\n",
    "        self.node_label=node_label\n",
    "        dict,self.nb_edge_labels=self.build_node_dictionnary(GraphList)\n",
    "        self.nb_labels=len(dict)\n",
    "        print(self.nb_edge_labels)\n",
    "        self.device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        nb_node_pair_label=self.nb_labels*(self.nb_labels-1)/2.0\n",
    "        nb_edge_pair_label=int(self.nb_edge_labels*(self.nb_edge_labels-1)/2)\n",
    "        \n",
    "        self.node_weighs=nn.Parameter(torch.tensor(1.0/(nb_node_pair_label+nb_edge_pair_label+2))+(1e-3)*torch.rand(int(self.nb_labels*(self.nb_labels-1)/2+1),requires_grad=True,device=self.device)) # all substitution costs+ nodeIns/Del. old version: 0 node subs, 1 nodeIns/Del, 2 : edgeSubs, 3 edgeIns/Del        \n",
    "        self.edge_weighs=nn.Parameter(torch.tensor(1.0/(nb_node_pair_label+nb_edge_pair_label+2))+(1e-3)*torch.rand(nb_edge_pair_label+1,requires_grad=True,device=self.device)) #edgeIns/Del\n",
    "        \n",
    "        self.card=torch.tensor([G.order() for G in GraphList]).to(self.device)\n",
    "        card_max=self.card.max()\n",
    "        self.A=torch.empty((len(GraphList),card_max*card_max),dtype=torch.int,device=self.device)\n",
    "        self.labels=torch.empty((len(GraphList),card_max),dtype=torch.int,device=self.device)\n",
    "        print(self.A.shape)\n",
    "        for k in range(len(GraphList)):\n",
    "            A,l=self.from_networkx_to_tensor(GraphList[k],dict)             \n",
    "            self.A[k,0:A.shape[1]]=A[0]\n",
    "            self.labels[k,0:l.shape[0]]=l\n",
    "        print('adjacency matrices',self.A)\n",
    "        print('node labels',self.labels)\n",
    "        print('order of the graphs',self.card)\n",
    "        \n",
    "    def forward(self, input):        \n",
    "        ged=torch.zeros(len(input)).to(self.device) \n",
    "        node_costs,nodeInsDel,edge_costs,edgeInsDel=self.from_weighs_to_costs()\n",
    "        \n",
    "            \n",
    "        \n",
    "        #print('weighs:',self.weighs.device,'device:',self.device,'card:',self.card.device,'A:',self.A.device,'labels:',self.labels.device)\n",
    "        for k in range(len(input)):            \n",
    "            g1=input[k][0]\n",
    "            g2=input[k][1]\n",
    "            n=self.card[g1]\n",
    "            m=self.card[g2]\n",
    "            \n",
    "            self.ring_g,self.ring_h = rings.build_rings(g1,edgeInsDel.size()), rings.build_rings(g2,edgeInsDel.size()) \n",
    "            \n",
    "            C=self.construct_cost_matrix(g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)      \n",
    "            #S=self.mapping_from_similarity(C,n,m)\n",
    "            #S=self.mapping_from_cost(C,n,m)   \n",
    "            #S=self.new_mapping_from_cost(C,n,m,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)\n",
    "            S=self.mapping_from_cost_sans_FW(n,m,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)\n",
    "            \n",
    "            v=torch.flatten(S)\n",
    "            \n",
    "            normalize_factor=1.0\n",
    "            if self.normalize:\n",
    "                nb_edge1=(self.A[g1][0:n*n] != torch.zeros(n*n,device=self.device)).int().sum()\n",
    "                nb_edge2=(self.A[g2][0:m*m] != torch.zeros(m*m,device=self.device)).int().sum()\n",
    "                normalize_factor=nodeInsDel*(n+m)+edgeInsDel*(nb_edge1+nb_edge2)\n",
    "            c=torch.diag(C)\n",
    "            D=C-torch.eye(C.shape[0],device=self.device)*c\n",
    "            ged[k]=(.5*v.T@D@v+c.T@v)/normalize_factor\n",
    "        max=torch.max(ged)\n",
    "        min=torch.min(ged)\n",
    "        ged=(ged-min)/(max-min)\n",
    "        \n",
    "        return ged\n",
    "    \n",
    "    def from_weighs_to_costs(self):\n",
    "        \n",
    "        #cn=torch.exp(self.node_weighs)\n",
    "        #ce=torch.exp(self.edge_weighs)\n",
    "        cn=self.node_weighs*self.node_weighs\n",
    "        ce=self.edge_weighs*self.edge_weighs\n",
    "        total_cost=cn.sum()+ce.sum()\n",
    "        cn=cn/total_cost #/max\n",
    "        ce=ce/total_cost\n",
    "        edgeInsDel=ce[-1]\n",
    "\n",
    "        node_costs=torch.zeros((self.nb_labels,self.nb_labels),device=self.device)\n",
    "        upper_part=torch.triu_indices(node_costs.shape[0],node_costs.shape[1],offset=1,device=self.device)        \n",
    "        node_costs[upper_part[0],upper_part[1]]=cn[0:-1]\n",
    "        node_costs=node_costs+node_costs.T\n",
    "\n",
    "        if self.nb_edge_labels>1:\n",
    "            edge_costs=torch.zeros((self.nb_edge_labels,self.nb_edge_labels),device=self.device)\n",
    "            upper_part=torch.triu_indices(edge_costs.shape[0],edge_costs.shape[1],offset=1,device=self.device)        \n",
    "            edge_costs[upper_part[0],upper_part[1]]=ce[0:-1]\n",
    "            edge_costs=edge_costs+edge_costs.T\n",
    "        else:\n",
    "            edge_costs=torch.zeros(0,device=self.device)\n",
    "        \n",
    "        return node_costs,cn[-1],edge_costs,edgeInsDel\n",
    "    \n",
    "    def build_node_dictionnary(self,GraphList):\n",
    "        #extraction de tous les labels d'atomes\n",
    "        node_labels=[]\n",
    "        for G in Gs:\n",
    "            for v in nx.nodes(G):\n",
    "                if not G.nodes[v][self.node_label][0] in node_labels:\n",
    "                    node_labels.append(G.nodes[v][self.node_label][0])\n",
    "        node_labels.sort()\n",
    "        #extraction d'un dictionnaire permettant de numéroter chaque label par un numéro.\n",
    "        dict={}\n",
    "        k=0\n",
    "        for label in node_labels:\n",
    "            dict[label]=k\n",
    "            k=k+1\n",
    "        print(node_labels)\n",
    "        print(dict,len(dict))\n",
    "    \n",
    "        return dict,max(max([[int(G[e[0]][e[1]]['bond_type']) for e in G.edges()] for G in GraphList]))\n",
    "    \n",
    "    def from_networkx_to_tensor(self,G,dict):    \n",
    "        A=torch.tensor(nx.to_scipy_sparse_matrix(G,dtype=int,weight='bond_type').todense(),dtype=torch.int)        \n",
    "        lab=[dict[G.nodes[v][self.node_label][0]] for v in nx.nodes(G)]\n",
    "   \n",
    "        return (A.view(1,A.shape[0]*A.shape[1]),torch.tensor(lab))\n",
    "\n",
    "    def construct_cost_matrix(self,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel):\n",
    "        n = self.card[g1].item()\n",
    "        m = self.card[g2].item()\n",
    "        \n",
    "        A1=torch.zeros((n+1,n+1),dtype=torch.int,device=self.device)\n",
    "        A1[0:n,0:n]=self.A[g1][0:n*n].view(n,n)\n",
    "        A2=torch.zeros((m+1,m+1),dtype=torch.int,device=self.device)\n",
    "        A2[0:m,0:m]=self.A[g2][0:m*m].view(m,m)\n",
    "        \n",
    "        \n",
    "         # costs: 0 node subs, 1 nodeIns/Del, 2 : edgeSubs, 3 edgeIns/Del\n",
    "        \n",
    "        #C=cost[3]*torch.cat([torch.cat([C12[l][k] for k in range(n+1)],1) for l in range(n+1)])\n",
    "        #Pas bien sur mais cela semble fonctionner.\n",
    "        C=edgeInsDel*self.matrix_edgeInsDel(A1,A2)\n",
    "        if self.nb_edge_labels>1:\n",
    "            for k in range(self.nb_edge_labels):\n",
    "                for l in range(self.nb_edge_labels):\n",
    "                    if k != l:\n",
    "#                    C.add_(self.matrix_edgeSubst(A1,A2,k+1,l+1),alpha=edge_costs[k][l])\n",
    "                        C=C+edge_costs[k][l]*self.matrix_edgeSubst(A1,A2,k+1,l+1)\n",
    "        #C=cost[3]*torch.tensor(np.array([ [  k!=l and A1[k//(m+1),l//(m+1)]^A2[k%(m+1),l%(m+1)] for k in range((n+1)*(m+1))] for l in range((n+1)*(m+1))]),device=self.device)        \n",
    "\n",
    "        l1=self.labels[g1][0:n]\n",
    "        l2=self.labels[g2][0:m]\n",
    "        D=torch.zeros((n+1)*(m+1),device=self.device)\n",
    "        D[n*(m+1):]=nodeInsDel\n",
    "        D[n*(m+1)+m]=0\n",
    "        D[[i*(m+1)+m for i in range(n)]]=nodeInsDel\n",
    "        D[[k for k in range(n*(m+1)) if k%(m+1) != m]]=torch.tensor([node_costs[l1[k//(m+1)],l2[k%(m+1)]] for k in range(n*(m+1)) if k%(m+1) != m],device=self.device )\n",
    "        mask = torch.diag(torch.ones_like(D))\n",
    "        C=mask*torch.diag(D) + (1. - mask)*C\n",
    "        \n",
    "        #C[range(len(C)),range(len(C))]=D\n",
    "      \n",
    "        return C\n",
    "    def matrix_edgeInsDel(self,A1,A2):\n",
    "        Abin1=(A1!=torch.zeros((A1.shape[0],A1.shape[1]),device=self.device))\n",
    "        Abin2=(A2!=torch.zeros((A2.shape[0],A2.shape[1]),device=self.device))\n",
    "        C1=torch.einsum('ij,kl->ijkl',torch.logical_not(Abin1),Abin2)\n",
    "        C2=torch.einsum('ij,kl->ijkl',Abin1,torch.logical_not(Abin2))\n",
    "        C12=torch.logical_or(C1,C2).int()\n",
    "    \n",
    "        return torch.cat(torch.unbind(torch.cat(torch.unbind(C12,1),1),0),1)\n",
    "\n",
    "    def matrix_edgeSubst(self,A1,A2,lab1,lab2):\n",
    "        Abin1=(A1==lab1*torch.ones((A1.shape[0],A1.shape[1]),device=self.device)).int()\n",
    "        Abin2=(A2==lab2*torch.ones((A2.shape[0],A2.shape[1]),device=self.device)).int()\n",
    "        C=torch.einsum('ij,kl->ijkl',Abin1,Abin2)\n",
    "        \n",
    "        return torch.cat(torch.unbind(torch.cat(torch.unbind(C,1),1),0),1)\n",
    "    \n",
    "    def similarity_from_cost(self,C):\n",
    "        N=C.shape[0]\n",
    "             \n",
    "        #return (torch.norm(C,p='fro')*torch.eye(N,device=self.device) -C)\n",
    "        return (C.max()*torch.eye(N,device=self.device) -C)\n",
    "    \n",
    "    def lsape_populate_instance(self,first_graph,second_graph,average_node_cost, average_edge_cost,alpha,lbda):       #ring_g, ring_h come from global ring with all graphs in so ring_g = rings['g'] and ring_h = rings['h']\n",
    "        g,h = Gs[first_graph], Gs[second_graph]\n",
    "        self.average_cost =[average_node_cost, average_edge_cost]\n",
    "        self.first_graph, self.second_graph = first_graph,second_graph\n",
    "        \n",
    "        node_costs,nodeInsDel,edge_costs,edgeInsDel=self.from_weighs_to_costs()\n",
    "\n",
    "        lsape_instance = [[0 for _ in range(len(g) + 1)] for __ in range(len(h) + 1)]\n",
    "        for g_node_index in range(len(g) + 1):\n",
    "            for h_node_index in range(len(h) + 1):\n",
    "                lsape_instance[h_node_index][g_node_index] = rings.compute_ring_distance(g,h,self.ring_g,self.ring_h,g_node_index,h_node_index,alpha,lbda,node_costs,nodeInsDel,edge_costs,edgeInsDel,first_graph,second_graph)\n",
    "        for i in lsape_instance :\n",
    "            i = torch.as_tensor(i)\n",
    "        lsape_instance = torch.as_tensor(lsape_instance)\n",
    "        #print(type(lsape_instance))\n",
    "        return lsape_instance\n",
    "    \n",
    "  \n",
    "    def mapping_from_cost_sans_FW(self,n,m,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel): \n",
    "        c_0 =self.lsape_populate_instance(g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)\n",
    "        x0=svd.eps_assigment_from_mapping(torch.exp(-c_0),10).view((n+1)*(m+1),1)\n",
    "        return x0\n",
    "    \n",
    "    def new_mapping_from_cost(self,C,n,m,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel): \n",
    "        c=torch.diag(C)       \n",
    "        c_0 =self.lsape_populate_instance(g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)\n",
    "        D=C-torch.eye(C.shape[0],device=self.device)*c\n",
    "        x0=svd.eps_assigment_from_mapping(torch.exp(-c_0),10).view((n+1)*(m+1),1)\n",
    "        return svd.franck_wolfe(x0,D,c,5,15,n,m)\n",
    "    \n",
    "    \n",
    "    def mapping_from_cost(self,C,n,m):\n",
    "        c=torch.diag(C)\n",
    "        D=C-torch.eye(C.shape[0],device=self.device)*c\n",
    "        x0=svd.eps_assigment_from_mapping(torch.exp(-.5*c.view(n+1,m+1)),10).view((n+1)*(m+1),1)\n",
    "        x=svd.franck_wolfe(x0,D,c,5,10,n,m)\n",
    "        def print_grad(grad):\n",
    "            if(grad.norm()!= 0.0):\n",
    "                print(grad)\n",
    "        \n",
    "#        x.register_hook(print_grad)\n",
    "        return x\n",
    "\n",
    "print('Gs=',len(Gs))\n",
    "model = Net(Gs,normalize=True,node_label='extended_label')\n",
    "\n",
    "params = list(model.parameters())\n",
    "print(len(params))\n",
    "print(params[0])\n",
    "#print(model(input))\n",
    "print(max([G.order() for G in Gs]),len(Gs))\n",
    "print('toto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4,  3, 25, 11, 21,  2, 11, 25, 14, 17, 17, 12])\n",
      "train graphs: tensor([19, 37, 59, 34, 18, 10, 19, 10, 53, 60, 60, 25,  6, 12, 61, 42, 49])\n",
      "couples= tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "          3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,\n",
      "          5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,\n",
      "          9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         12, 12, 12, 12, 13, 13, 13, 14, 14, 15],\n",
      "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  2,  3,\n",
      "          4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  3,  4,  5,  6,  7,\n",
      "          8,  9, 10, 11, 12, 13, 14, 15, 16,  4,  5,  6,  7,  8,  9, 10, 11, 12,\n",
      "         13, 14, 15, 16,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  6,  7,\n",
      "          8,  9, 10, 11, 12, 13, 14, 15, 16,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
      "         16,  8,  9, 10, 11, 12, 13, 14, 15, 16,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "         10, 11, 12, 13, 14, 15, 16, 11, 12, 13, 14, 15, 16, 12, 13, 14, 15, 16,\n",
      "         13, 14, 15, 16, 14, 15, 16, 15, 16, 16]])\n",
      "nb_class1/nb_class2= 12 5\n",
      "couples restreints: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "          3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,\n",
      "          5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,\n",
      "          9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11],\n",
      "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  2,  3,\n",
      "          4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  3,  4,  5,  6,  7,\n",
      "          8,  9, 10, 11, 12, 13, 14, 15, 16,  4,  5,  6,  7,  8,  9, 10, 11, 12,\n",
      "         13, 14, 15, 16,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  6,  7,\n",
      "          8,  9, 10, 11, 12, 13, 14, 15, 16,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
      "         16,  8,  9, 10, 11, 12, 13, 14, 15, 16,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "         10, 11, 12, 13, 14, 15, 16, 11, 12, 13, 14, 15, 16, 12, 13, 14, 15, 16]])\n",
      "old_size, new size= 126 126.0\n",
      "126\n",
      "data= tensor([[19, 37],\n",
      "        [19, 59],\n",
      "        [19, 34],\n",
      "        [19, 18],\n",
      "        [19, 10],\n",
      "        [19, 19],\n",
      "        [19, 10],\n",
      "        [19, 53],\n",
      "        [19, 60],\n",
      "        [19, 60],\n",
      "        [19, 25],\n",
      "        [19,  6],\n",
      "        [19, 12],\n",
      "        [19, 61],\n",
      "        [19, 42],\n",
      "        [19, 49],\n",
      "        [37, 59],\n",
      "        [37, 34],\n",
      "        [37, 18],\n",
      "        [37, 10],\n",
      "        [37, 19],\n",
      "        [37, 10],\n",
      "        [37, 53],\n",
      "        [37, 60],\n",
      "        [37, 60],\n",
      "        [37, 25],\n",
      "        [37,  6],\n",
      "        [37, 12],\n",
      "        [37, 61],\n",
      "        [37, 42],\n",
      "        [37, 49],\n",
      "        [59, 34],\n",
      "        [59, 18],\n",
      "        [59, 10],\n",
      "        [59, 19],\n",
      "        [59, 10],\n",
      "        [59, 53],\n",
      "        [59, 60],\n",
      "        [59, 60],\n",
      "        [59, 25],\n",
      "        [59,  6],\n",
      "        [59, 12],\n",
      "        [59, 61],\n",
      "        [59, 42],\n",
      "        [59, 49],\n",
      "        [34, 18],\n",
      "        [34, 10],\n",
      "        [34, 19],\n",
      "        [34, 10],\n",
      "        [34, 53],\n",
      "        [34, 60],\n",
      "        [34, 60],\n",
      "        [34, 25],\n",
      "        [34,  6],\n",
      "        [34, 12],\n",
      "        [34, 61],\n",
      "        [34, 42],\n",
      "        [34, 49],\n",
      "        [18, 10],\n",
      "        [18, 19],\n",
      "        [18, 10],\n",
      "        [18, 53],\n",
      "        [18, 60],\n",
      "        [18, 60],\n",
      "        [18, 25],\n",
      "        [18,  6],\n",
      "        [18, 12],\n",
      "        [18, 61],\n",
      "        [18, 42],\n",
      "        [18, 49],\n",
      "        [10, 19],\n",
      "        [10, 10],\n",
      "        [10, 53],\n",
      "        [10, 60],\n",
      "        [10, 60],\n",
      "        [10, 25],\n",
      "        [10,  6],\n",
      "        [10, 12],\n",
      "        [10, 61],\n",
      "        [10, 42],\n",
      "        [10, 49],\n",
      "        [19, 10],\n",
      "        [19, 53],\n",
      "        [19, 60],\n",
      "        [19, 60],\n",
      "        [19, 25],\n",
      "        [19,  6],\n",
      "        [19, 12],\n",
      "        [19, 61],\n",
      "        [19, 42],\n",
      "        [19, 49],\n",
      "        [10, 53],\n",
      "        [10, 60],\n",
      "        [10, 60],\n",
      "        [10, 25],\n",
      "        [10,  6],\n",
      "        [10, 12],\n",
      "        [10, 61],\n",
      "        [10, 42],\n",
      "        [10, 49],\n",
      "        [53, 60],\n",
      "        [53, 60],\n",
      "        [53, 25],\n",
      "        [53,  6],\n",
      "        [53, 12],\n",
      "        [53, 61],\n",
      "        [53, 42],\n",
      "        [53, 49],\n",
      "        [60, 60],\n",
      "        [60, 25],\n",
      "        [60,  6],\n",
      "        [60, 12],\n",
      "        [60, 61],\n",
      "        [60, 42],\n",
      "        [60, 49],\n",
      "        [60, 25],\n",
      "        [60,  6],\n",
      "        [60, 12],\n",
      "        [60, 61],\n",
      "        [60, 42],\n",
      "        [60, 49],\n",
      "        [25,  6],\n",
      "        [25, 12],\n",
      "        [25, 61],\n",
      "        [25, 42],\n",
      "        [25, 49]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "nb=len(Gs)\n",
    "class1=torch.tensor([k for k in range(len(y)) if y[k]==1])\n",
    "class2=torch.tensor([k for k in range(len(y)) if y[k]==0])\n",
    "\n",
    "nb_class1=12\n",
    "nb_class2=int((nb_class1-1)/2)\n",
    "train_size=nb_class1+nb_class2\n",
    "#train_size=20\n",
    "\n",
    "#if train_size % 2 == 0:\n",
    "#    nb_class1=int(train_size/2)\n",
    "#    nb_class2=int(train_size/2)\n",
    "#else:\n",
    "#    nb_class1=int(train_size/2)+1\n",
    "#    nb_class2=int(train_size/2)\n",
    "    \n",
    "print((torch.abs(10000*torch.randn(nb_class1)).int()%class1.size()[0]).long())\n",
    "random_class1=class1[(torch.abs(10000*torch.randn(nb_class1)).int()%class1.size()[0]).long()]\n",
    "random_class2=class2[(torch.abs(10000*torch.randn(nb_class2)).int()%class2.size()[0]).long()]\n",
    "train_graphs=torch.cat((random_class1,random_class2),0)\n",
    "print('train graphs:',train_graphs)\n",
    "\n",
    "\n",
    "couples=torch.triu_indices(train_size,train_size,offset=1)\n",
    "print('couples=',couples)\n",
    "print('nb_class1/nb_class2=',nb_class1,nb_class2)\n",
    "#combinations=itertools.combinations(range(nb),2)\n",
    "\n",
    "nb_elt=int(nb_class1*(nb_class1+2*nb_class2-1)/2)\n",
    "print('couples restreints:',couples[:,0:nb_elt])\n",
    "\n",
    "#nb_elt=int(train_size*(train_size-1)/2)\n",
    "data=torch.empty((nb_elt,2),dtype=torch.int)\n",
    "yt=torch.ones(nb_elt)\n",
    "print('old_size, new size=',nb_elt,.5*nb_class1*(nb_class1+2*nb_class2-1))\n",
    "data[0:nb_elt,0]=train_graphs[couples[0,0:nb_elt]]\n",
    "data[0:nb_elt,1]=train_graphs[couples[1,0:nb_elt]]\n",
    "\n",
    "\n",
    "#data[0:nb_elt,0]=train_graphs[couples[0]]\n",
    "#data[0:nb_elt,1]=train_graphs[couples[1]]\n",
    "print(nb_elt)\n",
    "#couples=[]\n",
    "for k in range(nb_elt):\n",
    "    if (y[data[k][0]]!=y[data[k][1]]):\n",
    "        yt[k]=-1.0        \n",
    "\n",
    "print('data=',data)\n",
    "\n",
    "#print(couples[1:2])\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")          # a CUDA device object    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 32\n",
      "1 4\n",
      "i :  tensor([[19, 59],\n",
      "        [18, 25],\n",
      "        [19, 61],\n",
      "        [25, 42],\n",
      "        [37, 53],\n",
      "        [59, 12],\n",
      "        [19, 60],\n",
      "        [10, 42]], dtype=torch.int32) \n",
      " j :  tensor([ 1.,  1., -1., -1.,  1., -1.,  1., -1.])\n",
      "i :  tensor([[34, 60],\n",
      "        [19, 53],\n",
      "        [18, 12],\n",
      "        [18, 42],\n",
      "        [59, 10],\n",
      "        [37, 42],\n",
      "        [19, 42],\n",
      "        [60, 49]], dtype=torch.int32) \n",
      " j :  tensor([ 1.,  1., -1., -1.,  1., -1., -1., -1.])\n",
      "i :  tensor([[59, 49],\n",
      "        [53, 49],\n",
      "        [19, 18],\n",
      "        [60, 49],\n",
      "        [19, 25],\n",
      "        [60,  6],\n",
      "        [19, 60],\n",
      "        [53, 60]], dtype=torch.int32) \n",
      " j :  tensor([-1., -1.,  1., -1.,  1., -1.,  1.,  1.])\n",
      "i :  tensor([[60, 60],\n",
      "        [59,  6],\n",
      "        [60, 25],\n",
      "        [18, 19],\n",
      "        [59, 18],\n",
      "        [59, 61],\n",
      "        [19, 10],\n",
      "        [37, 10]], dtype=torch.int32) \n",
      " j :  tensor([ 1., -1.,  1.,  1.,  1., -1.,  1.,  1.])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "\n",
    "[train_D, valid_D,train_L,valid_L]= train_test_split(data,yt, test_size=0.25,train_size=0.75, shuffle=True) #, stratify=yt)\n",
    "#print(train_D)   \n",
    "    \n",
    "DatasetTrain = TensorDataset(train_D, train_L)\n",
    "\n",
    "DatasetValid=TensorDataset(valid_D, valid_L)\n",
    "\n",
    "trainloader=torch.utils.data.DataLoader(DatasetTrain,batch_size=len(train_D),shuffle=True,drop_last=True, num_workers=0)\n",
    "#len(train_D) #31\n",
    "validationloader=torch.utils.data.DataLoader(DatasetValid, batch_size=8, drop_last=True,num_workers=0)\n",
    "\n",
    "print(len(train_D), len(valid_D))\n",
    "print(len(trainloader),len(validationloader))\n",
    "for i,j in validationloader:\n",
    "    print('i : ',i,'\\n j : ',j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if torch.cuda.device_count() > 1:\n",
    "#  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "#  model = nn.DataParallel(model)\n",
    "from triangular_losses import TriangularConstraint as triangular_constraint\n",
    "\n",
    "model.to(device)\n",
    "def classification(model,data,yt,nb_iter):\n",
    "\n",
    "    criterion = torch.nn.HingeEmbeddingLoss(margin=1.0,reduction='mean')\n",
    "    criterionTri=triangular_constraint()\n",
    "    optimizer = torch.optim.Adam(model.parameters()) #, lr=1e-3\n",
    "    #print(device)\n",
    "\n",
    "    #torch.cat((same_class[0:20],diff_class[0:20]),0).to(device)\n",
    "    whole_input=data.to(device)\n",
    "    target=yt.to(device) \n",
    "    #torch.ones(40,device=device)\n",
    "    #target[20:]=-1.0\n",
    "    #target=(yt[0:20]).to(device)\n",
    "    InsDel=np.empty((nb_iter,2))\n",
    "    node_costs,nodeInsDel,edge_costs,edgeInsDel=model.from_weighs_to_costs()\n",
    "    nodeSub=np.empty((nb_iter,int(node_costs.shape[0]*(node_costs.shape[0]-1)/2)))\n",
    "    edgeSub=np.empty((nb_iter,int(edge_costs.shape[0]*(edge_costs.shape[0]-1)/2)))\n",
    "    loss_plt=np.empty(nb_iter)\n",
    "    loss_train_plt=np.empty(nb_iter)\n",
    "    loss_valid_plt=np.empty(nb_iter)\n",
    "    min_valid_loss = np.inf\n",
    "    iter_min_valid_loss = 0\n",
    "    \n",
    "    for t in range(nb_iter):    \n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        tmp=np.inf\n",
    "        for train_data,train_labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            inputt=train_data.to(device)\n",
    "            \n",
    "            # Forward pass: Compute predicted y by passing data to the model\n",
    "            y_pred = model(whole_input).to(device)\n",
    "\n",
    "            # Compute and print loss\n",
    "            loss = criterion(y_pred, target).to(device)\n",
    "            node_costs,nodeInsDel,edge_costs,edgeInsDel=model.from_weighs_to_costs()\n",
    "            triangularInq=criterionTri(node_costs,nodeInsDel,edge_costs,edgeInsDel)\n",
    "            loss=loss*(1+triangularInq)\n",
    "            loss.to(device)\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            #optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print('loss.item of the train = ', t, loss.item())\n",
    "            train_loss =+ loss.item() #* train_data.size(0) \n",
    "            if (loss.item()<tmp): tmp=loss.item()\n",
    "        loss_plt[t]=loss.item()  \n",
    "        loss_train_plt[t]=train_loss /len(trainloader)\n",
    "        #loss_plt[t]=tmp\n",
    "            \n",
    "        if t % 100 == 99 or t==0:   \n",
    "            print('ged=',y_pred*target)  #train_labels\n",
    "            print('Distances: ',y_pred)\n",
    "            print('Loss Triangular:',triangularInq.item())\n",
    "            print('node_costs :')\n",
    "            print(node_costs)\n",
    "            print('nodeInsDel:',nodeInsDel.item())\n",
    "            print('edge_costs :')\n",
    "            print(edge_costs)\n",
    "            print('edgeInsDel:',edgeInsDel.item())\n",
    "            \n",
    "            \n",
    "            \n",
    "        for valid_data,valid_labels in validationloader:\n",
    "            \n",
    "            inputt=valid_data.to(device)\n",
    "            y_pred = model(inputt).to(device)\n",
    "            # Compute and print loss\n",
    "            loss = criterion(y_pred, valid_labels).to(device)    \n",
    "            loss.to(device)\n",
    "            \n",
    "            print('loss.item of the valid = ', t, loss.item())  \n",
    "            valid_loss = valid_loss + loss.item() #* valid_data.size(0)\n",
    "            \n",
    "        loss_valid_plt[t]=valid_loss / len(validationloader)   \n",
    "        \n",
    "        InsDel[t][0]=nodeInsDel.item()\n",
    "        InsDel[t][1]=edgeInsDel.item()\n",
    "        \n",
    "        \n",
    "        k=0\n",
    "        for p in range(node_costs.shape[0]):\n",
    "            for q in range(p+1,node_costs.shape[0]):\n",
    "                nodeSub[t][k]=node_costs[p][q]\n",
    "                k=k+1\n",
    "        k=0\n",
    "        for p in range(edge_costs.shape[0]):\n",
    "            for q in range(p+1,edge_costs.shape[0]):\n",
    "                edgeSub[t][k]=edge_costs[p][q]\n",
    "                k=k+1\n",
    "        \n",
    "            \n",
    "        print(f'Iteration {t+1} \\t\\t Training Loss: {\\\n",
    "        train_loss / len(trainloader)} \\t\\t Validation Loss: {\\\n",
    "        valid_loss / len(validationloader)}')\n",
    "        if min_valid_loss > valid_loss:\n",
    "            print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f})')\n",
    "            min_valid_loss = valid_loss\n",
    "            iter_min_valid_loss = t\n",
    "            nodeSub_min = node_costs\n",
    "            edgeSub_min = edge_costs\n",
    "            nodeInsDel_min = nodeInsDel\n",
    "            edgeInsDel_min = edgeInsDel\n",
    "            \n",
    "            \n",
    "    print('iter and min_valid_loss = ',iter_min_valid_loss, min_valid_loss)\n",
    "    '''\n",
    "    nodeInsDel_min = InsDel[iter_min_valid_loss][0]\n",
    "    edgeInsDel_min = InsDel[iter_min_valid_loss][1]\n",
    "    nodeSub_min = nodeSub[iter_min_valid_loss]\n",
    "    edgeSub_min = edgeSub[iter_min_valid_loss]\n",
    "    '''\n",
    "    print(' Min cost for nodeInsDel = ', nodeInsDel_min)\n",
    "    print(' Min cost for edgeInsDel = ', edgeInsDel_min)\n",
    "    print(' Min cost for nodeSub = ', nodeSub_min)\n",
    "    print(' Min cost for edgeSub = ', edgeSub_min)\n",
    "    torch.save(nodeInsDel_min, 'nodeInsDel_min', pickle_module=pkl) \n",
    "    torch.save(edgeInsDel_min, 'edgeInsDel_min', pickle_module=pkl) \n",
    "    torch.save(nodeSub_min, 'nodeSub_min', pickle_module=pkl) \n",
    "    torch.save(edgeSub_min, 'edgeSub_min', pickle_module=pkl)\n",
    "    return InsDel,nodeSub,edgeSub,loss_plt,loss_valid_plt,loss_train_plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item of the train =  0 0.3693350851535797\n",
      "ged= tensor([ 0.3859,  0.3234,  0.2993,  0.2914,  0.0000,  0.0000,  0.0000,  0.2591,\n",
      "         0.2987,  0.2987,  0.3743, -0.2228, -0.2621, -0.4488, -0.9263, -0.6886,\n",
      "         0.2535,  0.2650,  0.2336,  0.2825,  0.2740,  0.2825,  0.3091,  0.2861,\n",
      "         0.2861,  0.0521, -0.3068, -0.5058, -0.3355, -0.7366, -0.5054,  0.0587,\n",
      "         0.0432,  0.2721,  0.2722,  0.2721,  0.2680,  0.0563,  0.0563,  0.3374,\n",
      "        -0.2626, -0.4402, -0.3470, -0.7345, -0.5087,  0.0530,  0.2511,  0.2512,\n",
      "         0.2511,  0.2687,  0.0458,  0.0458,  0.3409, -0.2487, -0.4226, -0.3225,\n",
      "        -0.7342, -0.4725,  0.2211,  0.2211,  0.2211,  0.2766,  0.0438,  0.0438,\n",
      "         0.3244, -0.2461, -0.3732, -0.2932, -0.6886, -0.4946,  0.0000,  0.0000,\n",
      "         0.2670,  0.2963,  0.2963,  0.3765, -0.2308, -0.2716, -0.4547, -0.9405,\n",
      "        -0.6854,  0.0000,  0.2591,  0.2987,  0.2987,  0.3743, -0.2228, -0.2621,\n",
      "        -0.4488, -0.9263, -0.6886,  0.2670,  0.2963,  0.2963,  0.3765, -0.2308,\n",
      "        -0.2716, -0.4547, -0.9405, -0.6854,  0.3622,  0.3622,  0.5127, -0.0320,\n",
      "        -0.2858, -0.6335, -1.0000, -0.7299,  0.0416,  0.3455, -0.2561, -0.4246,\n",
      "        -0.3235, -0.7178, -0.4790,  0.3455, -0.2561, -0.4246, -0.3235, -0.7178,\n",
      "        -0.4790, -0.2971, -0.5038, -0.3381, -0.7170, -0.5059],\n",
      "       grad_fn=<MulBackward0>)\n",
      "Distances:  tensor([0.3859, 0.3234, 0.2993, 0.2914, 0.0000, 0.0000, 0.0000, 0.2591, 0.2987,\n",
      "        0.2987, 0.3743, 0.2228, 0.2621, 0.4488, 0.9263, 0.6886, 0.2535, 0.2650,\n",
      "        0.2336, 0.2825, 0.2740, 0.2825, 0.3091, 0.2861, 0.2861, 0.0521, 0.3068,\n",
      "        0.5058, 0.3355, 0.7366, 0.5054, 0.0587, 0.0432, 0.2721, 0.2722, 0.2721,\n",
      "        0.2680, 0.0563, 0.0563, 0.3374, 0.2626, 0.4402, 0.3470, 0.7345, 0.5087,\n",
      "        0.0530, 0.2511, 0.2512, 0.2511, 0.2687, 0.0458, 0.0458, 0.3409, 0.2487,\n",
      "        0.4226, 0.3225, 0.7342, 0.4725, 0.2211, 0.2211, 0.2211, 0.2766, 0.0438,\n",
      "        0.0438, 0.3244, 0.2461, 0.3732, 0.2932, 0.6886, 0.4946, 0.0000, 0.0000,\n",
      "        0.2670, 0.2963, 0.2963, 0.3765, 0.2308, 0.2716, 0.4547, 0.9405, 0.6854,\n",
      "        0.0000, 0.2591, 0.2987, 0.2987, 0.3743, 0.2228, 0.2621, 0.4488, 0.9263,\n",
      "        0.6886, 0.2670, 0.2963, 0.2963, 0.3765, 0.2308, 0.2716, 0.4547, 0.9405,\n",
      "        0.6854, 0.3622, 0.3622, 0.5127, 0.0320, 0.2858, 0.6335, 1.0000, 0.7299,\n",
      "        0.0416, 0.3455, 0.2561, 0.4246, 0.3235, 0.7178, 0.4790, 0.3455, 0.2561,\n",
      "        0.4246, 0.3235, 0.7178, 0.4790, 0.2971, 0.5038, 0.3381, 0.7170, 0.5059],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Loss Triangular: 0.0\n",
      "node_costs :\n",
      "tensor([[0.0000, 0.0062, 0.0057, 0.0062, 0.0057, 0.0058, 0.0063, 0.0055, 0.0059,\n",
      "         0.0055, 0.0060, 0.0056, 0.0062, 0.0054, 0.0059, 0.0065, 0.0065, 0.0050,\n",
      "         0.0049],\n",
      "        [0.0062, 0.0000, 0.0051, 0.0056, 0.0052, 0.0055, 0.0049, 0.0065, 0.0050,\n",
      "         0.0053, 0.0063, 0.0059, 0.0049, 0.0052, 0.0064, 0.0051, 0.0067, 0.0056,\n",
      "         0.0055],\n",
      "        [0.0057, 0.0051, 0.0000, 0.0051, 0.0065, 0.0054, 0.0055, 0.0053, 0.0051,\n",
      "         0.0064, 0.0055, 0.0064, 0.0061, 0.0061, 0.0059, 0.0050, 0.0055, 0.0062,\n",
      "         0.0049],\n",
      "        [0.0062, 0.0056, 0.0051, 0.0000, 0.0054, 0.0059, 0.0059, 0.0053, 0.0058,\n",
      "         0.0065, 0.0049, 0.0053, 0.0058, 0.0066, 0.0062, 0.0050, 0.0051, 0.0051,\n",
      "         0.0051],\n",
      "        [0.0057, 0.0052, 0.0065, 0.0054, 0.0000, 0.0052, 0.0059, 0.0062, 0.0061,\n",
      "         0.0055, 0.0053, 0.0056, 0.0066, 0.0054, 0.0049, 0.0057, 0.0061, 0.0058,\n",
      "         0.0056],\n",
      "        [0.0058, 0.0055, 0.0054, 0.0059, 0.0052, 0.0000, 0.0063, 0.0057, 0.0051,\n",
      "         0.0061, 0.0051, 0.0051, 0.0056, 0.0056, 0.0063, 0.0053, 0.0052, 0.0060,\n",
      "         0.0061],\n",
      "        [0.0063, 0.0049, 0.0055, 0.0059, 0.0059, 0.0063, 0.0000, 0.0062, 0.0056,\n",
      "         0.0059, 0.0065, 0.0063, 0.0062, 0.0056, 0.0055, 0.0049, 0.0066, 0.0055,\n",
      "         0.0055],\n",
      "        [0.0055, 0.0065, 0.0053, 0.0053, 0.0062, 0.0057, 0.0062, 0.0000, 0.0057,\n",
      "         0.0058, 0.0062, 0.0063, 0.0063, 0.0067, 0.0065, 0.0061, 0.0051, 0.0064,\n",
      "         0.0056],\n",
      "        [0.0059, 0.0050, 0.0051, 0.0058, 0.0061, 0.0051, 0.0056, 0.0057, 0.0000,\n",
      "         0.0060, 0.0053, 0.0050, 0.0049, 0.0062, 0.0049, 0.0063, 0.0057, 0.0055,\n",
      "         0.0057],\n",
      "        [0.0055, 0.0053, 0.0064, 0.0065, 0.0055, 0.0061, 0.0059, 0.0058, 0.0060,\n",
      "         0.0000, 0.0055, 0.0058, 0.0053, 0.0051, 0.0056, 0.0063, 0.0053, 0.0064,\n",
      "         0.0063],\n",
      "        [0.0060, 0.0063, 0.0055, 0.0049, 0.0053, 0.0051, 0.0065, 0.0062, 0.0053,\n",
      "         0.0055, 0.0000, 0.0063, 0.0066, 0.0051, 0.0049, 0.0057, 0.0049, 0.0052,\n",
      "         0.0051],\n",
      "        [0.0056, 0.0059, 0.0064, 0.0053, 0.0056, 0.0051, 0.0063, 0.0063, 0.0050,\n",
      "         0.0058, 0.0063, 0.0000, 0.0052, 0.0056, 0.0056, 0.0063, 0.0052, 0.0053,\n",
      "         0.0049],\n",
      "        [0.0062, 0.0049, 0.0061, 0.0058, 0.0066, 0.0056, 0.0062, 0.0063, 0.0049,\n",
      "         0.0053, 0.0066, 0.0052, 0.0000, 0.0062, 0.0063, 0.0052, 0.0050, 0.0057,\n",
      "         0.0066],\n",
      "        [0.0054, 0.0052, 0.0061, 0.0066, 0.0054, 0.0056, 0.0056, 0.0067, 0.0062,\n",
      "         0.0051, 0.0051, 0.0056, 0.0062, 0.0000, 0.0058, 0.0051, 0.0059, 0.0060,\n",
      "         0.0062],\n",
      "        [0.0059, 0.0064, 0.0059, 0.0062, 0.0049, 0.0063, 0.0055, 0.0065, 0.0049,\n",
      "         0.0056, 0.0049, 0.0056, 0.0063, 0.0058, 0.0000, 0.0050, 0.0062, 0.0058,\n",
      "         0.0061],\n",
      "        [0.0065, 0.0051, 0.0050, 0.0050, 0.0057, 0.0053, 0.0049, 0.0061, 0.0063,\n",
      "         0.0063, 0.0057, 0.0063, 0.0052, 0.0051, 0.0050, 0.0000, 0.0059, 0.0050,\n",
      "         0.0063],\n",
      "        [0.0065, 0.0067, 0.0055, 0.0051, 0.0061, 0.0052, 0.0066, 0.0051, 0.0057,\n",
      "         0.0053, 0.0049, 0.0052, 0.0050, 0.0059, 0.0062, 0.0059, 0.0000, 0.0050,\n",
      "         0.0049],\n",
      "        [0.0050, 0.0056, 0.0062, 0.0051, 0.0058, 0.0060, 0.0055, 0.0064, 0.0055,\n",
      "         0.0064, 0.0052, 0.0053, 0.0057, 0.0060, 0.0058, 0.0050, 0.0050, 0.0000,\n",
      "         0.0052],\n",
      "        [0.0049, 0.0055, 0.0049, 0.0051, 0.0056, 0.0061, 0.0055, 0.0056, 0.0057,\n",
      "         0.0063, 0.0051, 0.0049, 0.0066, 0.0062, 0.0061, 0.0063, 0.0049, 0.0052,\n",
      "         0.0000]], grad_fn=<AddBackward0>)\n",
      "nodeInsDel: 0.005144317634403706\n",
      "edge_costs :\n",
      "tensor([[0.0000, 0.0058, 0.0049],\n",
      "        [0.0058, 0.0000, 0.0067],\n",
      "        [0.0049, 0.0067, 0.0000]], grad_fn=<AddBackward0>)\n",
      "edgeInsDel: 0.004988777916878462\n",
      "loss.item of the valid =  0 0.2385893166065216\n",
      "loss.item of the valid =  0 0.2613212466239929\n",
      "loss.item of the valid =  0 0.30376192927360535\n",
      "loss.item of the valid =  0 0.34255826473236084\n",
      "Iteration 1 \t\t Training Loss: 0.3693350851535797 \t\t Validation Loss: 0.2865576893091202\n",
      "Validation Loss Decreased(inf--->1.146231)\n",
      "loss.item of the train =  1 0.35481899976730347\n",
      "loss.item of the valid =  1 0.23378948867321014\n",
      "loss.item of the valid =  1 0.2583467364311218\n",
      "loss.item of the valid =  1 0.29549986124038696\n",
      "loss.item of the valid =  1 0.31703463196754456\n",
      "Iteration 2 \t\t Training Loss: 0.35481899976730347 \t\t Validation Loss: 0.27616767957806587\n",
      "Validation Loss Decreased(1.146231--->1.104671)\n",
      "loss.item of the train =  2 0.3434411287307739\n",
      "loss.item of the valid =  2 0.23168180882930756\n",
      "loss.item of the valid =  2 0.2546958327293396\n",
      "loss.item of the valid =  2 0.28863123059272766\n",
      "loss.item of the valid =  2 0.2775149345397949\n",
      "Iteration 3 \t\t Training Loss: 0.3434411287307739 \t\t Validation Loss: 0.26313095167279243\n",
      "Validation Loss Decreased(1.104671--->1.052524)\n",
      "loss.item of the train =  3 0.33315977454185486\n",
      "loss.item of the valid =  3 0.22819998860359192\n",
      "loss.item of the valid =  3 0.247017502784729\n",
      "loss.item of the valid =  3 0.2800823152065277\n",
      "loss.item of the valid =  3 0.3187229037284851\n",
      "Iteration 4 \t\t Training Loss: 0.33315977454185486 \t\t Validation Loss: 0.26850567758083344\n",
      "loss.item of the train =  4 0.32084938883781433\n",
      "loss.item of the valid =  4 0.22812123596668243\n",
      "loss.item of the valid =  4 0.24613754451274872\n",
      "loss.item of the valid =  4 0.27841684222221375\n",
      "loss.item of the valid =  4 0.3283918499946594\n",
      "Iteration 5 \t\t Training Loss: 0.32084938883781433 \t\t Validation Loss: 0.2702668681740761\n",
      "loss.item of the train =  5 0.32041898369789124\n",
      "loss.item of the valid =  5 0.22996927797794342\n",
      "loss.item of the valid =  5 0.24723105132579803\n",
      "loss.item of the valid =  5 0.2762792706489563\n",
      "loss.item of the valid =  5 0.22721582651138306\n",
      "Iteration 6 \t\t Training Loss: 0.32041898369789124 \t\t Validation Loss: 0.2451738566160202\n",
      "Validation Loss Decreased(1.052524--->0.980695)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item of the train =  6 0.32140669226646423\n",
      "loss.item of the valid =  6 0.2345772236585617\n",
      "loss.item of the valid =  6 0.2439938336610794\n",
      "loss.item of the valid =  6 0.26560238003730774\n",
      "loss.item of the valid =  6 0.23751287162303925\n",
      "Iteration 7 \t\t Training Loss: 0.32140669226646423 \t\t Validation Loss: 0.24542157724499702\n",
      "loss.item of the train =  7 0.3214847445487976\n",
      "loss.item of the valid =  7 0.23568563163280487\n",
      "loss.item of the valid =  7 0.24272069334983826\n",
      "loss.item of the valid =  7 0.26342421770095825\n",
      "loss.item of the valid =  7 0.24614295363426208\n",
      "Iteration 8 \t\t Training Loss: 0.3214847445487976 \t\t Validation Loss: 0.24699337407946587\n",
      "loss.item of the train =  8 0.3220807909965515\n",
      "loss.item of the valid =  8 0.2292618602514267\n",
      "loss.item of the valid =  8 0.2421858012676239\n",
      "loss.item of the valid =  8 0.26907065510749817\n",
      "loss.item of the valid =  8 0.26681941747665405\n",
      "Iteration 9 \t\t Training Loss: 0.3220807909965515 \t\t Validation Loss: 0.2518344335258007\n",
      "loss.item of the train =  9 0.3185228109359741\n",
      "loss.item of the valid =  9 0.22835272550582886\n",
      "loss.item of the valid =  9 0.2452813982963562\n",
      "loss.item of the valid =  9 0.2751617431640625\n",
      "loss.item of the valid =  9 0.2716979384422302\n",
      "Iteration 10 \t\t Training Loss: 0.3185228109359741 \t\t Validation Loss: 0.25512345135211945\n",
      "loss.item of the train =  10 0.3180369436740875\n",
      "loss.item of the valid =  10 0.22882702946662903\n",
      "loss.item of the valid =  10 0.24843086302280426\n",
      "loss.item of the valid =  10 0.2789073586463928\n",
      "loss.item of the valid =  10 0.2320665419101715\n",
      "Iteration 11 \t\t Training Loss: 0.3180369436740875 \t\t Validation Loss: 0.2470579482614994\n",
      "loss.item of the train =  11 0.3201656639575958\n",
      "loss.item of the valid =  11 0.2284519374370575\n",
      "loss.item of the valid =  11 0.24836087226867676\n",
      "loss.item of the valid =  11 0.27920013666152954\n",
      "loss.item of the valid =  11 0.22901317477226257\n",
      "Iteration 12 \t\t Training Loss: 0.3201656639575958 \t\t Validation Loss: 0.2462565302848816\n",
      "loss.item of the train =  12 0.3193737268447876\n",
      "loss.item of the valid =  12 0.22721947729587555\n",
      "loss.item of the valid =  12 0.24593517184257507\n",
      "loss.item of the valid =  12 0.27741527557373047\n",
      "loss.item of the valid =  12 0.315804660320282\n",
      "Iteration 13 \t\t Training Loss: 0.3193737268447876 \t\t Validation Loss: 0.26659364625811577\n",
      "loss.item of the train =  13 0.3194190561771393\n",
      "loss.item of the valid =  13 0.2268911749124527\n",
      "loss.item of the valid =  13 0.24562156200408936\n",
      "loss.item of the valid =  13 0.2773023843765259\n",
      "loss.item of the valid =  13 0.3244860768318176\n",
      "Iteration 14 \t\t Training Loss: 0.3194190561771393 \t\t Validation Loss: 0.2685752995312214\n",
      "loss.item of the train =  14 0.31987783312797546\n",
      "loss.item of the valid =  14 0.22737368941307068\n",
      "loss.item of the valid =  14 0.24727006256580353\n",
      "loss.item of the valid =  14 0.27863550186157227\n",
      "loss.item of the valid =  14 0.24680739641189575\n",
      "Iteration 15 \t\t Training Loss: 0.31987783312797546 \t\t Validation Loss: 0.25002166256308556\n",
      "loss.item of the train =  15 0.3174300193786621\n",
      "loss.item of the valid =  15 0.22687508165836334\n",
      "loss.item of the valid =  15 0.24646399915218353\n",
      "loss.item of the valid =  15 0.2780890464782715\n",
      "loss.item of the valid =  15 0.2758042514324188\n",
      "Iteration 16 \t\t Training Loss: 0.3174300193786621 \t\t Validation Loss: 0.2568080946803093\n",
      "loss.item of the train =  16 0.31833505630493164\n",
      "loss.item of the valid =  16 0.22708329558372498\n",
      "loss.item of the valid =  16 0.247348353266716\n",
      "loss.item of the valid =  16 0.27886295318603516\n",
      "loss.item of the valid =  16 0.22345712780952454\n",
      "Iteration 17 \t\t Training Loss: 0.31833505630493164 \t\t Validation Loss: 0.24418793246150017\n",
      "Validation Loss Decreased(0.980695--->0.976752)\n",
      "loss.item of the train =  17 0.31830552220344543\n",
      "loss.item of the valid =  17 0.22648876905441284\n",
      "loss.item of the valid =  17 0.2462310492992401\n",
      "loss.item of the valid =  17 0.27807047963142395\n",
      "loss.item of the valid =  17 0.26828911900520325\n",
      "Iteration 18 \t\t Training Loss: 0.31830552220344543 \t\t Validation Loss: 0.25476985424757004\n",
      "loss.item of the train =  18 0.3182983994483948\n",
      "loss.item of the valid =  18 0.22657909989356995\n",
      "loss.item of the valid =  18 0.24675621092319489\n",
      "loss.item of the valid =  18 0.2785499393939972\n",
      "loss.item of the valid =  18 0.23359841108322144\n",
      "Iteration 19 \t\t Training Loss: 0.3182983994483948 \t\t Validation Loss: 0.24637091532349586\n",
      "loss.item of the train =  19 0.31773459911346436\n",
      "loss.item of the valid =  19 0.22591690719127655\n",
      "loss.item of the valid =  19 0.24531574547290802\n",
      "loss.item of the valid =  19 0.27754566073417664\n",
      "loss.item of the valid =  19 0.2939120829105377\n",
      "Iteration 20 \t\t Training Loss: 0.31773459911346436 \t\t Validation Loss: 0.26067259907722473\n",
      "loss.item of the train =  20 0.3193605840206146\n",
      "loss.item of the valid =  20 0.22596237063407898\n",
      "loss.item of the valid =  20 0.24562329053878784\n",
      "loss.item of the valid =  20 0.277831107378006\n",
      "loss.item of the valid =  20 0.2751789093017578\n",
      "Iteration 21 \t\t Training Loss: 0.3193605840206146 \t\t Validation Loss: 0.25614891946315765\n",
      "loss.item of the train =  21 0.31876981258392334\n",
      "loss.item of the valid =  21 0.22658184170722961\n",
      "loss.item of the valid =  21 0.2472919225692749\n",
      "loss.item of the valid =  21 0.2791846692562103\n",
      "loss.item of the valid =  21 0.22866524755954742\n",
      "Iteration 22 \t\t Training Loss: 0.31876981258392334 \t\t Validation Loss: 0.24543092027306557\n",
      "loss.item of the train =  22 0.31954559683799744\n",
      "loss.item of the valid =  22 0.22652870416641235\n",
      "loss.item of the valid =  22 0.24719423055648804\n",
      "loss.item of the valid =  22 0.27915072441101074\n",
      "loss.item of the valid =  22 0.2274639755487442\n",
      "Iteration 23 \t\t Training Loss: 0.31954559683799744 \t\t Validation Loss: 0.24508440867066383\n",
      "loss.item of the train =  23 0.3194284737110138\n",
      "loss.item of the valid =  23 0.22589166462421417\n",
      "loss.item of the valid =  23 0.24554754793643951\n",
      "loss.item of the valid =  23 0.2779088020324707\n",
      "loss.item of the valid =  23 0.2728056311607361\n",
      "Iteration 24 \t\t Training Loss: 0.3194284737110138 \t\t Validation Loss: 0.2555384114384651\n",
      "loss.item of the train =  24 0.31878772377967834\n",
      "loss.item of the valid =  24 0.22587701678276062\n",
      "loss.item of the valid =  24 0.24542659521102905\n",
      "loss.item of the valid =  24 0.2778506875038147\n",
      "loss.item of the valid =  24 0.27920225262641907\n",
      "Iteration 25 \t\t Training Loss: 0.31878772377967834 \t\t Validation Loss: 0.25708913803100586\n",
      "loss.item of the train =  25 0.31901443004608154\n",
      "loss.item of the valid =  25 0.22641907632350922\n",
      "loss.item of the valid =  25 0.24671171605587006\n",
      "loss.item of the valid =  25 0.27885064482688904\n",
      "loss.item of the valid =  25 0.21953892707824707\n",
      "Iteration 26 \t\t Training Loss: 0.31901443004608154 \t\t Validation Loss: 0.24288009107112885\n",
      "Validation Loss Decreased(0.976752--->0.971520)\n",
      "loss.item of the train =  26 0.31837740540504456\n",
      "loss.item of the valid =  26 0.22629334032535553\n",
      "loss.item of the valid =  26 0.24624751508235931\n",
      "loss.item of the valid =  26 0.2785017788410187\n",
      "loss.item of the valid =  26 0.24487623572349548\n",
      "Iteration 27 \t\t Training Loss: 0.31837740540504456 \t\t Validation Loss: 0.24897971749305725\n",
      "loss.item of the train =  27 0.3178824186325073\n",
      "loss.item of the valid =  27 0.22667676210403442\n",
      "loss.item of the valid =  27 0.24706147611141205\n",
      "loss.item of the valid =  27 0.27915793657302856\n",
      "loss.item of the valid =  27 0.22198975086212158\n",
      "Iteration 28 \t\t Training Loss: 0.3178824186325073 \t\t Validation Loss: 0.24372148141264915\n",
      "loss.item of the train =  28 0.3189321756362915\n",
      "loss.item of the valid =  28 0.22644709050655365\n",
      "loss.item of the valid =  28 0.24631595611572266\n",
      "loss.item of the valid =  28 0.2785695493221283\n",
      "loss.item of the valid =  28 0.248700812458992\n",
      "Iteration 29 \t\t Training Loss: 0.3189321756362915 \t\t Validation Loss: 0.25000835210084915\n",
      "loss.item of the train =  29 0.3179597556591034\n",
      "loss.item of the valid =  29 0.22672083973884583\n",
      "loss.item of the valid =  29 0.2468433678150177\n",
      "loss.item of the valid =  29 0.2789880931377411\n",
      "loss.item of the valid =  29 0.22495338320732117\n",
      "Iteration 30 \t\t Training Loss: 0.3179597556591034 \t\t Validation Loss: 0.24437642097473145\n",
      "loss.item of the train =  30 0.3182224929332733\n",
      "loss.item of the valid =  30 0.2263737916946411\n",
      "loss.item of the valid =  30 0.24578039348125458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item of the valid =  30 0.2781673073768616\n",
      "loss.item of the valid =  30 0.28153467178344727\n",
      "Iteration 31 \t\t Training Loss: 0.3182224929332733 \t\t Validation Loss: 0.25796404108405113\n",
      "loss.item of the train =  31 0.31897082924842834\n",
      "loss.item of the valid =  31 0.22655630111694336\n",
      "loss.item of the valid =  31 0.24608729779720306\n",
      "loss.item of the valid =  31 0.2783959209918976\n",
      "loss.item of the valid =  31 0.2710650861263275\n",
      "Iteration 32 \t\t Training Loss: 0.31897082924842834 \t\t Validation Loss: 0.2555261515080929\n",
      "loss.item of the train =  32 0.3185766339302063\n",
      "loss.item of the valid =  32 0.22717776894569397\n",
      "loss.item of the valid =  32 0.2475024163722992\n",
      "loss.item of the valid =  32 0.2795235812664032\n",
      "loss.item of the valid =  32 0.2227213978767395\n",
      "Iteration 33 \t\t Training Loss: 0.3185766339302063 \t\t Validation Loss: 0.24423129111528397\n",
      "loss.item of the train =  33 0.3192358613014221\n",
      "loss.item of the valid =  33 0.22719456255435944\n",
      "loss.item of the valid =  33 0.24740086495876312\n",
      "loss.item of the valid =  33 0.2794313430786133\n",
      "loss.item of the valid =  33 0.22063681483268738\n",
      "Iteration 34 \t\t Training Loss: 0.3192358613014221 \t\t Validation Loss: 0.2436658963561058\n",
      "loss.item of the train =  34 0.3188934922218323\n",
      "loss.item of the valid =  34 0.2266702502965927\n",
      "loss.item of the valid =  34 0.24591754376888275\n",
      "loss.item of the valid =  34 0.2782458961009979\n",
      "loss.item of the valid =  34 0.2882296144962311\n",
      "Iteration 35 \t\t Training Loss: 0.3188934922218323 \t\t Validation Loss: 0.2597658261656761\n",
      "loss.item of the train =  35 0.3190246522426605\n",
      "loss.item of the valid =  35 0.22666147351264954\n",
      "loss.item of the valid =  35 0.24573351442813873\n",
      "loss.item of the valid =  35 0.2780955731868744\n",
      "loss.item of the valid =  35 0.2991213798522949\n",
      "Iteration 36 \t\t Training Loss: 0.3190246522426605 \t\t Validation Loss: 0.2624029852449894\n",
      "loss.item of the train =  36 0.3193592429161072\n",
      "loss.item of the valid =  36 0.22712478041648865\n",
      "loss.item of the valid =  36 0.24678465723991394\n",
      "loss.item of the valid =  36 0.2788862884044647\n",
      "loss.item of the valid =  36 0.2540929317474365\n",
      "Iteration 37 \t\t Training Loss: 0.3193592429161072 \t\t Validation Loss: 0.25172216445207596\n",
      "loss.item of the train =  37 0.31777793169021606\n",
      "loss.item of the valid =  37 0.22791551053524017\n",
      "loss.item of the valid =  37 0.24857839941978455\n",
      "loss.item of the valid =  37 0.28040409088134766\n",
      "loss.item of the valid =  37 0.23280221223831177\n",
      "Iteration 38 \t\t Training Loss: 0.31777793169021606 \t\t Validation Loss: 0.24742505326867104\n",
      "loss.item of the train =  38 0.32092252373695374\n",
      "loss.item of the valid =  38 0.22816792130470276\n",
      "loss.item of the valid =  38 0.24903443455696106\n",
      "loss.item of the valid =  38 0.2808082699775696\n",
      "loss.item of the valid =  38 0.2377602458000183\n",
      "Iteration 39 \t\t Training Loss: 0.32092252373695374 \t\t Validation Loss: 0.24894271790981293\n",
      "loss.item of the train =  39 0.32179829478263855\n",
      "loss.item of the valid =  39 0.22800695896148682\n",
      "loss.item of the valid =  39 0.24852608144283295\n",
      "loss.item of the valid =  39 0.28030332922935486\n",
      "loss.item of the valid =  39 0.23103073239326477\n",
      "Iteration 40 \t\t Training Loss: 0.32179829478263855 \t\t Validation Loss: 0.24696677550673485\n",
      "loss.item of the train =  40 0.32057079672813416\n",
      "loss.item of the valid =  40 0.22742240130901337\n",
      "loss.item of the valid =  40 0.24693439900875092\n",
      "loss.item of the valid =  40 0.2788957953453064\n",
      "loss.item of the valid =  40 0.25863057374954224\n",
      "Iteration 41 \t\t Training Loss: 0.32057079672813416 \t\t Validation Loss: 0.25297079235315323\n",
      "loss.item of the train =  41 0.31772133708000183\n",
      "loss.item of the valid =  41 0.22728177905082703\n",
      "loss.item of the valid =  41 0.24638615548610687\n",
      "loss.item of the valid =  41 0.2784136235713959\n",
      "loss.item of the valid =  41 0.2865349352359772\n",
      "Iteration 42 \t\t Training Loss: 0.31772133708000183 \t\t Validation Loss: 0.25965412333607674\n",
      "loss.item of the train =  42 0.3186071515083313\n",
      "loss.item of the valid =  42 0.2275809943675995\n",
      "loss.item of the valid =  42 0.246968612074852\n",
      "loss.item of the valid =  42 0.2788239121437073\n",
      "loss.item of the valid =  42 0.26187676191329956\n",
      "Iteration 43 \t\t Training Loss: 0.3186071515083313 \t\t Validation Loss: 0.2538125701248646\n",
      "loss.item of the train =  43 0.3177288770675659\n",
      "loss.item of the valid =  43 0.2282191514968872\n",
      "loss.item of the valid =  43 0.24837395548820496\n",
      "loss.item of the valid =  43 0.2799762785434723\n",
      "loss.item of the valid =  43 0.22718586027622223\n",
      "Iteration 44 \t\t Training Loss: 0.3177288770675659 \t\t Validation Loss: 0.24593881145119667\n",
      "loss.item of the train =  44 0.3198437988758087\n",
      "loss.item of the valid =  44 0.2283513844013214\n",
      "loss.item of the valid =  44 0.24852007627487183\n",
      "loss.item of the valid =  44 0.2800482213497162\n",
      "loss.item of the valid =  44 0.22877690196037292\n",
      "Iteration 45 \t\t Training Loss: 0.3198437988758087 \t\t Validation Loss: 0.2464241459965706\n",
      "loss.item of the train =  45 0.32011544704437256\n",
      "loss.item of the valid =  45 0.2280607670545578\n",
      "loss.item of the valid =  45 0.2476142793893814\n",
      "loss.item of the valid =  45 0.27916213870048523\n",
      "loss.item of the valid =  45 0.2318802773952484\n",
      "Iteration 46 \t\t Training Loss: 0.32011544704437256 \t\t Validation Loss: 0.2466793656349182\n",
      "loss.item of the train =  46 0.3182285726070404\n",
      "loss.item of the valid =  46 0.22729778289794922\n",
      "loss.item of the valid =  46 0.24532470107078552\n",
      "loss.item of the valid =  46 0.2772185802459717\n",
      "loss.item of the valid =  46 0.3284408450126648\n",
      "Iteration 47 \t\t Training Loss: 0.3182285726070404 \t\t Validation Loss: 0.2695704773068428\n",
      "loss.item of the train =  47 0.319974809885025\n",
      "loss.item of the valid =  47 0.2270098626613617\n",
      "loss.item of the valid =  47 0.24413026869297028\n",
      "loss.item of the valid =  47 0.27619361877441406\n",
      "loss.item of the valid =  47 0.3630579113960266\n",
      "Iteration 48 \t\t Training Loss: 0.319974809885025 \t\t Validation Loss: 0.27759791538119316\n",
      "loss.item of the train =  48 0.3213593661785126\n",
      "loss.item of the valid =  48 0.22722607851028442\n",
      "loss.item of the valid =  48 0.24431860446929932\n",
      "loss.item of the valid =  48 0.27610301971435547\n",
      "loss.item of the valid =  48 0.3512561321258545\n",
      "Iteration 49 \t\t Training Loss: 0.3213593661785126 \t\t Validation Loss: 0.2747259587049484\n",
      "loss.item of the train =  49 0.3208414614200592\n",
      "loss.item of the valid =  49 0.2278507500886917\n",
      "loss.item of the valid =  49 0.2456328123807907\n",
      "loss.item of the valid =  49 0.2768317461013794\n",
      "loss.item of the valid =  49 0.29849934577941895\n",
      "Iteration 50 \t\t Training Loss: 0.3208414614200592 \t\t Validation Loss: 0.2622036635875702\n",
      "loss.item of the train =  50 0.31886547803878784\n",
      "loss.item of the valid =  50 0.22872471809387207\n",
      "loss.item of the valid =  50 0.2475459724664688\n",
      "loss.item of the valid =  50 0.2781880497932434\n",
      "loss.item of the valid =  50 0.22237738966941833\n",
      "Iteration 51 \t\t Training Loss: 0.31886547803878784 \t\t Validation Loss: 0.24420903250575066\n",
      "loss.item of the train =  51 0.31934210658073425\n",
      "loss.item of the valid =  51 0.22908122837543488\n",
      "loss.item of the valid =  51 0.24799136817455292\n",
      "loss.item of the valid =  51 0.27831709384918213\n",
      "loss.item of the valid =  51 0.229214608669281\n",
      "Iteration 52 \t\t Training Loss: 0.31934210658073425 \t\t Validation Loss: 0.24615107476711273\n",
      "loss.item of the train =  52 0.320623517036438\n",
      "loss.item of the valid =  52 0.22906802594661713\n",
      "loss.item of the valid =  52 0.24736902117729187\n",
      "loss.item of the valid =  52 0.27738234400749207\n",
      "loss.item of the valid =  52 0.22517560422420502\n",
      "Iteration 53 \t\t Training Loss: 0.320623517036438 \t\t Validation Loss: 0.24474874883890152\n",
      "loss.item of the train =  53 0.3200726807117462\n",
      "loss.item of the valid =  53 0.22871384024620056\n",
      "loss.item of the valid =  53 0.24560734629631042\n",
      "loss.item of the valid =  53 0.2753598093986511\n",
      "loss.item of the valid =  53 0.24330513179302216\n",
      "Iteration 54 \t\t Training Loss: 0.3200726807117462 \t\t Validation Loss: 0.24824653193354607\n",
      "loss.item of the train =  54 0.3178444504737854\n",
      "loss.item of the valid =  54 0.22794963419437408\n",
      "loss.item of the valid =  54 0.24224604666233063\n",
      "loss.item of the valid =  54 0.27210676670074463\n",
      "loss.item of the valid =  54 0.3325873613357544\n",
      "Iteration 55 \t\t Training Loss: 0.3178444504737854 \t\t Validation Loss: 0.26872245222330093\n",
      "loss.item of the train =  55 0.3201787769794464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item of the valid =  55 0.2275598794221878\n",
      "loss.item of the valid =  55 0.23963981866836548\n",
      "loss.item of the valid =  55 0.2694678008556366\n",
      "loss.item of the valid =  55 0.3716253638267517\n",
      "Iteration 56 \t\t Training Loss: 0.3201787769794464 \t\t Validation Loss: 0.2770732156932354\n",
      "loss.item of the train =  56 0.3214499056339264\n",
      "loss.item of the valid =  56 0.2275913655757904\n",
      "loss.item of the valid =  56 0.2381618618965149\n",
      "loss.item of the valid =  56 0.26744773983955383\n",
      "loss.item of the valid =  56 0.37862592935562134\n",
      "Iteration 57 \t\t Training Loss: 0.3214499056339264 \t\t Validation Loss: 0.2779567241668701\n",
      "loss.item of the train =  57 0.32157212495803833\n",
      "loss.item of the valid =  57 0.22800977528095245\n",
      "loss.item of the valid =  57 0.23780497908592224\n",
      "loss.item of the valid =  57 0.2661355435848236\n",
      "loss.item of the valid =  57 0.36504051089286804\n",
      "Iteration 58 \t\t Training Loss: 0.32157212495803833 \t\t Validation Loss: 0.2742477022111416\n",
      "loss.item of the train =  58 0.32103368639945984\n",
      "loss.item of the valid =  58 0.22872857749462128\n",
      "loss.item of the valid =  58 0.23831063508987427\n",
      "loss.item of the valid =  58 0.2656342387199402\n",
      "loss.item of the valid =  58 0.33869099617004395\n",
      "Iteration 59 \t\t Training Loss: 0.32103368639945984 \t\t Validation Loss: 0.2678411118686199\n",
      "loss.item of the train =  59 0.3202856481075287\n",
      "loss.item of the valid =  59 0.22964414954185486\n",
      "loss.item of the valid =  59 0.23936481773853302\n",
      "loss.item of the valid =  59 0.26540064811706543\n",
      "loss.item of the valid =  59 0.30517762899398804\n",
      "Iteration 60 \t\t Training Loss: 0.3202856481075287 \t\t Validation Loss: 0.25989681109786034\n",
      "loss.item of the train =  60 0.31962570548057556\n",
      "loss.item of the valid =  60 0.23081886768341064\n",
      "loss.item of the valid =  60 0.2407103031873703\n",
      "loss.item of the valid =  60 0.2653408646583557\n",
      "loss.item of the valid =  60 0.26848119497299194\n",
      "Iteration 61 \t\t Training Loss: 0.31962570548057556 \t\t Validation Loss: 0.25133780762553215\n",
      "loss.item of the train =  61 0.3194776177406311\n",
      "loss.item of the valid =  61 0.2326781451702118\n",
      "loss.item of the valid =  61 0.24211063981056213\n",
      "loss.item of the valid =  61 0.26532378792762756\n",
      "loss.item of the valid =  61 0.23316702246665955\n",
      "Iteration 62 \t\t Training Loss: 0.3194776177406311 \t\t Validation Loss: 0.24331989884376526\n",
      "loss.item of the train =  62 0.3198852837085724\n",
      "loss.item of the valid =  62 0.2343989610671997\n",
      "loss.item of the valid =  62 0.24349121749401093\n",
      "loss.item of the valid =  62 0.26533347368240356\n",
      "loss.item of the valid =  62 0.2376280575990677\n",
      "Iteration 63 \t\t Training Loss: 0.3198852837085724 \t\t Validation Loss: 0.24521292746067047\n",
      "loss.item of the train =  63 0.32094016671180725\n",
      "loss.item of the valid =  63 0.2356656789779663\n",
      "loss.item of the valid =  63 0.24411723017692566\n",
      "loss.item of the valid =  63 0.2648737132549286\n",
      "loss.item of the valid =  63 0.24192309379577637\n",
      "Iteration 64 \t\t Training Loss: 0.32094016671180725 \t\t Validation Loss: 0.24664492905139923\n",
      "loss.item of the train =  64 0.3215707540512085\n",
      "loss.item of the valid =  64 0.23657727241516113\n",
      "loss.item of the valid =  64 0.2441776841878891\n",
      "loss.item of the valid =  64 0.26406174898147583\n",
      "loss.item of the valid =  64 0.24521803855895996\n",
      "Iteration 65 \t\t Training Loss: 0.3215707540512085 \t\t Validation Loss: 0.2475086860358715\n",
      "loss.item of the train =  65 0.3221554756164551\n",
      "loss.item of the valid =  65 0.23740878701210022\n",
      "loss.item of the valid =  65 0.2443217784166336\n",
      "loss.item of the valid =  65 0.26336735486984253\n",
      "loss.item of the valid =  65 0.24852131307125092\n",
      "Iteration 66 \t\t Training Loss: 0.3221554756164551 \t\t Validation Loss: 0.24840480834245682\n",
      "loss.item of the train =  66 0.32295170426368713\n",
      "loss.item of the valid =  66 0.23813682794570923\n",
      "loss.item of the valid =  66 0.2444891780614853\n",
      "loss.item of the valid =  66 0.26276522874832153\n",
      "loss.item of the valid =  66 0.25163689255714417\n",
      "Iteration 67 \t\t Training Loss: 0.32295170426368713 \t\t Validation Loss: 0.24925703182816505\n",
      "loss.item of the train =  67 0.3236512541770935\n",
      "loss.item of the valid =  67 0.23874662816524506\n",
      "loss.item of the valid =  67 0.24463093280792236\n",
      "loss.item of the valid =  67 0.26223400235176086\n",
      "loss.item of the valid =  67 0.25446850061416626\n",
      "Iteration 68 \t\t Training Loss: 0.3236512541770935 \t\t Validation Loss: 0.25002001598477364\n",
      "loss.item of the train =  68 0.3242512345314026\n",
      "loss.item of the valid =  68 0.23923121392726898\n",
      "loss.item of the valid =  68 0.24470876157283783\n",
      "loss.item of the valid =  68 0.26175472140312195\n",
      "loss.item of the valid =  68 0.25697678327560425\n",
      "Iteration 69 \t\t Training Loss: 0.3242512345314026 \t\t Validation Loss: 0.25066787004470825\n",
      "loss.item of the train =  69 0.324748158454895\n",
      "loss.item of the valid =  69 0.23958130180835724\n",
      "loss.item of the valid =  69 0.2446913719177246\n",
      "loss.item of the valid =  69 0.26131075620651245\n",
      "loss.item of the valid =  69 0.26607033610343933\n",
      "Iteration 70 \t\t Training Loss: 0.324748158454895 \t\t Validation Loss: 0.2529134415090084\n",
      "loss.item of the train =  70 0.32514697313308716\n",
      "loss.item of the valid =  70 0.23979812860488892\n",
      "loss.item of the valid =  70 0.24456939101219177\n",
      "loss.item of the valid =  70 0.26089727878570557\n",
      "loss.item of the valid =  70 0.2750594913959503\n",
      "Iteration 71 \t\t Training Loss: 0.32514697313308716 \t\t Validation Loss: 0.25508107244968414\n",
      "loss.item of the train =  71 0.3254511058330536\n",
      "loss.item of the valid =  71 0.23987554013729095\n",
      "loss.item of the valid =  71 0.24432288110256195\n",
      "loss.item of the valid =  71 0.26050370931625366\n",
      "loss.item of the valid =  71 0.28386256098747253\n",
      "Iteration 72 \t\t Training Loss: 0.3254511058330536 \t\t Validation Loss: 0.2571411728858948\n",
      "loss.item of the train =  72 0.3256476819515228\n",
      "loss.item of the valid =  72 0.2398061752319336\n",
      "loss.item of the valid =  72 0.24393358826637268\n",
      "loss.item of the valid =  72 0.2601235806941986\n",
      "loss.item of the valid =  72 0.29264265298843384\n",
      "Iteration 73 \t\t Training Loss: 0.3256476819515228 \t\t Validation Loss: 0.2591264992952347\n",
      "loss.item of the train =  73 0.3257337808609009\n",
      "loss.item of the valid =  73 0.23957732319831848\n",
      "loss.item of the valid =  73 0.24338401854038239\n",
      "loss.item of the valid =  73 0.25975319743156433\n",
      "loss.item of the valid =  73 0.30153417587280273\n",
      "Iteration 74 \t\t Training Loss: 0.3257337808609009 \t\t Validation Loss: 0.261062178760767\n",
      "loss.item of the train =  74 0.3257046043872833\n",
      "loss.item of the valid =  74 0.23917041718959808\n",
      "loss.item of the valid =  74 0.24265694618225098\n",
      "loss.item of the valid =  74 0.2593926787376404\n",
      "loss.item of the valid =  74 0.31067216396331787\n",
      "Iteration 75 \t\t Training Loss: 0.3257046043872833 \t\t Validation Loss: 0.26297305151820183\n",
      "loss.item of the train =  75 0.32555246353149414\n",
      "loss.item of the valid =  75 0.23855960369110107\n",
      "loss.item of the valid =  75 0.24173405766487122\n",
      "loss.item of the valid =  75 0.25904756784439087\n",
      "loss.item of the valid =  75 0.32022613286972046\n",
      "Iteration 76 \t\t Training Loss: 0.32555246353149414 \t\t Validation Loss: 0.2648918405175209\n",
      "loss.item of the train =  76 0.32526686787605286\n",
      "loss.item of the valid =  76 0.2377094179391861\n",
      "loss.item of the valid =  76 0.24060092866420746\n",
      "loss.item of the valid =  76 0.2587297260761261\n",
      "loss.item of the valid =  76 0.3304358720779419\n",
      "Iteration 77 \t\t Training Loss: 0.32526686787605286 \t\t Validation Loss: 0.2668689861893654\n",
      "loss.item of the train =  77 0.32483547925949097\n",
      "loss.item of the valid =  77 0.23657344281673431\n",
      "loss.item of the valid =  77 0.23925110697746277\n",
      "loss.item of the valid =  77 0.25846004486083984\n",
      "loss.item of the valid =  77 0.3416731059551239\n",
      "Iteration 78 \t\t Training Loss: 0.32483547925949097 \t\t Validation Loss: 0.2689894251525402\n",
      "loss.item of the train =  78 0.32424649596214294\n",
      "loss.item of the valid =  78 0.23509597778320312\n",
      "loss.item of the valid =  78 0.2377021163702011\n",
      "loss.item of the valid =  78 0.2582705616950989\n",
      "loss.item of the valid =  78 0.3545176684856415\n",
      "Iteration 79 \t\t Training Loss: 0.32424649596214294 \t\t Validation Loss: 0.27139658108353615\n",
      "loss.item of the train =  79 0.3234923779964447\n",
      "loss.item of the valid =  79 0.23321588337421417\n",
      "loss.item of the valid =  79 0.23602493107318878\n",
      "loss.item of the valid =  79 0.2582116425037384\n",
      "loss.item of the valid =  79 0.36984342336654663\n",
      "Iteration 80 \t\t Training Loss: 0.3234923779964447 \t\t Validation Loss: 0.274323970079422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item of the train =  80 0.3225846588611603\n",
      "loss.item of the valid =  80 0.2310691624879837\n",
      "loss.item of the valid =  80 0.23435743153095245\n",
      "loss.item of the valid =  80 0.2582382261753082\n",
      "loss.item of the valid =  80 0.3881944417953491\n",
      "Iteration 81 \t\t Training Loss: 0.3225846588611603 \t\t Validation Loss: 0.2779648154973984\n",
      "loss.item of the train =  81 0.32179051637649536\n",
      "loss.item of the valid =  81 0.23001383244991302\n",
      "loss.item of the valid =  81 0.2329050600528717\n",
      "loss.item of the valid =  81 0.258402943611145\n",
      "loss.item of the valid =  81 0.4097956717014313\n",
      "Iteration 82 \t\t Training Loss: 0.32179051637649536 \t\t Validation Loss: 0.28277937695384026\n",
      "loss.item of the train =  82 0.3219616115093231\n",
      "loss.item of the valid =  82 0.22963733971118927\n",
      "loss.item of the valid =  82 0.23192062973976135\n",
      "loss.item of the valid =  82 0.2584697902202606\n",
      "loss.item of the valid =  82 0.4272352457046509\n",
      "Iteration 83 \t\t Training Loss: 0.3219616115093231 \t\t Validation Loss: 0.28681575134396553\n",
      "loss.item of the train =  83 0.32249921560287476\n",
      "loss.item of the valid =  83 0.2294631451368332\n",
      "loss.item of the valid =  83 0.23137341439723969\n",
      "loss.item of the valid =  83 0.2584787607192993\n",
      "loss.item of the valid =  83 0.4376275837421417\n",
      "Iteration 84 \t\t Training Loss: 0.32249921560287476 \t\t Validation Loss: 0.2892357259988785\n",
      "loss.item of the train =  84 0.3228878378868103\n",
      "loss.item of the valid =  84 0.2294262945652008\n",
      "loss.item of the valid =  84 0.2312469780445099\n",
      "loss.item of the valid =  84 0.2584729790687561\n",
      "loss.item of the valid =  84 0.4399111270904541\n",
      "Iteration 85 \t\t Training Loss: 0.3228878378868103 \t\t Validation Loss: 0.2897643446922302\n",
      "loss.item of the train =  85 0.32297781109809875\n",
      "loss.item of the valid =  85 0.22950632870197296\n",
      "loss.item of the valid =  85 0.2315179705619812\n",
      "loss.item of the valid =  85 0.25846973061561584\n",
      "loss.item of the valid =  85 0.4345872700214386\n",
      "Iteration 86 \t\t Training Loss: 0.32297781109809875 \t\t Validation Loss: 0.28852032497525215\n",
      "loss.item of the train =  86 0.32276615500450134\n",
      "loss.item of the valid =  86 0.22971339523792267\n",
      "loss.item of the valid =  86 0.2321353703737259\n",
      "loss.item of the valid =  86 0.25845974683761597\n",
      "loss.item of the valid =  86 0.4232783317565918\n",
      "Iteration 87 \t\t Training Loss: 0.32276615500450134 \t\t Validation Loss: 0.2858967110514641\n",
      "loss.item of the train =  87 0.3223639130592346\n",
      "loss.item of the valid =  87 0.23006200790405273\n",
      "loss.item of the valid =  87 0.2330278903245926\n",
      "loss.item of the valid =  87 0.2584194242954254\n",
      "loss.item of the valid =  87 0.40829795598983765\n",
      "Iteration 88 \t\t Training Loss: 0.3223639130592346 \t\t Validation Loss: 0.2824518196284771\n",
      "loss.item of the train =  88 0.3219287693500519\n",
      "loss.item of the valid =  88 0.23055347800254822\n",
      "loss.item of the valid =  88 0.23411498963832855\n",
      "loss.item of the valid =  88 0.2583291232585907\n",
      "loss.item of the valid =  88 0.3921605944633484\n",
      "Iteration 89 \t\t Training Loss: 0.3219287693500519 \t\t Validation Loss: 0.27878954634070396\n",
      "loss.item of the train =  89 0.3216020166873932\n",
      "loss.item of the valid =  89 0.23238420486450195\n",
      "loss.item of the valid =  89 0.23531492054462433\n",
      "loss.item of the valid =  89 0.25818514823913574\n",
      "loss.item of the valid =  89 0.37710750102996826\n",
      "Iteration 90 \t\t Training Loss: 0.3216020166873932 \t\t Validation Loss: 0.27574794366955757\n",
      "loss.item of the train =  90 0.32226359844207764\n",
      "loss.item of the valid =  90 0.23367805778980255\n",
      "loss.item of the valid =  90 0.23630960285663605\n",
      "loss.item of the valid =  90 0.2581344544887543\n",
      "loss.item of the valid =  90 0.3669840097427368\n",
      "Iteration 91 \t\t Training Loss: 0.32226359844207764 \t\t Validation Loss: 0.2737765312194824\n",
      "loss.item of the train =  91 0.32282111048698425\n",
      "loss.item of the valid =  91 0.23436643183231354\n",
      "loss.item of the valid =  91 0.23700667917728424\n",
      "loss.item of the valid =  91 0.25821202993392944\n",
      "loss.item of the valid =  91 0.36072367429733276\n",
      "Iteration 92 \t\t Training Loss: 0.32282111048698425 \t\t Validation Loss: 0.272577203810215\n",
      "loss.item of the train =  92 0.3231303095817566\n",
      "loss.item of the valid =  92 0.2345832735300064\n",
      "loss.item of the valid =  92 0.23740217089653015\n",
      "loss.item of the valid =  92 0.2583622932434082\n",
      "loss.item of the valid =  92 0.35725265741348267\n",
      "Iteration 93 \t\t Training Loss: 0.3231303095817566 \t\t Validation Loss: 0.27190009877085686\n",
      "loss.item of the train =  93 0.323182612657547\n",
      "loss.item of the valid =  93 0.23439690470695496\n",
      "loss.item of the valid =  93 0.23753675818443298\n",
      "loss.item of the valid =  93 0.25857383012771606\n",
      "loss.item of the valid =  93 0.3560599982738495\n",
      "Iteration 94 \t\t Training Loss: 0.323182612657547 \t\t Validation Loss: 0.2716418728232384\n",
      "loss.item of the train =  94 0.32300299406051636\n",
      "loss.item of the valid =  94 0.23383189737796783\n",
      "loss.item of the valid =  94 0.23747539520263672\n",
      "loss.item of the valid =  94 0.258865088224411\n",
      "loss.item of the valid =  94 0.3569137752056122\n",
      "Iteration 95 \t\t Training Loss: 0.32300299406051636 \t\t Validation Loss: 0.27177153900265694\n",
      "loss.item of the train =  95 0.32264190912246704\n",
      "loss.item of the valid =  95 0.23305729031562805\n",
      "loss.item of the valid =  95 0.23729974031448364\n",
      "loss.item of the valid =  95 0.259173721075058\n",
      "loss.item of the valid =  95 0.35959434509277344\n",
      "Iteration 96 \t\t Training Loss: 0.32264190912246704 \t\t Validation Loss: 0.2722812741994858\n",
      "loss.item of the train =  96 0.32220566272735596\n",
      "loss.item of the valid =  96 0.2320786416530609\n",
      "loss.item of the valid =  96 0.23710055649280548\n",
      "loss.item of the valid =  96 0.2595379650592804\n",
      "loss.item of the valid =  96 0.36391979455947876\n",
      "Iteration 97 \t\t Training Loss: 0.32220566272735596 \t\t Validation Loss: 0.2731592394411564\n",
      "loss.item of the train =  97 0.3216843605041504\n",
      "loss.item of the valid =  97 0.23148690164089203\n",
      "loss.item of the valid =  97 0.23699861764907837\n",
      "loss.item of the valid =  97 0.26000285148620605\n",
      "loss.item of the valid =  97 0.369629830121994\n",
      "Iteration 98 \t\t Training Loss: 0.3216843605041504 \t\t Validation Loss: 0.2745295502245426\n",
      "loss.item of the train =  98 0.3214642405509949\n",
      "loss.item of the valid =  98 0.2314673811197281\n",
      "loss.item of the valid =  98 0.2371421754360199\n",
      "loss.item of the valid =  98 0.2603724002838135\n",
      "loss.item of the valid =  98 0.37214037775993347\n",
      "Iteration 99 \t\t Training Loss: 0.3214642405509949 \t\t Validation Loss: 0.27528058364987373\n",
      "loss.item of the train =  99 0.32152414321899414\n",
      "ged= tensor([ 0.0406,  0.0041,  0.0066,  0.0152,  0.0669,  0.0662,  0.0669,  0.1032,\n",
      "         0.0082,  0.0082,  0.0491, -0.1029, -0.5115, -0.1885, -0.8746, -0.5224,\n",
      "         0.0783,  0.0784,  0.0797,  0.1683,  0.1693,  0.1683,  0.2685,  0.0825,\n",
      "         0.0825,  0.0421, -0.2842, -0.8470, -0.0159, -0.4465, -0.1781,  0.0544,\n",
      "         0.0511,  0.0958,  0.0933,  0.0958,  0.1894,  0.0533,  0.0533,  0.0015,\n",
      "        -0.1853, -0.6774, -0.0568, -0.5411, -0.3570,  0.0556,  0.1011,  0.0990,\n",
      "         0.1011,  0.1969,  0.0570,  0.0570,  0.0051, -0.1874, -0.6886, -0.0580,\n",
      "        -0.5609, -0.3623,  0.0977,  0.0937,  0.0977,  0.1884,  0.0548,  0.0548,\n",
      "         0.0000, -0.1838, -0.6867, -0.0468, -0.5382, -0.3430,  0.0669,  0.0647,\n",
      "         0.1002,  0.0065,  0.0065,  0.0489, -0.1014, -0.5095, -0.1871, -0.8821,\n",
      "        -0.5118,  0.0669,  0.1032,  0.0082,  0.0082,  0.0491, -0.1029, -0.5115,\n",
      "        -0.1885, -0.8746, -0.5224,  0.1002,  0.0065,  0.0065,  0.0489, -0.1014,\n",
      "        -0.5095, -0.1871, -0.8821, -0.5118,  0.0622,  0.0622,  0.1139, -0.0708,\n",
      "        -0.3513, -0.3325, -1.0000, -0.8592,  0.0560,  0.0063, -0.1890, -0.6806,\n",
      "        -0.0603, -0.5444, -0.3609,  0.0063, -0.1890, -0.6806, -0.0603, -0.5444,\n",
      "        -0.3609, -0.2864, -0.8583, -0.0160, -0.4277, -0.1863],\n",
      "       grad_fn=<MulBackward0>)\n",
      "Distances:  tensor([0.0406, 0.0041, 0.0066, 0.0152, 0.0669, 0.0662, 0.0669, 0.1032, 0.0082,\n",
      "        0.0082, 0.0491, 0.1029, 0.5115, 0.1885, 0.8746, 0.5224, 0.0783, 0.0784,\n",
      "        0.0797, 0.1683, 0.1693, 0.1683, 0.2685, 0.0825, 0.0825, 0.0421, 0.2842,\n",
      "        0.8470, 0.0159, 0.4465, 0.1781, 0.0544, 0.0511, 0.0958, 0.0933, 0.0958,\n",
      "        0.1894, 0.0533, 0.0533, 0.0015, 0.1853, 0.6774, 0.0568, 0.5411, 0.3570,\n",
      "        0.0556, 0.1011, 0.0990, 0.1011, 0.1969, 0.0570, 0.0570, 0.0051, 0.1874,\n",
      "        0.6886, 0.0580, 0.5609, 0.3623, 0.0977, 0.0937, 0.0977, 0.1884, 0.0548,\n",
      "        0.0548, 0.0000, 0.1838, 0.6867, 0.0468, 0.5382, 0.3430, 0.0669, 0.0647,\n",
      "        0.1002, 0.0065, 0.0065, 0.0489, 0.1014, 0.5095, 0.1871, 0.8821, 0.5118,\n",
      "        0.0669, 0.1032, 0.0082, 0.0082, 0.0491, 0.1029, 0.5115, 0.1885, 0.8746,\n",
      "        0.5224, 0.1002, 0.0065, 0.0065, 0.0489, 0.1014, 0.5095, 0.1871, 0.8821,\n",
      "        0.5118, 0.0622, 0.0622, 0.1139, 0.0708, 0.3513, 0.3325, 1.0000, 0.8592,\n",
      "        0.0560, 0.0063, 0.1890, 0.6806, 0.0603, 0.5444, 0.3609, 0.0063, 0.1890,\n",
      "        0.6806, 0.0603, 0.5444, 0.3609, 0.2864, 0.8583, 0.0160, 0.4277, 0.1863],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Loss Triangular: 0.0\n",
      "node_costs :\n",
      "tensor([[0.0000e+00, 2.1359e-06, 7.3612e-06, 2.0559e-06, 7.0649e-06, 5.8629e-06,\n",
      "         1.9198e-06, 9.6594e-06, 5.3908e-06, 1.0073e-05, 4.1178e-06, 9.1972e-06,\n",
      "         2.6447e-06, 1.0950e-05, 5.5030e-06, 6.6150e-07, 8.1388e-07, 1.7400e-05,\n",
      "         1.9191e-05],\n",
      "        [2.1359e-06, 0.0000e+00, 1.6659e-05, 9.2701e-06, 1.4000e-05, 9.3111e-06,\n",
      "         1.8557e-05, 6.3797e-07, 1.7277e-05, 1.2725e-05, 1.5925e-06, 4.8194e-06,\n",
      "         1.8662e-05, 1.4128e-05, 1.2354e-06, 1.6668e-05, 9.4424e-08, 8.4456e-06,\n",
      "         1.0599e-05],\n",
      "        [7.3612e-06, 1.6659e-05, 0.0000e+00, 1.5845e-05, 7.5887e-07, 1.1694e-05,\n",
      "         9.9292e-06, 1.2412e-05, 1.5929e-05, 1.1187e-06, 1.0291e-05, 1.1553e-06,\n",
      "         3.1095e-06, 3.4248e-06, 4.7847e-06, 1.6870e-05, 1.0472e-05, 2.6052e-06,\n",
      "         1.9885e-05],\n",
      "        [2.0559e-06, 9.2701e-06, 1.5845e-05, 0.0000e+00, 1.1323e-05, 4.6387e-06,\n",
      "         5.3172e-06, 1.2131e-05, 6.2862e-06, 4.6894e-07, 1.9700e-05, 1.2908e-05,\n",
      "         6.3344e-06, 4.2094e-07, 2.1415e-06, 1.8281e-05, 1.5642e-05, 1.6522e-05,\n",
      "         1.5387e-05],\n",
      "        [7.0649e-06, 1.4000e-05, 7.5887e-07, 1.1323e-05, 0.0000e+00, 1.3941e-05,\n",
      "         5.5710e-06, 2.0748e-06, 3.4775e-06, 1.0288e-05, 1.2078e-05, 8.0647e-06,\n",
      "         3.2992e-07, 1.2012e-05, 1.9530e-05, 7.3957e-06, 3.3853e-06, 6.0796e-06,\n",
      "         8.9580e-06],\n",
      "        [5.8629e-06, 9.3111e-06, 1.1694e-05, 4.6387e-06, 1.3941e-05, 0.0000e+00,\n",
      "         1.6739e-06, 7.6432e-06, 1.5867e-05, 3.4408e-06, 1.6227e-05, 1.6382e-05,\n",
      "         8.0674e-06, 8.4532e-06, 1.8987e-06, 1.3220e-05, 1.4318e-05, 4.5328e-06,\n",
      "         3.4459e-06],\n",
      "        [1.9198e-06, 1.8557e-05, 9.9292e-06, 5.3172e-06, 5.5710e-06, 1.6739e-06,\n",
      "         0.0000e+00, 2.6419e-06, 8.5455e-06, 4.9598e-06, 6.7866e-07, 1.6847e-06,\n",
      "         2.4750e-06, 8.9560e-06, 1.0396e-05, 1.9170e-05, 4.0086e-07, 1.0009e-05,\n",
      "         9.7358e-06],\n",
      "        [9.6594e-06, 6.3797e-07, 1.2412e-05, 1.2131e-05, 2.0748e-06, 7.6432e-06,\n",
      "         2.6419e-06, 0.0000e+00, 6.9182e-06, 6.3150e-06, 2.3315e-06, 1.4541e-06,\n",
      "         1.9552e-06, 1.4644e-07, 6.6360e-07, 2.8267e-06, 1.5275e-05, 1.2698e-06,\n",
      "         8.8660e-06],\n",
      "        [5.3908e-06, 1.7277e-05, 1.5929e-05, 6.2862e-06, 3.4775e-06, 1.5867e-05,\n",
      "         8.5455e-06, 6.9182e-06, 0.0000e+00, 4.6098e-06, 1.3381e-05, 1.8225e-05,\n",
      "         1.8837e-05, 2.0988e-06, 1.9369e-05, 1.5007e-06, 7.0790e-06, 9.9587e-06,\n",
      "         7.5474e-06],\n",
      "        [1.0073e-05, 1.2725e-05, 1.1187e-06, 4.6894e-07, 1.0288e-05, 3.4408e-06,\n",
      "         4.9598e-06, 6.3150e-06, 4.6098e-06, 0.0000e+00, 9.4529e-06, 6.7409e-06,\n",
      "         1.3248e-05, 1.6511e-05, 8.2549e-06, 1.8052e-06, 1.2970e-05, 1.3548e-06,\n",
      "         2.0308e-06],\n",
      "        [4.1178e-06, 1.5925e-06, 1.0291e-05, 1.9700e-05, 1.2078e-05, 1.6227e-05,\n",
      "         6.7866e-07, 2.3315e-06, 1.3381e-05, 9.4529e-06, 0.0000e+00, 1.4720e-06,\n",
      "         1.9390e-07, 1.5579e-05, 1.9503e-05, 7.9184e-06, 1.9298e-05, 1.4463e-05,\n",
      "         1.6286e-05],\n",
      "        [9.1972e-06, 4.8194e-06, 1.1553e-06, 1.2908e-05, 8.0647e-06, 1.6382e-05,\n",
      "         1.6847e-06, 1.4541e-06, 1.8225e-05, 6.7409e-06, 1.4720e-06, 0.0000e+00,\n",
      "         1.4423e-05, 8.6065e-06, 8.4323e-06, 1.4384e-06, 1.3764e-05, 1.3515e-05,\n",
      "         1.9054e-05],\n",
      "        [2.6447e-06, 1.8662e-05, 3.1095e-06, 6.3344e-06, 3.2992e-07, 8.0674e-06,\n",
      "         2.4750e-06, 1.9552e-06, 1.8837e-05, 1.3248e-05, 1.9390e-07, 1.4423e-05,\n",
      "         0.0000e+00, 2.6530e-06, 1.4673e-06, 1.4014e-05, 1.8137e-05, 7.6271e-06,\n",
      "         2.5756e-07],\n",
      "        [1.0950e-05, 1.4128e-05, 3.4248e-06, 4.2094e-07, 1.2012e-05, 8.4532e-06,\n",
      "         8.9560e-06, 1.4644e-07, 2.0988e-06, 1.6511e-05, 1.5579e-05, 8.6065e-06,\n",
      "         2.6530e-06, 0.0000e+00, 6.4613e-06, 1.5455e-05, 4.7782e-06, 4.5623e-06,\n",
      "         2.6299e-06],\n",
      "        [5.5030e-06, 1.2354e-06, 4.7847e-06, 2.1415e-06, 1.9530e-05, 1.8987e-06,\n",
      "         1.0396e-05, 6.6360e-07, 1.9369e-05, 8.2549e-06, 1.9503e-05, 8.4323e-06,\n",
      "         1.4673e-06, 6.4613e-06, 0.0000e+00, 1.7392e-05, 2.4153e-06, 6.6480e-06,\n",
      "         3.1940e-06],\n",
      "        [6.6150e-07, 1.6668e-05, 1.6870e-05, 1.8281e-05, 7.3957e-06, 1.3220e-05,\n",
      "         1.9170e-05, 2.8267e-06, 1.5007e-06, 1.8052e-06, 7.9184e-06, 1.4384e-06,\n",
      "         1.4014e-05, 1.5455e-05, 1.7392e-05, 0.0000e+00, 5.4317e-06, 1.8065e-05,\n",
      "         1.7480e-06],\n",
      "        [8.1388e-07, 9.4424e-08, 1.0472e-05, 1.5642e-05, 3.3853e-06, 1.4318e-05,\n",
      "         4.0086e-07, 1.5275e-05, 7.0790e-06, 1.2970e-05, 1.9298e-05, 1.3764e-05,\n",
      "         1.8137e-05, 4.7782e-06, 2.4153e-06, 5.4317e-06, 0.0000e+00, 1.8188e-05,\n",
      "         1.9423e-05],\n",
      "        [1.7400e-05, 8.4456e-06, 2.6052e-06, 1.6522e-05, 6.0796e-06, 4.5328e-06,\n",
      "         1.0009e-05, 1.2698e-06, 9.9587e-06, 1.3548e-06, 1.4463e-05, 1.3515e-05,\n",
      "         7.6271e-06, 4.5623e-06, 6.6480e-06, 1.8065e-05, 1.8188e-05, 0.0000e+00,\n",
      "         1.4013e-05],\n",
      "        [1.9191e-05, 1.0599e-05, 1.9885e-05, 1.5387e-05, 8.9580e-06, 3.4459e-06,\n",
      "         9.7358e-06, 8.8660e-06, 7.5474e-06, 2.0308e-06, 1.6286e-05, 1.9054e-05,\n",
      "         2.5756e-07, 2.6299e-06, 3.1940e-06, 1.7480e-06, 1.9423e-05, 1.4013e-05,\n",
      "         0.0000e+00]], grad_fn=<AddBackward0>)\n",
      "nodeInsDel: 0.5818319320678711\n",
      "edge_costs :\n",
      "tensor([[0.0000e+00, 1.2123e-01, 4.8936e-07],\n",
      "        [1.2123e-01, 0.0000e+00, 7.9264e-07],\n",
      "        [4.8936e-07, 7.9264e-07, 0.0000e+00]], grad_fn=<AddBackward0>)\n",
      "edgeInsDel: 0.2954656481742859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item of the valid =  99 0.2315569967031479\n",
      "loss.item of the valid =  99 0.23747369647026062\n",
      "loss.item of the valid =  99 0.2606603503227234\n",
      "loss.item of the valid =  99 0.37044501304626465\n",
      "Iteration 100 \t\t Training Loss: 0.32152414321899414 \t\t Validation Loss: 0.27503401413559914\n",
      "iter and min_valid_loss =  25 0.9715203642845154\n",
      " Min cost for nodeInsDel =  tensor(0.0598, grad_fn=<SelectBackward>)\n",
      " Min cost for edgeInsDel =  tensor(0.0331, grad_fn=<SelectBackward>)\n",
      " Min cost for nodeSub =  tensor([[0.0000, 0.0048, 0.0053, 0.0048, 0.0053, 0.0052, 0.0048, 0.0054, 0.0051,\n",
      "         0.0055, 0.0050, 0.0054, 0.0048, 0.0055, 0.0051, 0.0045, 0.0046, 0.0059,\n",
      "         0.0060],\n",
      "        [0.0048, 0.0000, 0.0059, 0.0054, 0.0057, 0.0054, 0.0060, 0.0045, 0.0059,\n",
      "         0.0056, 0.0047, 0.0051, 0.0060, 0.0057, 0.0047, 0.0059, 0.0044, 0.0054,\n",
      "         0.0055],\n",
      "        [0.0053, 0.0059, 0.0000, 0.0058, 0.0046, 0.0056, 0.0055, 0.0056, 0.0058,\n",
      "         0.0046, 0.0055, 0.0046, 0.0049, 0.0049, 0.0051, 0.0059, 0.0055, 0.0048,\n",
      "         0.0060],\n",
      "        [0.0048, 0.0054, 0.0058, 0.0000, 0.0056, 0.0050, 0.0051, 0.0056, 0.0052,\n",
      "         0.0045, 0.0060, 0.0057, 0.0052, 0.0045, 0.0048, 0.0060, 0.0058, 0.0059,\n",
      "         0.0058],\n",
      "        [0.0053, 0.0057, 0.0046, 0.0056, 0.0000, 0.0057, 0.0051, 0.0048, 0.0049,\n",
      "         0.0055, 0.0056, 0.0053, 0.0045, 0.0056, 0.0060, 0.0053, 0.0049, 0.0052,\n",
      "         0.0054],\n",
      "        [0.0052, 0.0054, 0.0056, 0.0050, 0.0057, 0.0000, 0.0047, 0.0053, 0.0058,\n",
      "         0.0049, 0.0058, 0.0059, 0.0053, 0.0054, 0.0048, 0.0057, 0.0057, 0.0050,\n",
      "         0.0049],\n",
      "        [0.0048, 0.0060, 0.0055, 0.0051, 0.0051, 0.0047, 0.0000, 0.0048, 0.0054,\n",
      "         0.0051, 0.0046, 0.0047, 0.0048, 0.0054, 0.0055, 0.0060, 0.0045, 0.0055,\n",
      "         0.0054],\n",
      "        [0.0054, 0.0045, 0.0056, 0.0056, 0.0048, 0.0053, 0.0048, 0.0000, 0.0052,\n",
      "         0.0052, 0.0048, 0.0047, 0.0048, 0.0044, 0.0045, 0.0049, 0.0058, 0.0047,\n",
      "         0.0054],\n",
      "        [0.0051, 0.0059, 0.0058, 0.0052, 0.0049, 0.0058, 0.0054, 0.0052, 0.0000,\n",
      "         0.0050, 0.0057, 0.0060, 0.0060, 0.0048, 0.0060, 0.0047, 0.0053, 0.0055,\n",
      "         0.0053],\n",
      "        [0.0055, 0.0056, 0.0046, 0.0045, 0.0055, 0.0049, 0.0051, 0.0052, 0.0050,\n",
      "         0.0000, 0.0054, 0.0052, 0.0057, 0.0059, 0.0053, 0.0047, 0.0057, 0.0047,\n",
      "         0.0048],\n",
      "        [0.0050, 0.0047, 0.0055, 0.0060, 0.0056, 0.0058, 0.0046, 0.0048, 0.0057,\n",
      "         0.0054, 0.0000, 0.0047, 0.0044, 0.0058, 0.0060, 0.0053, 0.0060, 0.0057,\n",
      "         0.0059],\n",
      "        [0.0054, 0.0051, 0.0046, 0.0057, 0.0053, 0.0059, 0.0047, 0.0047, 0.0060,\n",
      "         0.0052, 0.0047, 0.0000, 0.0057, 0.0054, 0.0054, 0.0047, 0.0057, 0.0057,\n",
      "         0.0060],\n",
      "        [0.0048, 0.0060, 0.0049, 0.0052, 0.0045, 0.0053, 0.0048, 0.0048, 0.0060,\n",
      "         0.0057, 0.0044, 0.0057, 0.0000, 0.0048, 0.0047, 0.0057, 0.0060, 0.0053,\n",
      "         0.0044],\n",
      "        [0.0055, 0.0057, 0.0049, 0.0045, 0.0056, 0.0054, 0.0054, 0.0044, 0.0048,\n",
      "         0.0059, 0.0058, 0.0054, 0.0048, 0.0000, 0.0052, 0.0058, 0.0051, 0.0050,\n",
      "         0.0048],\n",
      "        [0.0051, 0.0047, 0.0051, 0.0048, 0.0060, 0.0048, 0.0055, 0.0045, 0.0060,\n",
      "         0.0053, 0.0060, 0.0054, 0.0047, 0.0052, 0.0000, 0.0059, 0.0048, 0.0052,\n",
      "         0.0049],\n",
      "        [0.0045, 0.0059, 0.0059, 0.0060, 0.0053, 0.0057, 0.0060, 0.0049, 0.0047,\n",
      "         0.0047, 0.0053, 0.0047, 0.0057, 0.0058, 0.0059, 0.0000, 0.0051, 0.0059,\n",
      "         0.0047],\n",
      "        [0.0046, 0.0044, 0.0055, 0.0058, 0.0049, 0.0057, 0.0045, 0.0058, 0.0053,\n",
      "         0.0057, 0.0060, 0.0057, 0.0060, 0.0051, 0.0048, 0.0051, 0.0000, 0.0060,\n",
      "         0.0060],\n",
      "        [0.0059, 0.0054, 0.0048, 0.0059, 0.0052, 0.0050, 0.0055, 0.0047, 0.0055,\n",
      "         0.0047, 0.0057, 0.0057, 0.0053, 0.0050, 0.0052, 0.0059, 0.0060, 0.0000,\n",
      "         0.0057],\n",
      "        [0.0060, 0.0055, 0.0060, 0.0058, 0.0054, 0.0049, 0.0054, 0.0054, 0.0053,\n",
      "         0.0048, 0.0059, 0.0060, 0.0044, 0.0048, 0.0049, 0.0047, 0.0060, 0.0057,\n",
      "         0.0000]], grad_fn=<AddBackward0>)\n",
      " Min cost for edgeSub =  tensor([[0.0000e+00, 5.2132e-06, 1.1954e-03],\n",
      "        [5.2132e-06, 0.0000e+00, 1.5017e-03],\n",
      "        [1.1954e-03, 1.5017e-03, 0.0000e+00]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nb_iter=100\n",
    "InsDel, nodeSub,edgeSub,loss_plt,loss_valid_plt,loss_train_plt=classification(model,data,yt,nb_iter)\n",
    "plt.figure(0)\n",
    "plt.plot(InsDel[0:nb_iter,0],label=\"node\")\n",
    "plt.plot(InsDel[0:nb_iter,1],label=\"edge\")\n",
    "plt.title('Node/Edge insertion/deletion costs')\n",
    "plt.legend()\n",
    "plt.figure(1)\n",
    "for k in range(nodeSub.shape[1]):\n",
    "    plt.plot(nodeSub[0:nb_iter,k])\n",
    "plt.title('Node Substitutions costs')\n",
    "plt.figure(2)\n",
    "for k in range(edgeSub.shape[1]):\n",
    "    plt.plot(edgeSub[0:nb_iter,k])\n",
    "plt.title('Edge Substitutions costs')\n",
    "plt.figure(3)\n",
    "plt.plot(loss_plt)\n",
    "plt.title('Evolution of the train loss (loss_plt)')\n",
    "\n",
    "plt.figure(4)\n",
    "plt.plot(loss_valid_plt)\n",
    "plt.title('Evolution of the valid loss')\n",
    "'''\n",
    "plt.figure(5)\n",
    "plt.plot(loss_train_plt)\n",
    "plt.title('Evolution of the loss_train_plt')\n",
    "'''\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "A=torch.tensor(nx.to_scipy_sparse_matrix(Gs[0],dtype=int,weight='bond_type').todense(),dtype=torch.int) \n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plt, label='train loss')\n",
    "plt.plot(loss_valid_plt, label='valid loss')\n",
    "plt.title('Train and valid losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.plot(InsDel[0:500,0],label=\"node\")\n",
    "plt.plot(InsDel[0:500,1],label=\"edge\")\n",
    "plt.title('Node/Edge insertion/deletion costs')\n",
    "plt.legend()\n",
    "plt.figure(1)\n",
    "for k in range(nodeSub.shape[1]):\n",
    "    plt.plot(nodeSub[0:500,k])\n",
    "plt.title('node Substitution costs')\n",
    "plt.figure(2)\n",
    "for k in range(edgeSub.shape[1]):\n",
    "    plt.plot(edgeSub[0:500,k])\n",
    "plt.title('edge Substitution costs')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
