{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gklearn.utils.graphfiles import loadDataset\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "#matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "edge max label 3\n",
      "0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "def label_to_color(label):\n",
    "    if label == 'C':\n",
    "        return 0.1\n",
    "    elif label == 'O':\n",
    "        return 0.8\n",
    "    \n",
    "def nodes_to_color_sequence(G):\n",
    "    return [label_to_color(c[1]['label'][0]) for c in G.nodes(data=True)]\n",
    "\n",
    "Gs,y = loadDataset('DeepGED/MAO/dataset.ds')\n",
    "Gs=Gs[:45]\n",
    "y=y[:45]\n",
    "print(len(Gs))\n",
    "#for e in Gs[13].edges():\n",
    "#    print(Gs[13][e[0]][e[1]]['bond_type'])\n",
    "print('edge max label',max(max([[G[e[0]][e[1]]['bond_type'] for e in G.edges()] for G in Gs])))\n",
    "G1 = Gs[1]\n",
    "G2 = Gs[9]\n",
    "print(y[1],y[9])\n",
    "\n",
    "'''\n",
    "plt.figure(0)\n",
    "nx.draw_networkx(G1,with_labels=True,node_color = nodes_to_color_sequence(G1),cmap='autumn')\n",
    "\n",
    "plt.figure(1)\n",
    "nx.draw_networkx(G2,with_labels=True,node_color = nodes_to_color_sequence(G2),cmap='autumn')\n",
    "\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "import extended_label\n",
    "for g in Gs:\n",
    "    extended_label.compute_extended_labels(g)\n",
    "#for v in Gs[10].nodes():\n",
    "#        print(Gs[10].nodes[v])\n",
    "\n",
    "#print(nx.to_dict_of_lists(Gs[13]))\n",
    "\n",
    "\n",
    "\n",
    "#dict={'C':0,'N':1,'O':2}\n",
    "#A,labels=from_networkx_to_tensor2(Gs[13],dict)\n",
    "#print(A)\n",
    "#A1=(A==torch.ones(13,13)).int()\n",
    "#A2=(A==2*torch.ones(13,13)).int()\n",
    "#print(A1)\n",
    "#print(A2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gs= 45\n",
      "['C_1C', 'C_1C1C1N', 'C_1C1C2C', 'C_1C1N', 'C_1C1N2C', 'C_1C1O', 'C_1C1O2C', 'C_1C2C', 'C_1C3C', 'C_1N', 'C_1O', 'C_2C', 'C_2C2C', 'C_3C', 'N_1C', 'N_1C1C', 'N_1C1C1C', 'O_1C', 'O_1C1C']\n",
      "{'C_1C': 0, 'C_1C1C1N': 1, 'C_1C1C2C': 2, 'C_1C1N': 3, 'C_1C1N2C': 4, 'C_1C1O': 5, 'C_1C1O2C': 6, 'C_1C2C': 7, 'C_1C3C': 8, 'C_1N': 9, 'C_1O': 10, 'C_2C': 11, 'C_2C2C': 12, 'C_3C': 13, 'N_1C': 14, 'N_1C1C': 15, 'N_1C1C1C': 16, 'O_1C': 17, 'O_1C1C': 18} 19\n",
      "3\n",
      "torch.Size([45, 625])\n",
      "adjacency matrices tensor([[0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 1, 0,  ..., 0, 1, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.int32)\n",
      "node labels tensor([[15,  4,  7,  ...,  0,  0,  0],\n",
      "        [15,  4,  7,  ...,  0,  0,  0],\n",
      "        [15,  4,  7,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [16,  4,  7,  ...,  7,  5, 18],\n",
      "        [16,  4,  7,  ...,  0,  0,  0],\n",
      "        [16,  4,  7,  ...,  0,  0,  0]], device='cuda:0', dtype=torch.int32)\n",
      "order of the graphs tensor([11, 12, 14, 14, 15, 17, 15, 16, 19, 15, 16, 19, 12, 13, 15, 18, 16, 16,\n",
      "        17, 16, 17, 17, 18, 19, 19, 18, 18, 22, 22, 15, 14, 13, 16, 17, 17, 21,\n",
      "        17, 18, 18, 21, 24, 25, 25, 14, 17], device='cuda:0')\n",
      "2\n",
      "Parameter containing:\n",
      "tensor([0.0062, 0.0059, 0.0065, 0.0066, 0.0057, 0.0057, 0.0064, 0.0057, 0.0057,\n",
      "        0.0059, 0.0066, 0.0059, 0.0066, 0.0065, 0.0066, 0.0057, 0.0064, 0.0061,\n",
      "        0.0060, 0.0064, 0.0062, 0.0061, 0.0065, 0.0061, 0.0059, 0.0065, 0.0066,\n",
      "        0.0062, 0.0067, 0.0059, 0.0065, 0.0063, 0.0058, 0.0058, 0.0066, 0.0058,\n",
      "        0.0059, 0.0061, 0.0060, 0.0058, 0.0059, 0.0065, 0.0066, 0.0057, 0.0060,\n",
      "        0.0062, 0.0066, 0.0064, 0.0057, 0.0065, 0.0058, 0.0057, 0.0061, 0.0067,\n",
      "        0.0066, 0.0060, 0.0057, 0.0058, 0.0062, 0.0060, 0.0058, 0.0057, 0.0061,\n",
      "        0.0066, 0.0059, 0.0062, 0.0057, 0.0057, 0.0057, 0.0059, 0.0067, 0.0060,\n",
      "        0.0064, 0.0064, 0.0059, 0.0065, 0.0059, 0.0059, 0.0059, 0.0062, 0.0058,\n",
      "        0.0064, 0.0057, 0.0059, 0.0065, 0.0058, 0.0065, 0.0064, 0.0066, 0.0059,\n",
      "        0.0067, 0.0060, 0.0061, 0.0062, 0.0057, 0.0061, 0.0064, 0.0060, 0.0057,\n",
      "        0.0058, 0.0060, 0.0065, 0.0065, 0.0059, 0.0063, 0.0058, 0.0064, 0.0063,\n",
      "        0.0058, 0.0059, 0.0057, 0.0062, 0.0058, 0.0057, 0.0064, 0.0062, 0.0060,\n",
      "        0.0060, 0.0063, 0.0062, 0.0059, 0.0064, 0.0062, 0.0058, 0.0062, 0.0060,\n",
      "        0.0062, 0.0060, 0.0065, 0.0067, 0.0061, 0.0064, 0.0061, 0.0063, 0.0058,\n",
      "        0.0063, 0.0066, 0.0057, 0.0063, 0.0062, 0.0058, 0.0058, 0.0063, 0.0061,\n",
      "        0.0060, 0.0058, 0.0064, 0.0060, 0.0065, 0.0065, 0.0065, 0.0057, 0.0062,\n",
      "        0.0065, 0.0062, 0.0060, 0.0058, 0.0063, 0.0060, 0.0064, 0.0057, 0.0065,\n",
      "        0.0057, 0.0065, 0.0057, 0.0060, 0.0062, 0.0058, 0.0058, 0.0060, 0.0057,\n",
      "        0.0066], device='cuda:0', requires_grad=True)\n",
      "25 45\n",
      "toto\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import svd\n",
    "import rings\n",
    "from svd import iterated_power as compute_major_axis\n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "        \n",
    "    def __init__(self,GraphList,normalize=False,node_label='label'):\n",
    "        super(Net, self).__init__()   \n",
    "        self.normalize=normalize\n",
    "        self.node_label=node_label\n",
    "        dict,self.nb_edge_labels=self.build_node_dictionnary(GraphList)\n",
    "        self.nb_labels=len(dict)\n",
    "        print(self.nb_edge_labels)\n",
    "        self.device= torch.device(\"cuda:0\")  #'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        nb_node_pair_label=self.nb_labels*(self.nb_labels-1)/2.0\n",
    "        nb_edge_pair_label=int(self.nb_edge_labels*(self.nb_edge_labels-1)/2)\n",
    "        \n",
    "        self.node_weighs=nn.Parameter(torch.tensor(1.0/(nb_node_pair_label+nb_edge_pair_label+2))+(1e-3)*torch.rand(int(self.nb_labels*(self.nb_labels-1)/2+1),requires_grad=True,device=self.device)) # all substitution costs+ nodeIns/Del. old version: 0 node subs, 1 nodeIns/Del, 2 : edgeSubs, 3 edgeIns/Del        \n",
    "        self.edge_weighs=nn.Parameter(torch.tensor(1.0/(nb_node_pair_label+nb_edge_pair_label+2))+(1e-3)*torch.rand(nb_edge_pair_label+1,requires_grad=True,device=self.device)) #edgeIns/Del\n",
    "        \n",
    "        self.card=torch.tensor([G.order() for G in GraphList]).to(self.device)\n",
    "        card_max=self.card.max()\n",
    "        self.A=torch.zeros((len(GraphList),card_max*card_max),dtype=torch.int,device=self.device)\n",
    "        self.labels=torch.zeros((len(GraphList),card_max),dtype=torch.int,device=self.device)\n",
    "        print(self.A.shape)\n",
    "        for k in range(len(GraphList)):\n",
    "            A,l=self.from_networkx_to_tensor(GraphList[k],dict)             \n",
    "            self.A[k,0:A.shape[1]]=A[0]\n",
    "            self.labels[k,0:l.shape[0]]=l\n",
    "        print('adjacency matrices',self.A)\n",
    "        print('node labels',self.labels)\n",
    "        print('order of the graphs',self.card)\n",
    "        \n",
    "    def forward(self, input):  \n",
    "        self=self.to(self.device)\n",
    "        input=input.to(self.device)\n",
    "        ged=torch.zeros(len(input)).to(self.device) \n",
    "        node_costs,nodeInsDel,edge_costs,edgeInsDel=self.from_weighs_to_costs()\n",
    "        \n",
    "            \n",
    "        \n",
    "        #print('weighs:',self.weighs.device,'device:',self.device,'card:',self.card.device,'A:',self.A.device,'labels:',self.labels.device)\n",
    "        for k in range(len(input)):            \n",
    "            g1=input[k][0].to(self.device)\n",
    "            g2=input[k][1].to(self.device)\n",
    "            n=self.card[g1]\n",
    "            m=self.card[g2]\n",
    "            \n",
    "            self.ring_g,self.ring_h = rings.build_rings(g1,edgeInsDel.size()), rings.build_rings(g2,edgeInsDel.size()) \n",
    "            \n",
    "            C=self.construct_cost_matrix(g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)      \n",
    "            #S=self.mapping_from_similarity(C,n,m)\n",
    "            #S=self.mapping_from_cost(C,n,m)   \n",
    "            #S=self.new_mapping_from_cost(C,n,m,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)\n",
    "            S=self.mapping_from_cost_sans_FW(n,m,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)\n",
    "            \n",
    "            v=torch.flatten(S)\n",
    "            v=v.to(self.device)\n",
    "            normalize_factor=1.0\n",
    "            if self.normalize:\n",
    "                nb_edge1=(self.A[g1][0:n*n] != torch.zeros(n*n,device=self.device)).int().sum()\n",
    "                nb_edge2=(self.A[g2][0:m*m] != torch.zeros(m*m,device=self.device)).int().sum()\n",
    "                normalize_factor=nodeInsDel*(n+m)+edgeInsDel*(nb_edge1+nb_edge2)\n",
    "            c=torch.diag(C)\n",
    "            D=C-torch.eye(C.shape[0],device=self.device)*c\n",
    "            D=D.to(self.device)\n",
    "            ged[k]=(.5*v.T@D@v+c.T@v)/normalize_factor\n",
    "        max=torch.max(ged)\n",
    "        min=torch.min(ged)\n",
    "        ged=(ged-min)/(max-min)\n",
    "        \n",
    "        return ged\n",
    "    \n",
    "    def from_weighs_to_costs(self):\n",
    "        \n",
    "        #cn=torch.exp(self.node_weighs)\n",
    "        #ce=torch.exp(self.edge_weighs)\n",
    "        cn=self.node_weighs*self.node_weighs\n",
    "        ce=self.edge_weighs*self.edge_weighs\n",
    "        total_cost=cn.sum()+ce.sum()\n",
    "        cn=cn/total_cost #/max\n",
    "        ce=ce/total_cost\n",
    "        edgeInsDel=ce[-1]\n",
    "\n",
    "        node_costs=torch.zeros((self.nb_labels,self.nb_labels),device=self.device)\n",
    "        upper_part=torch.triu_indices(node_costs.shape[0],node_costs.shape[1],offset=1,device=self.device)        \n",
    "        node_costs[upper_part[0],upper_part[1]]=cn[0:-1]\n",
    "        node_costs=node_costs+node_costs.T\n",
    "\n",
    "        if self.nb_edge_labels>1:\n",
    "            edge_costs=torch.zeros((self.nb_edge_labels,self.nb_edge_labels),device=self.device)\n",
    "            upper_part=torch.triu_indices(edge_costs.shape[0],edge_costs.shape[1],offset=1,device=self.device)        \n",
    "            edge_costs[upper_part[0],upper_part[1]]=ce[0:-1]\n",
    "            edge_costs=edge_costs+edge_costs.T\n",
    "        else:\n",
    "            edge_costs=torch.zeros(0,device=self.device)\n",
    "        \n",
    "        return node_costs,cn[-1],edge_costs,edgeInsDel\n",
    "    \n",
    "    def build_node_dictionnary(self,GraphList):\n",
    "        #extraction de tous les labels d'atomes\n",
    "        node_labels=[]\n",
    "        for G in Gs:\n",
    "            for v in nx.nodes(G):\n",
    "                if not G.nodes[v][self.node_label][0] in node_labels:\n",
    "                    node_labels.append(G.nodes[v][self.node_label][0])\n",
    "        node_labels.sort()\n",
    "        #extraction d'un dictionnaire permettant de numéroter chaque label par un numéro.\n",
    "        dict={}\n",
    "        k=0\n",
    "        for label in node_labels:\n",
    "            dict[label]=k\n",
    "            k=k+1\n",
    "        print(node_labels)\n",
    "        print(dict,len(dict))\n",
    "    \n",
    "        return dict,max(max([[int(G[e[0]][e[1]]['bond_type']) for e in G.edges()] for G in GraphList]))\n",
    "    \n",
    "    def from_networkx_to_tensor(self,G,dict):    \n",
    "        A=torch.tensor(nx.to_scipy_sparse_matrix(G,dtype=int,weight='bond_type').todense(),dtype=torch.int)        \n",
    "        lab=[dict[G.nodes[v][self.node_label][0]] for v in nx.nodes(G)]\n",
    "   \n",
    "        return (A.view(1,A.shape[0]*A.shape[1]),torch.tensor(lab))\n",
    "\n",
    "    def construct_cost_matrix(self,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel):\n",
    "        n = self.card[g1].item()\n",
    "        m = self.card[g2].item()\n",
    "        \n",
    "        A1=torch.zeros((n+1,n+1),dtype=torch.int,device=self.device)\n",
    "        A1[0:n,0:n]=self.A[g1][0:n*n].view(n,n)\n",
    "        A2=torch.zeros((m+1,m+1),dtype=torch.int,device=self.device)\n",
    "        A2[0:m,0:m]=self.A[g2][0:m*m].view(m,m)\n",
    "        \n",
    "        \n",
    "         # costs: 0 node subs, 1 nodeIns/Del, 2 : edgeSubs, 3 edgeIns/Del\n",
    "        \n",
    "        #C=cost[3]*torch.cat([torch.cat([C12[l][k] for k in range(n+1)],1) for l in range(n+1)])\n",
    "        #Pas bien sur mais cela semble fonctionner.\n",
    "        C=edgeInsDel*self.matrix_edgeInsDel(A1,A2)\n",
    "        if self.nb_edge_labels>1:\n",
    "            for k in range(self.nb_edge_labels):\n",
    "                for l in range(self.nb_edge_labels):\n",
    "                    if k != l:\n",
    "#                    C.add_(self.matrix_edgeSubst(A1,A2,k+1,l+1),alpha=edge_costs[k][l])\n",
    "                        C=C+edge_costs[k][l]*self.matrix_edgeSubst(A1,A2,k+1,l+1)\n",
    "        #C=cost[3]*torch.tensor(np.array([ [  k!=l and A1[k//(m+1),l//(m+1)]^A2[k%(m+1),l%(m+1)] for k in range((n+1)*(m+1))] for l in range((n+1)*(m+1))]),device=self.device)        \n",
    "\n",
    "        l1=self.labels[g1][0:n]\n",
    "        l2=self.labels[g2][0:m]\n",
    "        D=torch.zeros((n+1)*(m+1),device=self.device)\n",
    "        D[n*(m+1):]=nodeInsDel\n",
    "        D[n*(m+1)+m]=0\n",
    "        D[[i*(m+1)+m for i in range(n)]]=nodeInsDel\n",
    "        D[[k for k in range(n*(m+1)) if k%(m+1) != m]]=torch.tensor([node_costs[l1[k//(m+1)],l2[k%(m+1)]] for k in range(n*(m+1)) if k%(m+1) != m],device=self.device )\n",
    "        mask = torch.diag(torch.ones_like(D))\n",
    "        C=mask*torch.diag(D) + (1. - mask)*C\n",
    "        \n",
    "        #C[range(len(C)),range(len(C))]=D\n",
    "      \n",
    "        return C\n",
    "    def matrix_edgeInsDel(self,A1,A2):\n",
    "        Abin1=(A1!=torch.zeros((A1.shape[0],A1.shape[1]),device=self.device))\n",
    "        Abin2=(A2!=torch.zeros((A2.shape[0],A2.shape[1]),device=self.device))\n",
    "        C1=torch.einsum('ij,kl->ijkl',torch.logical_not(Abin1),Abin2)\n",
    "        C2=torch.einsum('ij,kl->ijkl',Abin1,torch.logical_not(Abin2))\n",
    "        C12=torch.logical_or(C1,C2).int()\n",
    "    \n",
    "        return torch.cat(torch.unbind(torch.cat(torch.unbind(C12,1),1),0),1)\n",
    "\n",
    "    def matrix_edgeSubst(self,A1,A2,lab1,lab2):\n",
    "        Abin1=(A1==lab1*torch.ones((A1.shape[0],A1.shape[1]),device=self.device)).int()\n",
    "        Abin2=(A2==lab2*torch.ones((A2.shape[0],A2.shape[1]),device=self.device)).int()\n",
    "        C=torch.einsum('ij,kl->ijkl',Abin1,Abin2)\n",
    "        \n",
    "        return torch.cat(torch.unbind(torch.cat(torch.unbind(C,1),1),0),1)\n",
    "    \n",
    "    def similarity_from_cost(self,C):\n",
    "        N=C.shape[0]\n",
    "             \n",
    "        #return (torch.norm(C,p='fro')*torch.eye(N,device=self.device) -C)\n",
    "        return (C.max()*torch.eye(N,device=self.device) -C)\n",
    "    \n",
    "    def lsape_populate_instance(self,first_graph,second_graph,average_node_cost, average_edge_cost,alpha,lbda):       #ring_g, ring_h come from global ring with all graphs in so ring_g = rings['g'] and ring_h = rings['h']\n",
    "        g,h = Gs[first_graph], Gs[second_graph]\n",
    "        self.average_cost =[average_node_cost, average_edge_cost]\n",
    "        self.first_graph, self.second_graph = first_graph,second_graph\n",
    "        \n",
    "        node_costs,nodeInsDel,edge_costs,edgeInsDel=self.from_weighs_to_costs()\n",
    "\n",
    "        lsape_instance = [[0 for _ in range(len(g) + 1)] for __ in range(len(h) + 1)]\n",
    "        for g_node_index in range(len(g) + 1):\n",
    "            for h_node_index in range(len(h) + 1):\n",
    "                lsape_instance[h_node_index][g_node_index] = rings.compute_ring_distance(g,h,self.ring_g,self.ring_h,g_node_index,h_node_index,alpha,lbda,node_costs,nodeInsDel,edge_costs,edgeInsDel,first_graph,second_graph)\n",
    "        for i in lsape_instance :\n",
    "            i = torch.as_tensor(i)\n",
    "        lsape_instance = torch.as_tensor(lsape_instance)\n",
    "        #print(type(lsape_instance))\n",
    "        return lsape_instance\n",
    "    \n",
    "  \n",
    "    def mapping_from_cost_sans_FW(self,n,m,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel): \n",
    "        c_0 =self.lsape_populate_instance(g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)\n",
    "        x0=svd.eps_assigment_from_mapping(torch.exp(-c_0),10).view((n+1)*(m+1),1)\n",
    "        return x0\n",
    "    \n",
    "    def new_mapping_from_cost(self,C,n,m,g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel): \n",
    "        c=torch.diag(C)       \n",
    "        c_0 =self.lsape_populate_instance(g1,g2,node_costs,edge_costs,nodeInsDel,edgeInsDel)\n",
    "        D=C-torch.eye(C.shape[0],device=self.device)*c\n",
    "        x0=svd.eps_assigment_from_mapping(torch.exp(-c_0),10).view((n+1)*(m+1),1)\n",
    "        return svd.franck_wolfe(x0,D,c,5,15,n,m)\n",
    "    \n",
    "    \n",
    "    def mapping_from_cost(self,C,n,m):\n",
    "        c=torch.diag(C)\n",
    "        D=C-torch.eye(C.shape[0],device=self.device)*c\n",
    "        x0=svd.eps_assigment_from_mapping(torch.exp(-.5*c.view(n+1,m+1)),10).view((n+1)*(m+1),1)\n",
    "        x=svd.franck_wolfe(x0,D,c,5,10,n,m)\n",
    "        def print_grad(grad):\n",
    "            if(grad.norm()!= 0.0):\n",
    "                print(grad)\n",
    "        \n",
    "#        x.register_hook(print_grad)\n",
    "        return x\n",
    "\n",
    "print('Gs=',len(Gs))\n",
    "model = Net(Gs,normalize=True,node_label='extended_label')\n",
    "\n",
    "params = list(model.parameters())\n",
    "print(len(params))\n",
    "print(params[0])\n",
    "#print(model(input))\n",
    "print(max([G.order() for G in Gs]),len(Gs))\n",
    "print('toto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "nb=len(Gs)\n",
    "class1=torch.tensor([k for k in range(len(y)) if y[k]==1])\n",
    "class2=torch.tensor([k for k in range(len(y)) if y[k]==0])\n",
    "\n",
    "nb_class1=12\n",
    "nb_class2=int((nb_class1-1)/2)\n",
    "train_size=nb_class1+nb_class2\n",
    "#train_size=20\n",
    "\n",
    "#if train_size % 2 == 0:\n",
    "#    nb_class1=int(train_size/2)\n",
    "#    nb_class2=int(train_size/2)\n",
    "#else:\n",
    "#    nb_class1=int(train_size/2)+1\n",
    "#    nb_class2=int(train_size/2)\n",
    "    \n",
    "print((torch.abs(10000*torch.randn(nb_class1)).int()%class1.size()[0]).long())\n",
    "random_class1=class1[(torch.abs(10000*torch.randn(nb_class1)).int()%class1.size()[0]).long()]\n",
    "random_class2=class2[(torch.abs(10000*torch.randn(nb_class2)).int()%class2.size()[0]).long()]\n",
    "train_graphs=torch.cat((random_class1,random_class2),0)\n",
    "print('train graphs:',train_graphs)\n",
    "\n",
    "\n",
    "couples=torch.triu_indices(train_size,train_size,offset=1)\n",
    "print('couples=',couples)\n",
    "print('nb_class1/nb_class2=',nb_class1,nb_class2)\n",
    "#combinations=itertools.combinations(range(nb),2)\n",
    "\n",
    "nb_elt=int(nb_class1*(nb_class1+2*nb_class2-1)/2)\n",
    "print('couples restreints:',couples[:,0:nb_elt])\n",
    "\n",
    "#nb_elt=int(train_size*(train_size-1)/2)\n",
    "data=torch.empty((nb_elt,2),dtype=torch.int)\n",
    "yt=torch.ones(nb_elt)\n",
    "print('old_size, new size=',nb_elt,.5*nb_class1*(nb_class1+2*nb_class2-1))\n",
    "data[0:nb_elt,0]=train_graphs[couples[0,0:nb_elt]]\n",
    "data[0:nb_elt,1]=train_graphs[couples[1,0:nb_elt]]\n",
    "\n",
    "\n",
    "#data[0:nb_elt,0]=train_graphs[couples[0]]\n",
    "#data[0:nb_elt,1]=train_graphs[couples[1]]\n",
    "print(nb_elt)\n",
    "#couples=[]\n",
    "for k in range(nb_elt):\n",
    "    if (y[data[k][0]]!=y[data[k][1]]):\n",
    "        yt[k]=-1.0        \n",
    "\n",
    "print('data=',data,len(data))\n",
    "\n",
    "#print(couples[1:2])\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")          # a CUDA device object    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[train_D, valid_D,train_L,valid_L]= train_test_split(Gs,y, test_size=0.25,train_size=0.75, shuffle=True) #, stratify=yt)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[train_D, valid_D,train_L,valid_L]= train_test_split(Gs,y, test_size=0.25,train_size=0.75, shuffle=True) #, stratify=yt)\n",
    "  \n",
    "\n",
    "def creating_couples_after_splitting(train_D, valid_D,train_L,valid_L):\n",
    "    couples_train=[]\n",
    "    couples_test_train=[]\n",
    "    for i,g1_idx in enumerate(train_D): \n",
    "        for j,g2_idx in enumerate(train_D):\n",
    "            n=g1_idx.order()\n",
    "            m=g2_idx.order()\n",
    "            #print([n,m])\n",
    "            couples_train.append([n,m])\n",
    "    yt=np.ones(len(couples_train))\n",
    "    #print(yt,len(yt))\n",
    "    for k in couples_train:\n",
    "        if (y[k[0]]!=y[k[1]]):\n",
    "            yt[k]=-1.0  \n",
    "    for i,g1_idx in enumerate(valid_D):\n",
    "        for j,g2_idx in enumerate(train_D):\n",
    "            n=g1_idx.order()\n",
    "            m=g2_idx.order()\n",
    "            couples_test_train.append([n,m])\n",
    "            \n",
    "    yv=np.ones(len(couples_test_train))\n",
    "    #print(yt,len(yt))\n",
    "    for k in couples_test_train:\n",
    "        if (y[k[0]]!=y[k[1]]):\n",
    "            yv[k]=-1.0\n",
    "            \n",
    "    return torch.tensor(couples_train),yt,torch.tensor(couples_test_train),yv\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[11]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def different_sets(my_train_D,my_valid_D): #verifying that the two sets contain different graphs\n",
    "    #my_list=[i for i in range(68)] \n",
    "    cp=my_valid_D\n",
    "\n",
    "    for i in range(len(my_valid_D)):\n",
    "        if my_valid_D[i] in my_train_D:\n",
    "            tmp=random.choice(Gs)\n",
    "            if tmp not in my_train_D: \n",
    "                cp[i]=tmp\n",
    "        #else: cp[i]=my_valid_D[i]\n",
    "    my_valid_D=cp\n",
    "    \n",
    "    return my_train_D,my_valid_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def different_sets_ints(my_train_D,my_valid_D):\n",
    "    #my_list=[i for i in range(68)] \n",
    "    my_list=[i for i in range(45)] \n",
    "    cp=my_valid_D\n",
    "\n",
    "    for i in range(len(my_valid_D)):\n",
    "        if my_valid_D[i] in my_train_D:\n",
    "            tmp=random.choice(my_list)\n",
    "            if tmp not in my_train_D: \n",
    "                cp[i]=tmp\n",
    "        #else: cp[i]=my_valid_D[i]\n",
    "    my_valid_D=cp\n",
    "    \n",
    "    return my_train_D,my_valid_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x7f2dd7e98370>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gs[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_data(train_D, valid_D,train_L,valid_L): #getting orders of the graphs from the train and valid sets\n",
    "    my_train_D=[]\n",
    "    my_valid_D=[]\n",
    "    for i,g1_idx in enumerate(train_D): \n",
    "        n=g1_idx.order()\n",
    "        my_train_D.append(n)\n",
    "    for i,g1_idx in enumerate(valid_D): \n",
    "        n=g1_idx.order()\n",
    "        my_valid_D.append(n)\n",
    "        \n",
    "    my_train_D,my_valid_D=different_sets_ints(my_train_D,my_valid_D)\n",
    "    \n",
    "    return my_train_D,my_valid_D\n",
    "\n",
    "#my_train_D,my_valid_D,train_L,valid_L=getting_data(train_D, valid_D,train_L,valid_L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "\n",
    "def splitting(Gs): #data\n",
    "    \n",
    "    #[train_D, valid_D,train_L,valid_L]= train_test_split(data,yt, test_size=0.25,train_size=0.75, shuffle=True) #, stratify=yt)\n",
    "    [train_D, valid_D,train_L,valid_L]= train_test_split(Gs,y, test_size=0.25,train_size=0.75, shuffle=True, stratify=y)\n",
    "    train_D, valid_D=different_sets(train_D,valid_D)\n",
    "    \n",
    "    couples_train,yt,couples_test_train,yv = creating_couples_after_splitting(train_D, valid_D,train_L,valid_L)\n",
    "    yt=torch.tensor(yt)\n",
    "    yv=torch.tensor(yv)\n",
    "    #DatasetTrain = TensorDataset(train_D, train_L)\n",
    "    DatasetTrain = TensorDataset(couples_train,yt) \n",
    "    \n",
    "    #DatasetValid=TensorDataset(valid_D, valid_L)\n",
    "    DatasetValid=TensorDataset(couples_test_train, yv)\n",
    "\n",
    "    trainloader=torch.utils.data.DataLoader(DatasetTrain,batch_size=len(couples_train),shuffle=True,drop_last=True, num_workers=0)\n",
    "\n",
    "    validationloader=torch.utils.data.DataLoader(DatasetValid, batch_size=len(couples_test_train), drop_last=True,num_workers=0) #64,128,\n",
    "\n",
    "    '''\n",
    "    print(len(train_D), len(valid_D))\n",
    "    print(len(train_L), len(valid_L))\n",
    "    print(len(trainloader),len(validationloader))\n",
    "    for i,j in validationloader:\n",
    "        print('i : ',i,'\\n j : ',j)\n",
    "    '''\n",
    "    \n",
    "    print(len(trainloader),len(validationloader))\n",
    "    print(len(trainloader),len(validationloader))\n",
    "    \n",
    "    train_D,valid_D=getting_data(train_D, valid_D,train_L,valid_L)\n",
    "    torch.save(train_D, 'train_D', pickle_module=pkl) \n",
    "    torch.save(valid_D, 'valid_D', pickle_module=pkl) \n",
    "    torch.save(train_L, 'train_L', pickle_module=pkl) \n",
    "    torch.save(valid_L, 'valid_L', pickle_module=pkl) \n",
    "    \n",
    "    print(\"valid_D = \",valid_D,len(valid_D))\n",
    "    \n",
    "    return trainloader,validationloader,couples_train,yt,couples_test_train,yv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if torch.cuda.device_count() > 1:\n",
    "#  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "#  model = nn.DataParallel(model)\n",
    "from triangular_losses import TriangularConstraint as triangular_constraint\n",
    "\n",
    "model.to(device)\n",
    "def classification(model,Gs,nb_iter):\n",
    "    trainloader,validationloader,couples_train,yt,couples_test_train,yv=splitting(Gs)\n",
    "    criterion = torch.nn.HingeEmbeddingLoss(margin=1.0,reduction='mean')\n",
    "    criterionTri=triangular_constraint()\n",
    "    optimizer = torch.optim.Adam(model.parameters()) #, lr=1e-3\n",
    "    #print(device)\n",
    "\n",
    "    #torch.cat((same_class[0:20],diff_class[0:20]),0).to(device)\n",
    "    #whole_input=data.to(device)\n",
    "    train_input=couples_train.to(device)\n",
    "    valid_input=couples_test_train.to(device)\n",
    "    \n",
    "    target=yt.to(device) \n",
    "    #torch.ones(40,device=device)\n",
    "    #target[20:]=-1.0\n",
    "    #target=(yt[0:20]).to(device)\n",
    "    InsDel=np.empty((nb_iter,2))\n",
    "    node_costs,nodeInsDel,edge_costs,edgeInsDel=model.from_weighs_to_costs()\n",
    "    nodeSub=np.empty((nb_iter,int(node_costs.shape[0]*(node_costs.shape[0]-1)/2)))\n",
    "    edgeSub=np.empty((nb_iter,int(edge_costs.shape[0]*(edge_costs.shape[0]-1)/2)))\n",
    "    loss_plt=np.empty(nb_iter)\n",
    "    loss_train_plt=np.empty(nb_iter)\n",
    "    loss_valid_plt=np.empty(nb_iter)\n",
    "    min_valid_loss = np.inf\n",
    "    iter_min_valid_loss = 0\n",
    "    \n",
    "    for t in range(nb_iter):    \n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        tmp=np.inf\n",
    "        for train_data,train_labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #inputt=train_data.to(device)\n",
    "            \n",
    "            # Forward pass: Compute predicted y by passing data to the model\n",
    "            y_pred = model(train_input).to(device)  #whole_input\n",
    "\n",
    "            # Compute and print loss\n",
    "            loss = criterion(y_pred, target).to(device)\n",
    "            node_costs,nodeInsDel,edge_costs,edgeInsDel=model.from_weighs_to_costs()\n",
    "            triangularInq=criterionTri(node_costs,nodeInsDel,edge_costs,edgeInsDel)\n",
    "            loss=loss*(1+triangularInq)\n",
    "            loss.to(device)\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            #optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print('loss.item of the train = ', t, loss.item())\n",
    "            train_loss =+ loss.item() #* train_data.size(0) \n",
    "            if (loss.item()<tmp): tmp=loss.item()\n",
    "        loss_plt[t]=loss.item()  \n",
    "        loss_train_plt[t]=train_loss /len(trainloader)\n",
    "        #loss_plt[t]=tmp\n",
    "            \n",
    "        if t % 100 == 99 or t==0:   \n",
    "            print('ged=',y_pred*target)  #train_labels\n",
    "            print('Distances: ',y_pred)\n",
    "            print('Loss Triangular:',triangularInq.item())\n",
    "            print('node_costs :')\n",
    "            print(node_costs)\n",
    "            print('nodeInsDel:',nodeInsDel.item())\n",
    "            print('edge_costs :')\n",
    "            print(edge_costs)\n",
    "            print('edgeInsDel:',edgeInsDel.item())\n",
    "            \n",
    "            \n",
    "            \n",
    "        for valid_data,valid_labels in validationloader:\n",
    "            \n",
    "            inputt=valid_data.to(device)\n",
    "            y_pred = model(inputt).to(device)\n",
    "            # Compute and print loss\n",
    "            valid_labels=valid_labels.to(device)\n",
    "            loss = criterion(y_pred, valid_labels).to(device)    \n",
    "            loss.to(device)\n",
    "            \n",
    "            print('loss.item of the valid = ', t, loss.item())  \n",
    "            valid_loss = valid_loss + loss.item() #* valid_data.size(0)\n",
    "            \n",
    "        loss_valid_plt[t]=valid_loss / len(validationloader)   \n",
    "        \n",
    "        InsDel[t][0]=nodeInsDel.item()\n",
    "        InsDel[t][1]=edgeInsDel.item()\n",
    "        \n",
    "        \n",
    "        k=0\n",
    "        for p in range(node_costs.shape[0]):\n",
    "            for q in range(p+1,node_costs.shape[0]):\n",
    "                nodeSub[t][k]=node_costs[p][q]\n",
    "                k=k+1\n",
    "        k=0\n",
    "        for p in range(edge_costs.shape[0]):\n",
    "            for q in range(p+1,edge_costs.shape[0]):\n",
    "                edgeSub[t][k]=edge_costs[p][q]\n",
    "                k=k+1\n",
    "        \n",
    "            \n",
    "        print(f'Iteration {t+1} \\t\\t Training Loss: {train_loss / len(trainloader)} \\t\\t Validation Loss: {valid_loss/len(validationloader)}')\n",
    "        if min_valid_loss > valid_loss:\n",
    "            print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f})')\n",
    "            min_valid_loss = valid_loss\n",
    "            iter_min_valid_loss = t\n",
    "            nodeSub_min = node_costs\n",
    "            edgeSub_min = edge_costs\n",
    "            nodeInsDel_min = nodeInsDel\n",
    "            edgeInsDel_min = edgeInsDel\n",
    "            \n",
    "            \n",
    "    print('iter and min_valid_loss = ',iter_min_valid_loss, min_valid_loss)\n",
    "    '''\n",
    "    nodeInsDel_min = InsDel[iter_min_valid_loss][0]\n",
    "    edgeInsDel_min = InsDel[iter_min_valid_loss][1]\n",
    "    nodeSub_min = nodeSub[iter_min_valid_loss]\n",
    "    edgeSub_min = edgeSub[iter_min_valid_loss]\n",
    "    '''\n",
    "    print(' Min cost for nodeInsDel = ', nodeInsDel_min)\n",
    "    print(' Min cost for edgeInsDel = ', edgeInsDel_min)\n",
    "    print(' Min cost for nodeSub = ', nodeSub_min)\n",
    "    print(' Min cost for edgeSub = ', edgeSub_min)\n",
    "    torch.save(nodeInsDel_min, 'nodeInsDel_min', pickle_module=pkl) \n",
    "    torch.save(edgeInsDel_min, 'edgeInsDel_min', pickle_module=pkl) \n",
    "    torch.save(nodeSub_min, 'nodeSub_min', pickle_module=pkl) \n",
    "    torch.save(edgeSub_min, 'edgeSub_min', pickle_module=pkl)\n",
    "    return InsDel,nodeSub,edgeSub,loss_plt,loss_valid_plt,loss_train_plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "1 1\n",
      "1 1\n",
      "valid_D =  [13, 3, 35, 29, 22, 27, 15, 22, 17, 8, 14, 22] 12\n",
      "loss.item of the train =  0 0.38658103346824646\n",
      "ged= tensor([0.3736, 0.4857, 0.3181,  ..., 0.1067, 0.1019, 0.3659], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "Distances:  tensor([0.3736, 0.4857, 0.3181,  ..., 0.1067, 0.1019, 0.3659], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "Loss Triangular: 0.0\n",
      "node_costs :\n",
      "tensor([[0.0000, 0.0057, 0.0053, 0.0064, 0.0067, 0.0050, 0.0049, 0.0062, 0.0049,\n",
      "         0.0049, 0.0053, 0.0066, 0.0052, 0.0065, 0.0064, 0.0065, 0.0049, 0.0062,\n",
      "         0.0056],\n",
      "        [0.0057, 0.0000, 0.0055, 0.0063, 0.0058, 0.0055, 0.0065, 0.0056, 0.0052,\n",
      "         0.0064, 0.0065, 0.0058, 0.0067, 0.0052, 0.0065, 0.0060, 0.0051, 0.0050,\n",
      "         0.0067],\n",
      "        [0.0053, 0.0055, 0.0000, 0.0051, 0.0052, 0.0056, 0.0054, 0.0051, 0.0053,\n",
      "         0.0065, 0.0065, 0.0049, 0.0054, 0.0057, 0.0065, 0.0061, 0.0050, 0.0064,\n",
      "         0.0051],\n",
      "        [0.0064, 0.0063, 0.0051, 0.0000, 0.0050, 0.0057, 0.0067, 0.0066, 0.0054,\n",
      "         0.0049, 0.0051, 0.0059, 0.0054, 0.0051, 0.0050, 0.0056, 0.0067, 0.0052,\n",
      "         0.0059],\n",
      "        [0.0067, 0.0058, 0.0052, 0.0050, 0.0000, 0.0049, 0.0050, 0.0049, 0.0052,\n",
      "         0.0067, 0.0054, 0.0061, 0.0062, 0.0053, 0.0063, 0.0052, 0.0053, 0.0053,\n",
      "         0.0059],\n",
      "        [0.0050, 0.0055, 0.0056, 0.0057, 0.0049, 0.0000, 0.0051, 0.0062, 0.0050,\n",
      "         0.0052, 0.0064, 0.0051, 0.0063, 0.0061, 0.0067, 0.0053, 0.0067, 0.0054,\n",
      "         0.0056],\n",
      "        [0.0049, 0.0065, 0.0054, 0.0067, 0.0050, 0.0051, 0.0000, 0.0058, 0.0049,\n",
      "         0.0055, 0.0062, 0.0054, 0.0049, 0.0050, 0.0055, 0.0064, 0.0064, 0.0052,\n",
      "         0.0060],\n",
      "        [0.0062, 0.0056, 0.0051, 0.0066, 0.0049, 0.0062, 0.0058, 0.0000, 0.0052,\n",
      "         0.0062, 0.0060, 0.0050, 0.0053, 0.0049, 0.0058, 0.0050, 0.0049, 0.0062,\n",
      "         0.0058],\n",
      "        [0.0049, 0.0052, 0.0053, 0.0054, 0.0052, 0.0050, 0.0049, 0.0052, 0.0000,\n",
      "         0.0055, 0.0055, 0.0060, 0.0058, 0.0053, 0.0061, 0.0059, 0.0052, 0.0058,\n",
      "         0.0054],\n",
      "        [0.0049, 0.0064, 0.0065, 0.0049, 0.0067, 0.0052, 0.0055, 0.0062, 0.0055,\n",
      "         0.0000, 0.0059, 0.0054, 0.0063, 0.0067, 0.0056, 0.0061, 0.0055, 0.0061,\n",
      "         0.0051],\n",
      "        [0.0053, 0.0065, 0.0065, 0.0051, 0.0054, 0.0064, 0.0062, 0.0060, 0.0055,\n",
      "         0.0059, 0.0000, 0.0059, 0.0066, 0.0049, 0.0059, 0.0059, 0.0051, 0.0051,\n",
      "         0.0061],\n",
      "        [0.0066, 0.0058, 0.0049, 0.0059, 0.0061, 0.0051, 0.0054, 0.0050, 0.0060,\n",
      "         0.0054, 0.0059, 0.0000, 0.0057, 0.0055, 0.0050, 0.0062, 0.0055, 0.0063,\n",
      "         0.0065],\n",
      "        [0.0052, 0.0067, 0.0054, 0.0054, 0.0062, 0.0063, 0.0049, 0.0053, 0.0058,\n",
      "         0.0063, 0.0066, 0.0057, 0.0000, 0.0063, 0.0049, 0.0059, 0.0063, 0.0058,\n",
      "         0.0055],\n",
      "        [0.0065, 0.0052, 0.0057, 0.0051, 0.0053, 0.0061, 0.0050, 0.0049, 0.0053,\n",
      "         0.0067, 0.0049, 0.0055, 0.0063, 0.0000, 0.0051, 0.0060, 0.0054, 0.0061,\n",
      "         0.0049],\n",
      "        [0.0064, 0.0065, 0.0065, 0.0050, 0.0063, 0.0067, 0.0055, 0.0058, 0.0061,\n",
      "         0.0056, 0.0059, 0.0050, 0.0049, 0.0051, 0.0000, 0.0064, 0.0049, 0.0064,\n",
      "         0.0049],\n",
      "        [0.0065, 0.0060, 0.0061, 0.0056, 0.0052, 0.0053, 0.0064, 0.0050, 0.0059,\n",
      "         0.0061, 0.0059, 0.0062, 0.0059, 0.0060, 0.0064, 0.0000, 0.0054, 0.0057,\n",
      "         0.0050],\n",
      "        [0.0049, 0.0051, 0.0050, 0.0067, 0.0053, 0.0067, 0.0064, 0.0049, 0.0052,\n",
      "         0.0055, 0.0051, 0.0055, 0.0063, 0.0054, 0.0049, 0.0054, 0.0000, 0.0051,\n",
      "         0.0054],\n",
      "        [0.0062, 0.0050, 0.0064, 0.0052, 0.0053, 0.0054, 0.0052, 0.0062, 0.0058,\n",
      "         0.0061, 0.0051, 0.0063, 0.0058, 0.0061, 0.0064, 0.0057, 0.0051, 0.0000,\n",
      "         0.0050],\n",
      "        [0.0056, 0.0067, 0.0051, 0.0059, 0.0059, 0.0056, 0.0060, 0.0058, 0.0054,\n",
      "         0.0051, 0.0061, 0.0065, 0.0055, 0.0049, 0.0049, 0.0050, 0.0054, 0.0050,\n",
      "         0.0000]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "nodeInsDel: 0.006555370520800352\n",
      "edge_costs :\n",
      "tensor([[0.0000, 0.0056, 0.0061],\n",
      "        [0.0056, 0.0000, 0.0062],\n",
      "        [0.0061, 0.0062, 0.0000]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "edgeInsDel: 0.005852199625223875\n",
      "loss.item of the valid =  0 0.3757871389389038\n",
      "Iteration 1 \t\t Training Loss: 0.38658103346824646 \t\t Validation Loss: 0.3757871389389038\n",
      "Validation Loss Decreased(inf--->0.375787)\n",
      "loss.item of the train =  1 0.35060134530067444\n",
      "loss.item of the valid =  1 0.35163548588752747\n",
      "Iteration 2 \t\t Training Loss: 0.35060134530067444 \t\t Validation Loss: 0.35163548588752747\n",
      "Validation Loss Decreased(0.375787--->0.351635)\n",
      "loss.item of the train =  2 0.3258873522281647\n",
      "loss.item of the valid =  2 0.32690876722335815\n",
      "Iteration 3 \t\t Training Loss: 0.3258873522281647 \t\t Validation Loss: 0.32690876722335815\n",
      "Validation Loss Decreased(0.351635--->0.326909)\n",
      "loss.item of the train =  3 0.30076292157173157\n",
      "loss.item of the valid =  3 0.30303800106048584\n",
      "Iteration 4 \t\t Training Loss: 0.30076292157173157 \t\t Validation Loss: 0.30303800106048584\n",
      "Validation Loss Decreased(0.326909--->0.303038)\n",
      "loss.item of the train =  4 0.2761857211589813\n",
      "loss.item of the valid =  4 0.2750919461250305\n",
      "Iteration 5 \t\t Training Loss: 0.2761857211589813 \t\t Validation Loss: 0.2750919461250305\n",
      "Validation Loss Decreased(0.303038--->0.275092)\n",
      "loss.item of the train =  5 0.2476290911436081\n",
      "loss.item of the valid =  5 0.24326840043067932\n",
      "Iteration 6 \t\t Training Loss: 0.2476290911436081 \t\t Validation Loss: 0.24326840043067932\n",
      "Validation Loss Decreased(0.275092--->0.243268)\n",
      "loss.item of the train =  6 0.21533600986003876\n",
      "loss.item of the valid =  6 0.2265554815530777\n",
      "Iteration 7 \t\t Training Loss: 0.21533600986003876 \t\t Validation Loss: 0.2265554815530777\n",
      "Validation Loss Decreased(0.243268--->0.226555)\n",
      "loss.item of the train =  7 0.19721046090126038\n",
      "loss.item of the valid =  7 0.23917515575885773\n",
      "Iteration 8 \t\t Training Loss: 0.19721046090126038 \t\t Validation Loss: 0.23917515575885773\n",
      "loss.item of the train =  8 0.2076837122440338\n",
      "loss.item of the valid =  8 0.24187909066677094\n",
      "Iteration 9 \t\t Training Loss: 0.2076837122440338 \t\t Validation Loss: 0.24187909066677094\n",
      "loss.item of the train =  9 0.20984256267547607\n",
      "loss.item of the valid =  9 0.23817305266857147\n",
      "Iteration 10 \t\t Training Loss: 0.20984256267547607 \t\t Validation Loss: 0.23817305266857147\n",
      "loss.item of the train =  10 0.20659689605236053\n",
      "loss.item of the valid =  10 0.22991277277469635\n",
      "Iteration 11 \t\t Training Loss: 0.20659689605236053 \t\t Validation Loss: 0.22991277277469635\n",
      "loss.item of the train =  11 0.1995338797569275\n",
      "loss.item of the valid =  11 0.21796582639217377\n",
      "Iteration 12 \t\t Training Loss: 0.1995338797569275 \t\t Validation Loss: 0.21796582639217377\n",
      "Validation Loss Decreased(0.226555--->0.217966)\n",
      "loss.item of the train =  12 0.1894191950559616\n",
      "loss.item of the valid =  12 0.214218407869339\n",
      "Iteration 13 \t\t Training Loss: 0.1894191950559616 \t\t Validation Loss: 0.214218407869339\n",
      "Validation Loss Decreased(0.217966--->0.214218)\n",
      "loss.item of the train =  13 0.18725065886974335\n",
      "loss.item of the valid =  13 0.21797731518745422\n",
      "Iteration 14 \t\t Training Loss: 0.18725065886974335 \t\t Validation Loss: 0.21797731518745422\n",
      "loss.item of the train =  14 0.19130995869636536\n",
      "loss.item of the valid =  14 0.21315592527389526\n",
      "Iteration 15 \t\t Training Loss: 0.19130995869636536 \t\t Validation Loss: 0.21315592527389526\n",
      "Validation Loss Decreased(0.214218--->0.213156)\n",
      "loss.item of the train =  15 0.1868116408586502\n",
      "loss.item of the valid =  15 0.20083054900169373\n",
      "Iteration 16 \t\t Training Loss: 0.1868116408586502 \t\t Validation Loss: 0.20083054900169373\n",
      "Validation Loss Decreased(0.213156--->0.200831)\n",
      "loss.item of the train =  16 0.1748766303062439\n",
      "loss.item of the valid =  16 0.20041058957576752\n",
      "Iteration 17 \t\t Training Loss: 0.1748766303062439 \t\t Validation Loss: 0.20041058957576752\n",
      "Validation Loss Decreased(0.200831--->0.200411)\n",
      "loss.item of the train =  17 0.17356258630752563\n",
      "loss.item of the valid =  17 0.20127975940704346\n",
      "Iteration 18 \t\t Training Loss: 0.17356258630752563 \t\t Validation Loss: 0.20127975940704346\n",
      "loss.item of the train =  18 0.17386557161808014\n",
      "loss.item of the valid =  18 0.19791825115680695\n",
      "Iteration 19 \t\t Training Loss: 0.17386557161808014 \t\t Validation Loss: 0.19791825115680695\n",
      "Validation Loss Decreased(0.200411--->0.197918)\n",
      "loss.item of the train =  19 0.17060132324695587\n",
      "loss.item of the valid =  19 0.19058196246623993\n",
      "Iteration 20 \t\t Training Loss: 0.17060132324695587 \t\t Validation Loss: 0.19058196246623993\n",
      "Validation Loss Decreased(0.197918--->0.190582)\n",
      "loss.item of the train =  20 0.1639690399169922\n",
      "loss.item of the valid =  20 0.17913614213466644\n",
      "Iteration 21 \t\t Training Loss: 0.1639690399169922 \t\t Validation Loss: 0.17913614213466644\n",
      "Validation Loss Decreased(0.190582--->0.179136)\n",
      "loss.item of the train =  21 0.15348869562149048\n",
      "loss.item of the valid =  21 0.1697808802127838\n",
      "Iteration 22 \t\t Training Loss: 0.15348869562149048 \t\t Validation Loss: 0.1697808802127838\n",
      "Validation Loss Decreased(0.179136--->0.169781)\n",
      "loss.item of the train =  22 0.1437060534954071\n",
      "loss.item of the valid =  22 0.17111332714557648\n",
      "Iteration 23 \t\t Training Loss: 0.1437060534954071 \t\t Validation Loss: 0.17111332714557648\n",
      "loss.item of the train =  23 0.14401353895664215\n",
      "loss.item of the valid =  23 0.17080563306808472\n",
      "Iteration 24 \t\t Training Loss: 0.14401353895664215 \t\t Validation Loss: 0.17080563306808472\n",
      "loss.item of the train =  24 0.1431795358657837\n",
      "loss.item of the valid =  24 0.17904961109161377\n",
      "Iteration 25 \t\t Training Loss: 0.1431795358657837 \t\t Validation Loss: 0.17904961109161377\n",
      "loss.item of the train =  25 0.15027651190757751\n",
      "loss.item of the valid =  25 0.18135571479797363\n",
      "Iteration 26 \t\t Training Loss: 0.15027651190757751 \t\t Validation Loss: 0.18135571479797363\n",
      "loss.item of the train =  26 0.1514691859483719\n",
      "loss.item of the valid =  26 0.1760462373495102\n",
      "Iteration 27 \t\t Training Loss: 0.1514691859483719 \t\t Validation Loss: 0.1760462373495102\n",
      "loss.item of the train =  27 0.14542141556739807\n",
      "loss.item of the valid =  27 0.18430209159851074\n",
      "Iteration 28 \t\t Training Loss: 0.14542141556739807 \t\t Validation Loss: 0.18430209159851074\n",
      "loss.item of the train =  28 0.15068696439266205\n",
      "loss.item of the valid =  28 0.19023732841014862\n",
      "Iteration 29 \t\t Training Loss: 0.15068696439266205 \t\t Validation Loss: 0.19023732841014862\n",
      "loss.item of the train =  29 0.15436683595180511\n",
      "loss.item of the valid =  29 0.19388355314731598\n",
      "Iteration 30 \t\t Training Loss: 0.15436683595180511 \t\t Validation Loss: 0.19388355314731598\n",
      "loss.item of the train =  30 0.1565134972333908\n",
      "loss.item of the valid =  30 0.19542929530143738\n",
      "Iteration 31 \t\t Training Loss: 0.1565134972333908 \t\t Validation Loss: 0.19542929530143738\n",
      "loss.item of the train =  31 0.15729239583015442\n",
      "loss.item of the valid =  31 0.1950438916683197\n",
      "Iteration 32 \t\t Training Loss: 0.15729239583015442 \t\t Validation Loss: 0.1950438916683197\n",
      "loss.item of the train =  32 0.15683513879776\n",
      "loss.item of the valid =  32 0.19288168847560883\n",
      "Iteration 33 \t\t Training Loss: 0.15683513879776 \t\t Validation Loss: 0.19288168847560883\n",
      "loss.item of the train =  33 0.1552462875843048\n",
      "loss.item of the valid =  33 0.18909254670143127\n",
      "Iteration 34 \t\t Training Loss: 0.1552462875843048 \t\t Validation Loss: 0.18909254670143127\n",
      "loss.item of the train =  34 0.1526145488023758\n",
      "loss.item of the valid =  34 0.1838373839855194\n",
      "Iteration 35 \t\t Training Loss: 0.1526145488023758 \t\t Validation Loss: 0.1838373839855194\n",
      "loss.item of the train =  35 0.14902707934379578\n",
      "loss.item of the valid =  35 0.17729738354682922\n",
      "Iteration 36 \t\t Training Loss: 0.14902707934379578 \t\t Validation Loss: 0.17729738354682922\n",
      "loss.item of the train =  36 0.14457932114601135\n",
      "loss.item of the valid =  36 0.18475057184696198\n",
      "Iteration 37 \t\t Training Loss: 0.14457932114601135 \t\t Validation Loss: 0.18475057184696198\n",
      "loss.item of the train =  37 0.15237802267074585\n",
      "loss.item of the valid =  37 0.18771807849407196\n",
      "Iteration 38 \t\t Training Loss: 0.15237802267074585 \t\t Validation Loss: 0.18771807849407196\n",
      "loss.item of the train =  38 0.1555558443069458\n",
      "loss.item of the valid =  38 0.1848127841949463\n",
      "Iteration 39 \t\t Training Loss: 0.1555558443069458 \t\t Validation Loss: 0.1848127841949463\n",
      "loss.item of the train =  39 0.1530078649520874\n",
      "loss.item of the valid =  39 0.17688272893428802\n",
      "Iteration 40 \t\t Training Loss: 0.1530078649520874 \t\t Validation Loss: 0.17688272893428802\n",
      "loss.item of the train =  40 0.14558762311935425\n",
      "loss.item of the valid =  40 0.17570807039737701\n",
      "Iteration 41 \t\t Training Loss: 0.14558762311935425 \t\t Validation Loss: 0.17570807039737701\n",
      "loss.item of the train =  41 0.14361245930194855\n",
      "loss.item of the valid =  41 0.17909109592437744\n",
      "Iteration 42 \t\t Training Loss: 0.14361245930194855 \t\t Validation Loss: 0.17909109592437744\n",
      "loss.item of the train =  42 0.14602236449718475\n",
      "loss.item of the valid =  42 0.1804727464914322\n",
      "Iteration 43 \t\t Training Loss: 0.14602236449718475 \t\t Validation Loss: 0.1804727464914322\n",
      "loss.item of the train =  43 0.14706668257713318\n",
      "loss.item of the valid =  43 0.18015532195568085\n",
      "Iteration 44 \t\t Training Loss: 0.14706668257713318 \t\t Validation Loss: 0.18015532195568085\n",
      "loss.item of the train =  44 0.146945521235466\n",
      "loss.item of the valid =  44 0.17839765548706055\n",
      "Iteration 45 \t\t Training Loss: 0.146945521235466 \t\t Validation Loss: 0.17839765548706055\n",
      "loss.item of the train =  45 0.14582443237304688\n",
      "loss.item of the valid =  45 0.17541712522506714\n",
      "Iteration 46 \t\t Training Loss: 0.14582443237304688 \t\t Validation Loss: 0.17541712522506714\n",
      "loss.item of the train =  46 0.1438390612602234\n",
      "loss.item of the valid =  46 0.17139500379562378\n",
      "Iteration 47 \t\t Training Loss: 0.1438390612602234 \t\t Validation Loss: 0.17139500379562378\n",
      "loss.item of the train =  47 0.14110004901885986\n",
      "loss.item of the valid =  47 0.1664884090423584\n",
      "Iteration 48 \t\t Training Loss: 0.14110004901885986 \t\t Validation Loss: 0.1664884090423584\n",
      "Validation Loss Decreased(0.169781--->0.166488)\n",
      "loss.item of the train =  48 0.13770291209220886\n",
      "loss.item of the valid =  48 0.16303344070911407\n",
      "Iteration 49 \t\t Training Loss: 0.13770291209220886 \t\t Validation Loss: 0.16303344070911407\n",
      "Validation Loss Decreased(0.166488--->0.163033)\n",
      "loss.item of the train =  49 0.13566997647285461\n",
      "loss.item of the valid =  49 0.16396425664424896\n",
      "Iteration 50 \t\t Training Loss: 0.13566997647285461 \t\t Validation Loss: 0.16396425664424896\n",
      "loss.item of the train =  50 0.13698025047779083\n",
      "loss.item of the valid =  50 0.16033165156841278\n",
      "Iteration 51 \t\t Training Loss: 0.13698025047779083 \t\t Validation Loss: 0.16033165156841278\n",
      "Validation Loss Decreased(0.163033--->0.160332)\n",
      "loss.item of the train =  51 0.13382300734519958\n",
      "loss.item of the valid =  51 0.16146700084209442\n",
      "Iteration 52 \t\t Training Loss: 0.13382300734519958 \t\t Validation Loss: 0.16146700084209442\n",
      "loss.item of the train =  52 0.13454343378543854\n",
      "loss.item of the valid =  52 0.1630164384841919\n",
      "Iteration 53 \t\t Training Loss: 0.13454343378543854 \t\t Validation Loss: 0.1630164384841919\n",
      "loss.item of the train =  53 0.13543014228343964\n",
      "loss.item of the valid =  53 0.16412504017353058\n",
      "Iteration 54 \t\t Training Loss: 0.13543014228343964 \t\t Validation Loss: 0.16412504017353058\n",
      "loss.item of the train =  54 0.13616147637367249\n",
      "loss.item of the valid =  54 0.1639614701271057\n",
      "Iteration 55 \t\t Training Loss: 0.13616147637367249 \t\t Validation Loss: 0.1639614701271057\n",
      "loss.item of the train =  55 0.1359807401895523\n",
      "loss.item of the valid =  55 0.1638791561126709\n",
      "Iteration 56 \t\t Training Loss: 0.1359807401895523 \t\t Validation Loss: 0.1638791561126709\n",
      "loss.item of the train =  56 0.13607287406921387\n",
      "loss.item of the valid =  56 0.1616056263446808\n",
      "Iteration 57 \t\t Training Loss: 0.13607287406921387 \t\t Validation Loss: 0.1616056263446808\n",
      "loss.item of the train =  57 0.13406047224998474\n",
      "loss.item of the valid =  57 0.15944904088974\n",
      "Iteration 58 \t\t Training Loss: 0.13406047224998474 \t\t Validation Loss: 0.15944904088974\n",
      "Validation Loss Decreased(0.160332--->0.159449)\n",
      "loss.item of the train =  58 0.13236740231513977\n",
      "loss.item of the valid =  58 0.16276603937149048\n",
      "Iteration 59 \t\t Training Loss: 0.13236740231513977 \t\t Validation Loss: 0.16276603937149048\n",
      "loss.item of the train =  59 0.13571235537528992\n",
      "loss.item of the valid =  59 0.16397352516651154\n",
      "Iteration 60 \t\t Training Loss: 0.13571235537528992 \t\t Validation Loss: 0.16397352516651154\n",
      "loss.item of the train =  60 0.1367645412683487\n",
      "loss.item of the valid =  60 0.16061078011989594\n",
      "Iteration 61 \t\t Training Loss: 0.1367645412683487 \t\t Validation Loss: 0.16061078011989594\n",
      "loss.item of the train =  61 0.1333906501531601\n",
      "loss.item of the valid =  61 0.16166310012340546\n",
      "Iteration 62 \t\t Training Loss: 0.1333906501531601 \t\t Validation Loss: 0.16166310012340546\n",
      "loss.item of the train =  62 0.13357143104076385\n",
      "loss.item of the valid =  62 0.16408105194568634\n",
      "Iteration 63 \t\t Training Loss: 0.13357143104076385 \t\t Validation Loss: 0.16408105194568634\n",
      "loss.item of the train =  63 0.13523143529891968\n",
      "loss.item of the valid =  63 0.16502980887889862\n",
      "Iteration 64 \t\t Training Loss: 0.13523143529891968 \t\t Validation Loss: 0.16502980887889862\n",
      "loss.item of the train =  64 0.13584981858730316\n",
      "loss.item of the valid =  64 0.1646501123905182\n",
      "Iteration 65 \t\t Training Loss: 0.13584981858730316 \t\t Validation Loss: 0.1646501123905182\n",
      "loss.item of the train =  65 0.1355278193950653\n",
      "loss.item of the valid =  65 0.16468629240989685\n",
      "Iteration 66 \t\t Training Loss: 0.1355278193950653 \t\t Validation Loss: 0.16468629240989685\n",
      "loss.item of the train =  66 0.1357860118150711\n",
      "loss.item of the valid =  66 0.1619216799736023\n",
      "Iteration 67 \t\t Training Loss: 0.1357860118150711 \t\t Validation Loss: 0.1619216799736023\n",
      "loss.item of the train =  67 0.13361729681491852\n",
      "loss.item of the valid =  67 0.16057877242565155\n",
      "Iteration 68 \t\t Training Loss: 0.13361729681491852 \t\t Validation Loss: 0.16057877242565155\n",
      "loss.item of the train =  68 0.13224147260189056\n",
      "loss.item of the valid =  68 0.15997937321662903\n",
      "Iteration 69 \t\t Training Loss: 0.13224147260189056 \t\t Validation Loss: 0.15997937321662903\n",
      "loss.item of the train =  69 0.13199877738952637\n",
      "loss.item of the valid =  69 0.16096746921539307\n",
      "Iteration 70 \t\t Training Loss: 0.13199877738952637 \t\t Validation Loss: 0.16096746921539307\n",
      "loss.item of the train =  70 0.1326865255832672\n",
      "loss.item of the valid =  70 0.16112270951271057\n",
      "Iteration 71 \t\t Training Loss: 0.1326865255832672 \t\t Validation Loss: 0.16112270951271057\n",
      "loss.item of the train =  71 0.13221047818660736\n",
      "loss.item of the valid =  71 0.1623930037021637\n",
      "Iteration 72 \t\t Training Loss: 0.13221047818660736 \t\t Validation Loss: 0.1623930037021637\n",
      "loss.item of the train =  72 0.13299055397510529\n",
      "loss.item of the valid =  72 0.162213534116745\n",
      "Iteration 73 \t\t Training Loss: 0.13299055397510529 \t\t Validation Loss: 0.162213534116745\n",
      "loss.item of the train =  73 0.13276761770248413\n",
      "loss.item of the valid =  73 0.16069425642490387\n",
      "Iteration 74 \t\t Training Loss: 0.13276761770248413 \t\t Validation Loss: 0.16069425642490387\n",
      "loss.item of the train =  74 0.1316208839416504\n",
      "loss.item of the valid =  74 0.16183045506477356\n",
      "Iteration 75 \t\t Training Loss: 0.1316208839416504 \t\t Validation Loss: 0.16183045506477356\n",
      "loss.item of the train =  75 0.13302169740200043\n",
      "loss.item of the valid =  75 0.1603488326072693\n",
      "Iteration 76 \t\t Training Loss: 0.13302169740200043 \t\t Validation Loss: 0.1603488326072693\n",
      "loss.item of the train =  76 0.13161639869213104\n",
      "loss.item of the valid =  76 0.16098780930042267\n",
      "Iteration 77 \t\t Training Loss: 0.13161639869213104 \t\t Validation Loss: 0.16098780930042267\n",
      "loss.item of the train =  77 0.1316055953502655\n",
      "loss.item of the valid =  77 0.16215632855892181\n",
      "Iteration 78 \t\t Training Loss: 0.1316055953502655 \t\t Validation Loss: 0.16215632855892181\n",
      "loss.item of the train =  78 0.133844256401062\n",
      "loss.item of the valid =  78 0.1630154252052307\n",
      "Iteration 79 \t\t Training Loss: 0.133844256401062 \t\t Validation Loss: 0.1630154252052307\n",
      "loss.item of the train =  79 0.13419625163078308\n",
      "loss.item of the valid =  79 0.1635410636663437\n",
      "Iteration 80 \t\t Training Loss: 0.13419625163078308 \t\t Validation Loss: 0.1635410636663437\n",
      "loss.item of the train =  80 0.1331189125776291\n",
      "loss.item of the valid =  80 0.16261930763721466\n",
      "Iteration 81 \t\t Training Loss: 0.1331189125776291 \t\t Validation Loss: 0.16261930763721466\n",
      "loss.item of the train =  81 0.13238279521465302\n",
      "loss.item of the valid =  81 0.1603558212518692\n",
      "Iteration 82 \t\t Training Loss: 0.13238279521465302 \t\t Validation Loss: 0.1603558212518692\n",
      "loss.item of the train =  82 0.1307346671819687\n",
      "loss.item of the valid =  82 0.16404446959495544\n",
      "Iteration 83 \t\t Training Loss: 0.1307346671819687 \t\t Validation Loss: 0.16404446959495544\n",
      "loss.item of the train =  83 0.13450779020786285\n",
      "loss.item of the valid =  83 0.16443954408168793\n",
      "Iteration 84 \t\t Training Loss: 0.13450779020786285 \t\t Validation Loss: 0.16443954408168793\n",
      "loss.item of the train =  84 0.13484160602092743\n",
      "loss.item of the valid =  84 0.15959930419921875\n",
      "Iteration 85 \t\t Training Loss: 0.13484160602092743 \t\t Validation Loss: 0.15959930419921875\n",
      "loss.item of the train =  85 0.13010403513908386\n",
      "loss.item of the valid =  85 0.1641155183315277\n",
      "Iteration 86 \t\t Training Loss: 0.13010403513908386 \t\t Validation Loss: 0.1641155183315277\n",
      "loss.item of the train =  86 0.13305330276489258\n",
      "loss.item of the valid =  86 0.1672205626964569\n",
      "Iteration 87 \t\t Training Loss: 0.13305330276489258 \t\t Validation Loss: 0.1672205626964569\n",
      "loss.item of the train =  87 0.13580632209777832\n",
      "loss.item of the valid =  87 0.1698359251022339\n",
      "Iteration 88 \t\t Training Loss: 0.13580632209777832 \t\t Validation Loss: 0.1698359251022339\n",
      "loss.item of the train =  88 0.13683360815048218\n",
      "loss.item of the valid =  88 0.17077825963497162\n",
      "Iteration 89 \t\t Training Loss: 0.13683360815048218 \t\t Validation Loss: 0.17077825963497162\n",
      "loss.item of the train =  89 0.13740228116512299\n",
      "loss.item of the valid =  89 0.17019036412239075\n",
      "Iteration 90 \t\t Training Loss: 0.13740228116512299 \t\t Validation Loss: 0.17019036412239075\n",
      "loss.item of the train =  90 0.13725417852401733\n",
      "loss.item of the valid =  90 0.16946779191493988\n",
      "Iteration 91 \t\t Training Loss: 0.13725417852401733 \t\t Validation Loss: 0.16946779191493988\n",
      "loss.item of the train =  91 0.13634952902793884\n",
      "loss.item of the valid =  91 0.16732314229011536\n",
      "Iteration 92 \t\t Training Loss: 0.13634952902793884 \t\t Validation Loss: 0.16732314229011536\n",
      "loss.item of the train =  92 0.13481733202934265\n",
      "loss.item of the valid =  92 0.1638730764389038\n",
      "Iteration 93 \t\t Training Loss: 0.13481733202934265 \t\t Validation Loss: 0.1638730764389038\n",
      "loss.item of the train =  93 0.1324167400598526\n",
      "loss.item of the valid =  93 0.16009534895420074\n",
      "Iteration 94 \t\t Training Loss: 0.1324167400598526 \t\t Validation Loss: 0.16009534895420074\n",
      "loss.item of the train =  94 0.12995845079421997\n",
      "loss.item of the valid =  94 0.16148868203163147\n",
      "Iteration 95 \t\t Training Loss: 0.12995845079421997 \t\t Validation Loss: 0.16148868203163147\n",
      "loss.item of the train =  95 0.1313297599554062\n",
      "loss.item of the valid =  95 0.1600268930196762\n",
      "Iteration 96 \t\t Training Loss: 0.1313297599554062 \t\t Validation Loss: 0.1600268930196762\n",
      "loss.item of the train =  96 0.13124167919158936\n",
      "loss.item of the valid =  96 0.16145695745944977\n",
      "Iteration 97 \t\t Training Loss: 0.13124167919158936 \t\t Validation Loss: 0.16145695745944977\n",
      "loss.item of the train =  97 0.13108542561531067\n",
      "loss.item of the valid =  97 0.1624177247285843\n",
      "Iteration 98 \t\t Training Loss: 0.13108542561531067 \t\t Validation Loss: 0.1624177247285843\n",
      "loss.item of the train =  98 0.13111013174057007\n",
      "loss.item of the valid =  98 0.16176235675811768\n",
      "Iteration 99 \t\t Training Loss: 0.13111013174057007 \t\t Validation Loss: 0.16176235675811768\n",
      "loss.item of the train =  99 0.1305893063545227\n",
      "ged= tensor([0.0285, 0.1621, 0.1257,  ..., 0.0423, 0.0536, 0.0396], device='cuda:0',\n",
      "       dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "Distances:  tensor([0.0285, 0.1621, 0.1257,  ..., 0.0423, 0.0536, 0.0396], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "Loss Triangular: 0.031666867434978485\n",
      "node_costs :\n",
      "tensor([[0.0000, 0.0054, 0.0052, 0.0058, 0.0059, 0.0049, 0.0049, 0.0057, 0.0049,\n",
      "         0.0049, 0.0052, 0.0059, 0.0051, 0.0058, 0.0058, 0.0058, 0.0049, 0.0057,\n",
      "         0.0053],\n",
      "        [0.0054, 0.0000, 0.0052, 0.0057, 0.0054, 0.0053, 0.0058, 0.0053, 0.0051,\n",
      "         0.0058, 0.0058, 0.0054, 0.0060, 0.0051, 0.0058, 0.0056, 0.0050, 0.0050,\n",
      "         0.0059],\n",
      "        [0.0052, 0.0052, 0.0000, 0.0050, 0.0051, 0.0053, 0.0052, 0.0050, 0.0052,\n",
      "         0.0058, 0.0058, 0.0049, 0.0052, 0.0054, 0.0058, 0.0056, 0.0049, 0.0058,\n",
      "         0.0050],\n",
      "        [0.0058, 0.0057, 0.0050, 0.0000, 0.0049, 0.0054, 0.0059, 0.0059, 0.0052,\n",
      "         0.0049, 0.0050, 0.0055, 0.0052, 0.0050, 0.0050, 0.0053, 0.0059, 0.0051,\n",
      "         0.0055],\n",
      "        [0.0059, 0.0054, 0.0051, 0.0049, 0.0000, 0.0049, 0.0049, 0.0049, 0.0051,\n",
      "         0.0059, 0.0052, 0.0056, 0.0056, 0.0051, 0.0057, 0.0051, 0.0051, 0.0052,\n",
      "         0.0055],\n",
      "        [0.0049, 0.0053, 0.0053, 0.0054, 0.0049, 0.0000, 0.0050, 0.0057, 0.0049,\n",
      "         0.0051, 0.0058, 0.0050, 0.0057, 0.0056, 0.0059, 0.0051, 0.0059, 0.0052,\n",
      "         0.0053],\n",
      "        [0.0049, 0.0058, 0.0052, 0.0059, 0.0049, 0.0050, 0.0000, 0.0054, 0.0049,\n",
      "         0.0053, 0.0056, 0.0052, 0.0049, 0.0050, 0.0053, 0.0058, 0.0058, 0.0051,\n",
      "         0.0056],\n",
      "        [0.0057, 0.0053, 0.0050, 0.0059, 0.0049, 0.0057, 0.0054, 0.0000, 0.0051,\n",
      "         0.0057, 0.0056, 0.0050, 0.0051, 0.0049, 0.0055, 0.0050, 0.0049, 0.0056,\n",
      "         0.0054],\n",
      "        [0.0049, 0.0051, 0.0052, 0.0052, 0.0051, 0.0049, 0.0049, 0.0051, 0.0000,\n",
      "         0.0052, 0.0053, 0.0056, 0.0054, 0.0051, 0.0056, 0.0055, 0.0051, 0.0055,\n",
      "         0.0052],\n",
      "        [0.0049, 0.0058, 0.0058, 0.0049, 0.0059, 0.0051, 0.0053, 0.0057, 0.0052,\n",
      "         0.0000, 0.0055, 0.0052, 0.0057, 0.0059, 0.0053, 0.0056, 0.0053, 0.0056,\n",
      "         0.0050],\n",
      "        [0.0052, 0.0058, 0.0058, 0.0050, 0.0052, 0.0058, 0.0056, 0.0056, 0.0053,\n",
      "         0.0055, 0.0000, 0.0055, 0.0059, 0.0049, 0.0055, 0.0055, 0.0051, 0.0050,\n",
      "         0.0056],\n",
      "        [0.0059, 0.0054, 0.0049, 0.0055, 0.0056, 0.0050, 0.0052, 0.0050, 0.0056,\n",
      "         0.0052, 0.0055, 0.0000, 0.0053, 0.0053, 0.0050, 0.0056, 0.0053, 0.0057,\n",
      "         0.0058],\n",
      "        [0.0051, 0.0060, 0.0052, 0.0052, 0.0056, 0.0057, 0.0049, 0.0051, 0.0054,\n",
      "         0.0057, 0.0059, 0.0053, 0.0000, 0.0057, 0.0049, 0.0055, 0.0057, 0.0054,\n",
      "         0.0052],\n",
      "        [0.0058, 0.0051, 0.0054, 0.0050, 0.0051, 0.0056, 0.0050, 0.0049, 0.0051,\n",
      "         0.0059, 0.0049, 0.0053, 0.0057, 0.0000, 0.0050, 0.0055, 0.0052, 0.0056,\n",
      "         0.0049],\n",
      "        [0.0058, 0.0058, 0.0058, 0.0050, 0.0057, 0.0059, 0.0053, 0.0055, 0.0056,\n",
      "         0.0053, 0.0055, 0.0050, 0.0049, 0.0050, 0.0000, 0.0058, 0.0049, 0.0058,\n",
      "         0.0049],\n",
      "        [0.0058, 0.0056, 0.0056, 0.0053, 0.0051, 0.0051, 0.0058, 0.0050, 0.0055,\n",
      "         0.0056, 0.0055, 0.0056, 0.0055, 0.0055, 0.0058, 0.0000, 0.0052, 0.0054,\n",
      "         0.0050],\n",
      "        [0.0049, 0.0050, 0.0049, 0.0059, 0.0051, 0.0059, 0.0058, 0.0049, 0.0051,\n",
      "         0.0053, 0.0051, 0.0053, 0.0057, 0.0052, 0.0049, 0.0052, 0.0000, 0.0050,\n",
      "         0.0052],\n",
      "        [0.0057, 0.0050, 0.0058, 0.0051, 0.0052, 0.0052, 0.0051, 0.0056, 0.0055,\n",
      "         0.0056, 0.0050, 0.0057, 0.0054, 0.0056, 0.0058, 0.0054, 0.0050, 0.0000,\n",
      "         0.0050],\n",
      "        [0.0053, 0.0059, 0.0050, 0.0055, 0.0055, 0.0053, 0.0056, 0.0054, 0.0052,\n",
      "         0.0050, 0.0056, 0.0058, 0.0052, 0.0049, 0.0049, 0.0050, 0.0052, 0.0050,\n",
      "         0.0000]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "nodeInsDel: 3.664436931671844e-08\n",
      "edge_costs :\n",
      "tensor([[0.0000, 0.0488, 0.0340],\n",
      "        [0.0488, 0.0000, 0.0010],\n",
      "        [0.0340, 0.0010, 0.0000]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "edgeInsDel: 0.0009477500570937991\n",
      "loss.item of the valid =  99 0.16489052772521973\n",
      "Iteration 100 \t\t Training Loss: 0.1305893063545227 \t\t Validation Loss: 0.16489052772521973\n",
      "iter and min_valid_loss =  57 0.15944904088974\n",
      " Min cost for nodeInsDel =  tensor(2.8361e-06, device='cuda:0', grad_fn=<SelectBackward>)\n",
      " Min cost for edgeInsDel =  tensor(0.0007, device='cuda:0', grad_fn=<SelectBackward>)\n",
      " Min cost for nodeSub =  tensor([[0.0000, 0.0056, 0.0054, 0.0059, 0.0060, 0.0052, 0.0052, 0.0058, 0.0052,\n",
      "         0.0052, 0.0054, 0.0060, 0.0053, 0.0060, 0.0059, 0.0060, 0.0052, 0.0058,\n",
      "         0.0055],\n",
      "        [0.0056, 0.0000, 0.0055, 0.0059, 0.0056, 0.0055, 0.0060, 0.0055, 0.0053,\n",
      "         0.0059, 0.0060, 0.0056, 0.0061, 0.0053, 0.0060, 0.0058, 0.0053, 0.0052,\n",
      "         0.0061],\n",
      "        [0.0054, 0.0055, 0.0000, 0.0053, 0.0053, 0.0055, 0.0054, 0.0053, 0.0054,\n",
      "         0.0060, 0.0060, 0.0052, 0.0054, 0.0056, 0.0060, 0.0058, 0.0052, 0.0059,\n",
      "         0.0053],\n",
      "        [0.0059, 0.0059, 0.0053, 0.0000, 0.0052, 0.0056, 0.0061, 0.0060, 0.0054,\n",
      "         0.0052, 0.0053, 0.0057, 0.0054, 0.0053, 0.0052, 0.0055, 0.0060, 0.0053,\n",
      "         0.0057],\n",
      "        [0.0060, 0.0056, 0.0053, 0.0052, 0.0000, 0.0052, 0.0052, 0.0052, 0.0053,\n",
      "         0.0061, 0.0054, 0.0058, 0.0058, 0.0054, 0.0059, 0.0053, 0.0054, 0.0054,\n",
      "         0.0057],\n",
      "        [0.0052, 0.0055, 0.0055, 0.0056, 0.0052, 0.0000, 0.0053, 0.0058, 0.0052,\n",
      "         0.0053, 0.0059, 0.0053, 0.0059, 0.0058, 0.0060, 0.0054, 0.0061, 0.0054,\n",
      "         0.0055],\n",
      "        [0.0052, 0.0060, 0.0054, 0.0061, 0.0052, 0.0053, 0.0000, 0.0056, 0.0052,\n",
      "         0.0055, 0.0058, 0.0054, 0.0052, 0.0052, 0.0055, 0.0059, 0.0059, 0.0053,\n",
      "         0.0057],\n",
      "        [0.0058, 0.0055, 0.0053, 0.0060, 0.0052, 0.0058, 0.0056, 0.0000, 0.0053,\n",
      "         0.0058, 0.0057, 0.0052, 0.0054, 0.0052, 0.0057, 0.0052, 0.0052, 0.0058,\n",
      "         0.0056],\n",
      "        [0.0052, 0.0053, 0.0054, 0.0054, 0.0053, 0.0052, 0.0052, 0.0053, 0.0000,\n",
      "         0.0055, 0.0055, 0.0057, 0.0056, 0.0054, 0.0058, 0.0057, 0.0053, 0.0056,\n",
      "         0.0054],\n",
      "        [0.0052, 0.0059, 0.0060, 0.0052, 0.0061, 0.0053, 0.0055, 0.0058, 0.0055,\n",
      "         0.0000, 0.0057, 0.0054, 0.0059, 0.0061, 0.0055, 0.0058, 0.0055, 0.0058,\n",
      "         0.0053],\n",
      "        [0.0054, 0.0060, 0.0060, 0.0053, 0.0054, 0.0059, 0.0058, 0.0057, 0.0055,\n",
      "         0.0057, 0.0000, 0.0057, 0.0060, 0.0052, 0.0057, 0.0057, 0.0053, 0.0053,\n",
      "         0.0058],\n",
      "        [0.0060, 0.0056, 0.0052, 0.0057, 0.0058, 0.0053, 0.0054, 0.0052, 0.0057,\n",
      "         0.0054, 0.0057, 0.0000, 0.0056, 0.0055, 0.0052, 0.0058, 0.0055, 0.0059,\n",
      "         0.0060],\n",
      "        [0.0053, 0.0061, 0.0054, 0.0054, 0.0058, 0.0059, 0.0052, 0.0054, 0.0056,\n",
      "         0.0059, 0.0060, 0.0056, 0.0000, 0.0059, 0.0052, 0.0057, 0.0059, 0.0056,\n",
      "         0.0055],\n",
      "        [0.0060, 0.0053, 0.0056, 0.0053, 0.0054, 0.0058, 0.0052, 0.0052, 0.0054,\n",
      "         0.0061, 0.0052, 0.0055, 0.0059, 0.0000, 0.0053, 0.0057, 0.0054, 0.0058,\n",
      "         0.0052],\n",
      "        [0.0059, 0.0060, 0.0060, 0.0052, 0.0059, 0.0060, 0.0055, 0.0057, 0.0058,\n",
      "         0.0055, 0.0057, 0.0052, 0.0052, 0.0053, 0.0000, 0.0059, 0.0052, 0.0059,\n",
      "         0.0052],\n",
      "        [0.0060, 0.0058, 0.0058, 0.0055, 0.0053, 0.0054, 0.0059, 0.0052, 0.0057,\n",
      "         0.0058, 0.0057, 0.0058, 0.0057, 0.0057, 0.0059, 0.0000, 0.0054, 0.0056,\n",
      "         0.0052],\n",
      "        [0.0052, 0.0053, 0.0052, 0.0060, 0.0054, 0.0061, 0.0059, 0.0052, 0.0053,\n",
      "         0.0055, 0.0053, 0.0055, 0.0059, 0.0054, 0.0052, 0.0054, 0.0000, 0.0053,\n",
      "         0.0054],\n",
      "        [0.0058, 0.0052, 0.0059, 0.0053, 0.0054, 0.0054, 0.0053, 0.0058, 0.0056,\n",
      "         0.0058, 0.0053, 0.0059, 0.0056, 0.0058, 0.0059, 0.0056, 0.0053, 0.0000,\n",
      "         0.0052],\n",
      "        [0.0055, 0.0061, 0.0053, 0.0057, 0.0057, 0.0055, 0.0057, 0.0056, 0.0054,\n",
      "         0.0053, 0.0058, 0.0060, 0.0055, 0.0052, 0.0052, 0.0052, 0.0054, 0.0052,\n",
      "         0.0000]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      " Min cost for edgeSub =  tensor([[0.0000, 0.0324, 0.0126],\n",
      "        [0.0324, 0.0000, 0.0034],\n",
      "        [0.0126, 0.0034, 0.0000]], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(len(Gs))\n",
    "nb_iter=100\n",
    "InsDel, nodeSub,edgeSub,loss_plt,loss_valid_plt,loss_train_plt=classification(model,Gs,nb_iter)\n",
    "plt.figure(0)\n",
    "plt.plot(InsDel[0:nb_iter,0],label=\"node\")\n",
    "plt.plot(InsDel[0:nb_iter,1],label=\"edge\")\n",
    "plt.title('Node/Edge insertion/deletion costs')\n",
    "plt.legend()\n",
    "plt.figure(1)\n",
    "for k in range(nodeSub.shape[1]):\n",
    "    plt.plot(nodeSub[0:nb_iter,k])\n",
    "plt.title('Node Substitutions costs')\n",
    "plt.figure(2)\n",
    "for k in range(edgeSub.shape[1]):\n",
    "    plt.plot(edgeSub[0:nb_iter,k])\n",
    "plt.title('Edge Substitutions costs')\n",
    "plt.figure(3)\n",
    "plt.plot(loss_plt)\n",
    "plt.title('Evolution of the train loss (loss_plt)')\n",
    "\n",
    "plt.figure(4)\n",
    "plt.plot(loss_valid_plt)\n",
    "plt.title('Evolution of the valid loss')\n",
    "'''\n",
    "plt.figure(5)\n",
    "plt.plot(loss_train_plt)\n",
    "plt.title('Evolution of the loss_train_plt')\n",
    "'''\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=torch.tensor(nx.to_scipy_sparse_matrix(Gs[0],dtype=int,weight='bond_type').todense(),dtype=torch.int) \n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plt, label='train loss')\n",
    "plt.plot(loss_valid_plt, label='valid loss')\n",
    "plt.title('Train and valid losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card = torch.tensor([G.order() for G in Gs]).to(device)\n",
    "print(Gs[0].order())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.plot(InsDel[0:500,0],label=\"node\")\n",
    "plt.plot(InsDel[0:500,1],label=\"edge\")\n",
    "plt.title('Node/Edge insertion/deletion costs')\n",
    "plt.legend()\n",
    "plt.figure(1)\n",
    "for k in range(nodeSub.shape[1]):\n",
    "    plt.plot(nodeSub[0:500,k])\n",
    "plt.title('node Substitution costs')\n",
    "plt.figure(2)\n",
    "for k in range(edgeSub.shape[1]):\n",
    "    plt.plot(edgeSub[0:500,k])\n",
    "plt.title('edge Substitution costs')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
